import streamlit as st
from pathlib import Path
import json
import requests
from utils import (
    check_ollama_model_status,
    start_ollama_model,
    stop_ollama_model,
    chat_with_model,
    get_available_models,
    fetch_ollama_models
)
import pandas as pd

def show():
    st.title("ğŸ§  Ollama ëª¨ë¸ ê´€ë¦¬")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'installed_models' not in st.session_state:
        st.session_state.installed_models = get_available_models()
    if 'available_models' not in st.session_state:
        st.session_state.available_models = []
    if 'show_model_list' not in st.session_state:
        st.session_state.show_model_list = False
    
    # ìƒë‹¨ ì„¤ëª…
    st.markdown("""
    ### ğŸ“š Ollama ëª¨ë¸ í—ˆë¸Œ
    Ollamaì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ LLM ëª¨ë¸ë“¤ì„ í™•ì¸í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ ì„¹ì…˜
    st.markdown("### ğŸ”„ ì„¤ì¹˜ëœ ëª¨ë¸")
    if st.button("ìƒˆë¡œê³ ì¹¨", key="refresh_installed", type="primary"):
        st.session_state.installed_models = get_available_models()
    
    if st.session_state.installed_models:
        for model in st.session_state.installed_models:
            st.markdown(f"- `{model}`")
    else:
        st.info("ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ ì‹¤í–‰ ì„¹ì…˜
    st.markdown("---")
    st.markdown("### ğŸš€ ëª¨ë¸ ì‹¤í–‰")
    
    # ëª¨ë¸ ì„ íƒ
    col_select, col_info = st.columns([2, 1])
    
    with col_select:
        # ì„ íƒ ë°©ì‹ ë¼ë””ì˜¤ ë²„íŠ¼
        select_method = st.radio(
            "ëª¨ë¸ ì„ íƒ ë°©ì‹",
            ["ëª©ë¡ì—ì„œ ì„ íƒ", "ì§ì ‘ ì…ë ¥"],
            horizontal=True,
            key="select_method"
        )
        
        if select_method == "ëª©ë¡ì—ì„œ ì„ íƒ":
            # ì„¤ì¹˜ëœ ëª¨ë¸ê³¼ ê¸°ë³¸ ëª¨ë¸ì„ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
            installed_models = st.session_state.installed_models
            default_models = ["llama2:7b", "llama2:13b", "llama2:70b", "gemma:2b", "gemma:7b", "qwen:7b", "qwen:14b"]
            
            # ì„¤ì¹˜ëœ ëª¨ë¸ê³¼ ê¸°ë³¸ ëª¨ë¸ì„ êµ¬ë¶„í•˜ì—¬ ì˜µì…˜ ìƒì„±
            model_options = []
            
            # ì„¤ì¹˜ëœ ëª¨ë¸ ì¶”ê°€
            if installed_models:
                model_options.append({"label": "--- ì„¤ì¹˜ëœ ëª¨ë¸ ---", "disabled": True})
                for model in sorted(installed_models):
                    model_options.append({"label": f"ğŸ“¦ {model}", "value": model})
            
            # ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€
            model_options.append({"label": "--- ì„¤ì¹˜ ê°€ëŠ¥í•œ ëª¨ë¸ ---", "disabled": True})
            for model in sorted(default_models):
                if model not in installed_models:
                    model_options.append({"label": f"ğŸ’« {model}", "value": model})
            
            model_choice = st.selectbox(
                "ì‹¤í–‰í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
                options=[opt["value"] for opt in model_options if "value" in opt],
                format_func=lambda x: next((opt["label"] for opt in model_options if "value" in opt and opt["value"] == x), x),
                help="ì„¤ì¹˜ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤."
            )
        else:
            # ì§ì ‘ ì…ë ¥ í•„ë“œ
            model_choice = st.text_input(
                "ì‹¤í–‰í•  ëª¨ë¸ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: llama2:7b",
                help="ëª¨ë¸ëª…:ë²„ì „ í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: llama2:7b, gemma:2b)"
            )
    
    with col_info:
        if select_method == "ëª©ë¡ì—ì„œ ì„ íƒ":
            st.markdown("""
            #### ì•„ì´ì½˜ ì„¤ëª…
            - ğŸ“¦ : ì´ë¯¸ ì„¤ì¹˜ëœ ëª¨ë¸
            - ğŸ’« : ì„¤ì¹˜ ê°€ëŠ¥í•œ ëª¨ë¸
            """)
        else:
            st.markdown("""
            #### ğŸ’¡ ì…ë ¥ í˜•ì‹
            - ê¸°ë³¸ í˜•ì‹: `ëª¨ë¸ëª…:ë²„ì „`
            - ë²„ì „ í‘œê¸°: `7b`, `13b`, `70b` ë“±
            - ì˜ˆì‹œ: `llama2:7b`, `gemma:2b`
            """)
    
    # ì‹¤í–‰ ë²„íŠ¼ë“¤ì„ ë‚˜ë€íˆ ë°°ì¹˜
    col3, col4, col5 = st.columns(3)
    
    with col3:
        start_disabled = not model_choice  # ëª¨ë¸ì´ ì„ íƒ/ì…ë ¥ë˜ì§€ ì•Šì€ ê²½ìš° ë²„íŠ¼ ë¹„í™œì„±í™”
        if st.button("ğŸš€ ëª¨ë¸ ì‹œì‘", type="primary", key="start_model", disabled=start_disabled):
            success, message = start_ollama_model(model_choice)
            if success:
                st.success(message)
            else:
                st.error(message)
    
    with col4:
        if st.button("ğŸ›‘ ëª¨ë¸ ì¤‘ì§€", type="secondary", key="stop_model"):
            success, message = stop_ollama_model()
            if success:
                st.success(message)
            else:
                st.error(message)
    
    with col5:
        if st.button("ğŸ” ìƒíƒœ í™•ì¸", type="secondary", key="check_status"):
            is_running = check_ollama_model_status(model_choice)
            if is_running:
                st.success(f"âœ… {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                st.warning(f"âŒ {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì„¹ì…˜ (ë³„ë„ íƒ­ìœ¼ë¡œ ë¶„ë¦¬)
    st.markdown("---")
    with st.expander("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡", expanded=False):
        if st.button("ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°", key="fetch_models", type="primary"):
            st.session_state.available_models = fetch_ollama_models()
            st.session_state.show_model_list = True
        
        # ëª¨ë¸ ëª©ë¡ í‘œì‹œ
        if st.session_state.show_model_list and st.session_state.available_models:
            st.markdown("### ì „ì²´ ëª¨ë¸ ëª©ë¡")
            
            # ëª¨ë¸ ëª©ë¡ì„ í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œ
            model_data = []
            for i, model_info in enumerate(st.session_state.available_models, 1):
                model_code = model_info['code']
                
                # ê° ëª¨ë¸ì— ëŒ€í•œ í–‰ ì¶”ê°€
                model_data.append({
                    "ë²ˆí˜¸": i,
                    "ëª¨ë¸ëª…": model_info["name"],
                    "íŒŒë¼ë¯¸í„°": model_info['parameters'],
                    "ì„¤ì¹˜": "âœ…" if model_info['code'] in st.session_state.installed_models else "âŒ",
                    "ì‹¤í–‰ ì½”ë“œ": model_code
                })
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° í‘œì‹œ
            df = pd.DataFrame(model_data)
            
            # í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§ì„ ìœ„í•œ CSS
            st.markdown("""
            <style>
            .model-table {
                font-size: 14px;
                text-align: left;
            }
            .model-table th {
                background-color: #f0f2f6;
                padding: 8px;
            }
            .model-table td {
                padding: 8px;
            }
            .model-table tr:nth-child(even) {
                background-color: #f8f9fa;
            }
            </style>
            """, unsafe_allow_html=True)
            
            # í…Œì´ë¸” í‘œì‹œ
            for _, row in df.iterrows():
                col1, col2, col3, col4, col5 = st.columns([0.3, 1.2, 0.8, 0.4, 2.8])
                with col1:
                    st.write(f"#{row['ë²ˆí˜¸']}")
                with col2:
                    st.write(f"`{row['ëª¨ë¸ëª…']}`")
                with col3:
                    st.write(row['íŒŒë¼ë¯¸í„°'])
                with col4:
                    st.write(row['ì„¤ì¹˜'])
                with col5:
                    st.code(row['ì‹¤í–‰ ì½”ë“œ'], language="bash")
            
            # ì‚¬ìš© ë°©ë²• ì„¤ëª…
            st.info("""
            ğŸ’¡ 'ë³µì‚¬' ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ëª¨ë¸ ì‹¤í–‰ ì½”ë“œê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë©ë‹ˆë‹¤.
            
            ì‹¤í–‰ ì˜ˆì‹œ:
            ```bash
            ollama run llama2:7b
            ```
            """)
    
    st.markdown("---")
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    st.subheader("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    st.info(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{model_choice}**")
    
    test_prompt = st.text_area("í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", height=100)
    
    if st.button("ğŸ’« í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
        if not test_prompt:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ë¨¼ì € ëª¨ë¸ ìƒíƒœ í™•ì¸
            if not check_ollama_model_status(model_choice):
                st.error(f"âŒ {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            else:
                response = chat_with_model(model_choice, test_prompt)
                st.markdown("### ğŸ¤– ëª¨ë¸ ì‘ë‹µ:")
                st.markdown(response) 