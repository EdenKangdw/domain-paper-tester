import streamlit as st
from pathlib import Path
import json
import requests
from utils import (
    check_ollama_model_status,
    start_ollama_model,
    stop_ollama_model,
    chat_with_model,
    get_available_models,
    fetch_ollama_models
)

def show():
    st.title("ğŸ§  Ollama ëª¨ë¸ ì‹¤í–‰ ë„ìš°ë¯¸")
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    installed_models = get_available_models()
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"):
            installed_models = get_available_models()
            if installed_models:
                st.success(f"ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(installed_models)}")
            else:
                st.info("ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        if st.button("ğŸ“š Ollama í—ˆë¸Œ ëª¨ë¸ ëª©ë¡ ë³´ê¸°"):
            available_models = fetch_ollama_models()
            if available_models:
                st.success(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìˆ˜: {len(available_models)}ê°œ")
                # ëª¨ë¸ ëª©ë¡ì„ ì—¬ëŸ¬ ì—´ë¡œ í‘œì‹œ
                cols = st.columns(3)
                for i, model in enumerate(available_models):
                    cols[i % 3].markdown(f"- `{model}`")
            else:
                st.warning("ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # ëª¨ë¸ ì„ íƒ (ì„¤ì¹˜ëœ ëª¨ë¸ + ê¸°ë³¸ ëª¨ë¸)
    default_models = ["llama2", "gemma", "qwen", "deepseek"]
    all_models = sorted(set(installed_models + default_models))
    model_choice = st.selectbox(
        "ì‹¤í–‰í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
        all_models,
        help="ì„¤ì¹˜ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤."
    )
    
    col3, col4, col5 = st.columns(3)
    
    with col3:
        if st.button("ğŸš€ ëª¨ë¸ ì‹œì‘"):
            success, message = start_ollama_model(model_choice)
            if not success:
                st.error(message)
    
    with col4:
        if st.button("ğŸ›‘ ëª¨ë¸ ì¤‘ì§€"):
            success, message = stop_ollama_model()
            if not success:
                st.error(message)
    
    with col5:
        if st.button("ğŸ” ëª¨ë¸ ìƒíƒœ í™•ì¸"):
            is_running = check_ollama_model_status(model_choice)
            if is_running:
                st.success(f"âœ… {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                st.warning(f"âŒ {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    st.subheader("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    st.info(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{model_choice}**")
    
    test_prompt = st.text_area("í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", height=100)
    
    if st.button("ğŸ’« í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
        if not test_prompt:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ë¨¼ì € ëª¨ë¸ ìƒíƒœ í™•ì¸
            if not check_ollama_model_status(model_choice):
                st.error(f"âŒ {model_choice} ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            else:
                response = chat_with_model(model_choice, test_prompt)
                st.markdown("### ğŸ¤– ëª¨ë¸ ì‘ë‹µ:")
                st.markdown(response) 