import streamlit as st
from pathlib import Path
import json
from transformers import AutoTokenizer
from utils import check_ollama_model_status, get_available_models, OLLAMA_API_BASE, get_model_response
import requests
import random
from typing import List, Tuple
import re
from datetime import datetime
import time
import os

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    st.warning("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

# Model tokenizer settings (íŒŒë¼ë¯¸í„° ìˆ˜ ë¬´ì‹œí•˜ê³  ê¸°ë³¸ ëª¨ë¸ëª…ë§Œ ì‚¬ìš©)
MODEL_TOKENIZER_MAP = {
    "llama2": "meta-llama/Llama-2-7b-hf",
    "llama2:7b": "meta-llama/Llama-2-7b-hf",
    "gemma": "google/gemma-7b",
    "gemma:7b": "google/gemma-7b",
    "gemma:2b": "google/gemma-2b",
    "qwen": "Qwen/Qwen-7B",
    "qwen:7b": "Qwen/Qwen-7B",
    "qwen:latest": "Qwen/Qwen-7B",
    "deepseek": "deepseek-ai/deepseek-coder-7b-base",
    "deepseek:7b": "deepseek-ai/deepseek-coder-7b-base",
    "deepseek-r1": "deepseek-ai/deepseek-llm-7b-base",
    "deepseek-r1:7b": "deepseek-ai/deepseek-llm-7b-base",
    "deepseek-r1-distill-llama": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "mistral": "mistralai/Mistral-7B-v0.1",
    "mistral:7b": "mistralai/Mistral-7B-v0.1",
    "mixtral": "mistralai/Mixtral-8x7B-v0.1",
    "yi": "01-ai/Yi-6B",
    "openchat": "openchat/openchat",
    "neural": "neural-chat/neural-chat-7b-v3-1",
    "phi": "microsoft/phi-2",
    "stable": "stabilityai/stable-code-3b"
}

# Origin í”„ë¡¬í”„íŠ¸ ìºì‹œ
_origin_prompts_cache = {}

def clean_deepseek_response(response_text: str) -> str:
    """DeepSeek ëª¨ë¸ì˜ <think> íƒœê·¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©ì„ ì œê±°í•˜ê³ , íƒœê·¸ ë’¤ì˜ ë‚´ìš©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not response_text:
        return response_text
    
    print(f"Original DeepSeek response: {repr(response_text)}")  # ë””ë²„ê¹…ìš©
    
    # <think> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
    if "<think>" in response_text:
        # <think> íƒœê·¸ì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
        think_start = response_text.find("<think>")
        
        # </think> íƒœê·¸ì˜ ë ìœ„ì¹˜ ì°¾ê¸°
        think_end = response_text.find("</think>")
        
        print(f"Think tags found: start={think_start}, end={think_end}")  # ë””ë²„ê¹…ìš©
        
        if think_end != -1:
            # </think> íƒœê·¸ ì´í›„ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (</think>ëŠ” 7ì)
            after_think = response_text[think_end + 7:].strip()
            
            # ë§Œì•½ </think> ì´í›„ì— ë‚´ìš©ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            if after_think:
                response_text = after_think
                print(f"Using after think content: {repr(after_think)}")  # ë””ë²„ê¹…ìš©
            else:
                # </think> ì´í›„ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì¬ì‹œë„ ìœ ë„)
                print("No content after think tags, returning empty")
                return ""
        else:
            # </think> íƒœê·¸ê°€ ì—†ìœ¼ë©´ <think> íƒœê·¸ë¶€í„° ëê¹Œì§€ ì œê±°
            response_text = response_text[:think_start].strip()
            print(f"No closing tag, using before think: {repr(response_text)}")  # ë””ë²„ê¹…ìš©
    else:
        print(f"No think tags found, using original: {repr(response_text)}")  # ë””ë²„ê¹…ìš©
    
    # ìµœì¢… ê²€ì¦: ë¹ˆ ë¬¸ìì—´ì´ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì¬ì‹œë„ ìœ ë„)
    if not response_text.strip():
        print("Cleaned response is empty, returning empty")
        return ""
    
    print(f"Final cleaned response: {repr(response_text)}")  # ë””ë²„ê¹…ìš©
    return response_text

def clean_prompt_text(prompt_text: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œì™€ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤."""
    if not prompt_text:
        return ""
    
    # ì•ë’¤ ê³µë°± ì œê±°
    prompt_text = prompt_text.strip()
    
    # ì•ë’¤ ë”°ì˜´í‘œ ì œê±° (ì¤‘ì²©ëœ ë”°ì˜´í‘œë„ ì²˜ë¦¬)
    while (prompt_text.startswith('"') and prompt_text.endswith('"')) or \
          (prompt_text.startswith("'") and prompt_text.endswith("'")):
        prompt_text = prompt_text[1:-1].strip()
    
    # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. ", "2. ", "a) ", "b) " ë“±)
    import re
    prompt_text = re.sub(r'^\d+\.\s*', '', prompt_text)  # "1. ", "2. " ë“± ì œê±°
    prompt_text = re.sub(r'^[a-z]\)\s*', '', prompt_text)  # "a) ", "b) " ë“± ì œê±°
    prompt_text = re.sub(r'^[A-Z]\)\s*', '', prompt_text)  # "A) ", "B) " ë“± ì œê±°
    prompt_text = re.sub(r'^[ivx]+\)\s*', '', prompt_text, flags=re.IGNORECASE)  # "i) ", "ii) " ë“± ì œê±°
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    prompt_text = re.sub(r'\s+', ' ', prompt_text)
    
    return prompt_text.strip()

def load_origin_prompts():
    """Origin í´ë”ì—ì„œ ëª¨ë¸ë³„, ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ìºì‹œ í™•ì¸
    if "origin_prompts_cache" in st.session_state:
        return st.session_state.origin_prompts_cache
    
    origin_dir = Path("dataset/origin")
    if not origin_dir.exists():
        st.error("dataset/origin í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    prompts_cache = {}
    
    # ëª¨ë¸ë³„ í´ë” í™•ì¸
    for model_dir in origin_dir.iterdir():
        if model_dir.is_dir():
            model_name = model_dir.name
            prompts_cache[model_name] = {}
            
            # ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            for domain in ["economy", "legal", "medical", "technical"]:
                domain_dir = model_dir / domain
                prompt_file = domain_dir / f"{domain}_prompts.json"
                
                if prompt_file.exists():
                    try:
                        with open(prompt_file, 'r', encoding='utf-8') as f:
                            prompts_data = json.load(f)
                            # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                            prompts = [item["prompt"] for item in prompts_data]
                            prompts_cache[model_name][domain.capitalize()] = prompts
                    except Exception as e:
                        st.error(f"Error loading prompts for {model_name}/{domain}: {str(e)}")
                        prompts_cache[model_name][domain.capitalize()] = []
                else:
                    st.warning(f"Prompt file not found: {prompt_file}")
                    prompts_cache[model_name][domain.capitalize()] = []
    
    # ì„¸ì…˜ ìƒíƒœì— ìºì‹œ ì €ì¥
    st.session_state.origin_prompts_cache = prompts_cache
    return prompts_cache

@st.cache_data(ttl=60)  # 60ì´ˆ ìºì‹œë¡œ ì—°ì¥
def get_running_models():
    """Get list of currently running Ollama models with caching"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            running_models = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ìµœì í™”
            for model in models:
                try:
                    # ë” ë¹ ë¥¸ ìƒíƒœ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                    if check_ollama_model_status_fast(model["name"]):
                        running_models.append(model["name"])
                except:
                    continue  # ê°œë³„ ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°
            
            return running_models
        return []
    except:
        return []

def check_ollama_model_status_fast(model_name):
    """ë¹ ë¥¸ ëª¨ë¸ ìƒíƒœ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)"""
    try:
        # ë¨¼ì € ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ í™•ì¸
        response = requests.get(f"{OLLAMA_API_BASE}/api/ps", timeout=3)
        if response.status_code == 200:
            running_models = response.json().get("models", [])
            for model in running_models:
                if model.get("name") == model_name:
                    return True
        
        # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ì— ì—†ìœ¼ë©´ ì§ì ‘ í…ŒìŠ¤íŠ¸
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": "test",
                "stream": False
            },
            timeout=15  # íƒ€ì„ì•„ì›ƒì„ 15ì´ˆë¡œ ì—°ì¥ (í° ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤)
        )
        return response.status_code == 200
    except Exception as e:
        print(f"Model status check error for {model_name}: {str(e)}")
        return False



def tokenize_and_extract_words(text, tokenizer):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ë‹¨ì–´ë¥¼ ì¶”ì¶œ"""
    print("\n=== Tokenization Debug ===")
    print(f"Original text: {text}")
    
    tokens = tokenizer.tokenize(text)
    print(f"Tokens: {tokens}")
    
    # êµ¬ë‘ì  ì œê±°ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def clean_word(word):
        # ë‹¨ì–´ ëì˜ êµ¬ë‘ì  ì œê±° (ì‰¼í‘œ, ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±)
        return word.rstrip(',.!?:;')
    
    # ë‹¨ì–´ ë¶„ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def split_into_words(text):
        # ê¸°ë³¸ì ì¸ ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
        raw_words = []
        current_word = ""
        for char in text:
            if char.isspace():
                if current_word:
                    raw_words.append(current_word)
                    current_word = ""
            else:
                current_word += char
        if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬
            raw_words.append(current_word)
        
        # êµ¬ë‘ì  ì œê±° ë° ì •ë¦¬
        words = [clean_word(word) for word in raw_words]
        # ë¹ˆ ë¬¸ìì—´ ì œê±°
        words = [word for word in words if word.strip()]
        return words
    
    # í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë‹¨ì–´ë¡œ ë¶„ë¦¬
    words = split_into_words(text)
    print(f"Split words: {words}")
    
    # ê° ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ ì°¾ê¸°
    word_positions = []
    current_pos = 0
    text_lower = text.lower()
    
    for word in words:
        word_lower = word.lower()
        # í˜„ì¬ ìœ„ì¹˜ë¶€í„° ë‹¨ì–´ ì°¾ê¸°
        while current_pos < len(text):
            pos = text_lower.find(word_lower, current_pos)
            if pos != -1:
                word_positions.append((pos, pos + len(word), word))
                current_pos = pos + len(word)
                break
            current_pos += 1
    
    # ë‹¨ì–´ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    word_positions.sort(key=lambda x: x[0])
    
    # ì •ë ¬ëœ ìˆœì„œë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¬êµ¬ì„±
    words = [wp[2] for wp in word_positions]
    
    # í† í°ê³¼ ë‹¨ì–´ ë§¤í•‘
    word_to_tokens = {}
    current_token_pos = 0
    current_token_idx = 0
    
    for start, end, word in word_positions:
        token_indices = []
        token_text = ""
        
        while current_token_idx < len(tokens) and current_token_pos <= end:
            token = tokens[current_token_idx].replace('â–', '')
            if token.strip():
                token_text += token
                if current_token_pos >= start and current_token_pos < end:
                    token_indices.append(current_token_idx)
                current_token_pos += len(token)
            current_token_idx += 1
        
        if token_indices:
            word_to_tokens[word] = token_indices
    
    print("\n=== Word Processing Results ===")
    print(f"Final words: {words}")
    print("Word to token mapping:")
    for word, token_indices in word_to_tokens.items():
        token_values = [tokens[idx] for idx in token_indices]
        print(f"Word: {word}")
        print(f"Token indices: {token_indices}")
        print(f"Token values: {token_values}\n")
    
    return tokens, words, word_to_tokens

def format_word_and_token_info(tokens, words, word_to_tokens):
    """Format token and word information"""
    token_entries = [
        f'Token[{i}] = >>>{token}<<< (raw: {repr(token)})'
        for i, token in enumerate(tokens)
    ]
    
    word_entries = [
        f'Word[{i}] = >>>{word}<<<'
        for i, word in enumerate(words)
    ]
    
    return "\n".join(token_entries), "\n".join(word_entries)

def calculate_evidence_indices(evidence_tokens, all_tokens):
    """LLMì´ ì¶”ì¶œí•œ evidence í† í°ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    evidence_indices = []
    
    for evidence_token in evidence_tokens:
        # í† í° ëª©ë¡ì—ì„œ evidence í† í°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        found = False
        for i, token in enumerate(all_tokens):
            # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            if token == evidence_token:
                evidence_indices.append(i)
                found = True
                break
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (í† í°ì´ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰œ ê²½ìš°)
            elif evidence_token in token or token in evidence_token:
                evidence_indices.append(i)
                found = True
                break
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ ì¶œë ¥
        if not found:
            print(f"Warning: Could not find index for evidence token '{evidence_token}' in token list")
    
    return evidence_indices

def create_evidence_query(word_list, prompt, domain):
    """Evidence ì¶”ì¶œì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€ - í˜¸í™˜ì„±)"""
    # ë‹¨ì–´ ëª©ë¡ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
    words = [word.strip() for word in word_list.split('\n') if word.strip()]
    
    # ë‹¨ì–´ ëª©ë¡ì„ í‘œ í˜•ì‹ìœ¼ë¡œ ìƒì„±
    word_table = []
    for i, word in enumerate(words):
        word_table.append(f"| {i} | {word} |")
    word_table = "| Index | Word |\n|-------|------|\n" + "\n".join(word_table)

    return f"""You are a JSON API that extracts evidence tokens from text. Follow these instructions exactly:

1. From the token list below, identify tokens related to the '{domain}' domain.
2. Return ONLY a JSON object with this exact format:
{{
    "evidence_token_index": [numbers],
    "evidence": [tokens]
}}

Token List:
{word_table}

Input Text: "{prompt}"

Rules:
- evidence_token_index must be an array of numbers
- evidence must be an array of exact tokens from the list
- arrays must have the same length
- do not add any explanation or text outside the JSON
- do not modify or format the tokens
- do not use markdown or code blocks
"""

def extract_json_from_response(response):
    """Extract JSON from response"""
    import re
    # ë¬¸ìì—´ë¡œ ë³€í™˜ ë³´ì¥
    if not isinstance(response, str):
        response = str(response)
    
    # ì‘ë‹µì´ ì´ë¯¸ JSON ê°ì²´ì¸ ê²½ìš°
    if isinstance(response, dict):
        return response

    try:
        # ë¨¼ì € ì „ì²´ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
        return json.loads(response)
    except json.JSONDecodeError:
        try:
            # JSON í˜•ì‹ì˜ ë¬¸ìì—´ì„ ì°¾ìŒ (ì•ë’¤ì˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°)
            json_match = re.search(r'(\{(?:[^{}]|(?:\{[^{}]*\}))*\})', response)
            if not json_match:
                raise ValueError("Could not find JSON format response")
            
            json_str = json_match.group(1)
            
            # JSON ë¬¸ìì—´ ì •ë¦¬
            # 1. ì¤„ë°”ê¿ˆê³¼ ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€ê²½
            json_str = re.sub(r'\s+', ' ', json_str).strip()
            
            # 2. ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì°¾ì•„ì„œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            # ë¨¼ì € ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œë¥¼ ì„ì‹œ ì¹˜í™˜
            json_str = json_str.replace('\\"', '___ESCAPED_QUOTE___')
            # ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            json_str = re.sub(r'(?<!\\)"([^"]*)"', r'"\1"', json_str)
            # ì„ì‹œ ì¹˜í™˜ëœ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œ ë³µì›
            json_str = json_str.replace('___ESCAPED_QUOTE___', '\\"')
            
            # 3. ì‘ì€ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë¬¸ìì—´ì„ í°ë”°ì˜´í‘œë¡œ ë³€ê²½
            json_str = re.sub(r"'([^']*)'", r'"\1"', json_str)
            
            # 4. í°ë”°ì˜´í‘œ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*"\s*', '"', json_str)
            
            # 5. ì½¤ë§ˆ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*,\s*', ',', json_str)
            
            # 6. ì¤‘ê´„í˜¸ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*{\s*', '{', json_str)
            json_str = re.sub(r'\s*}\s*', '}', json_str)
            
            return json.loads(json_str)
        except Exception as e:
            print(f"Failed to parse JSON: {response}")
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸ ì‘ë‹µ: {response}")

def validate_evidence(result, words):
    """Validate evidence results"""
    required_fields = ["evidence_word_index", "evidence", "explanation"]
    missing_fields = [field for field in required_fields if field not in result]
    if missing_fields:
        raise ValueError(f"Missing fields: {', '.join(missing_fields)}")
    
    evidence_word_index = result["evidence_word_index"]
    evidence = result["evidence"]
    
    if not isinstance(evidence_word_index, list):
        raise ValueError("evidence_word_index must be an array ([])")
    if not isinstance(evidence, list):
        raise ValueError("evidence must be an array ([])")
    
    # ë‹¨ì–´ ëª©ë¡ ì •ë¦¬ (ê³µë°± ì œê±°)
    words = [word.strip() for word in words if word.strip()]
    
    # Validate indices
    invalid_indices = []
    for i, idx in enumerate(evidence_word_index):
        if not isinstance(idx, int):
            invalid_indices.append({"position": i, "index": idx, "reason": "not an integer"})
        elif not (0 <= idx < len(words)):
            invalid_indices.append({"position": i, "index": idx, "reason": f"out of range (0-{len(words)-1})"})
    
    if invalid_indices:
        details = [
            f"Position {e['position']}: Index {e['index']} ({e['reason']})"
            for e in invalid_indices
        ]
        raise ValueError(f"Invalid indices found:\n" + "\n".join(details))
    
    # Check if evidence and evidence_word_index lengths match
    if len(evidence) != len(evidence_word_index):
        raise ValueError(f"Array lengths don't match (evidence: {len(evidence)}, index: {len(evidence_word_index)})")
    
    # Check for words not in the list
    invalid_words = []
    for i, word in enumerate(evidence):
        if word not in words:
            invalid_words.append({
                "position": i,
                "word": word,
                "available_words": words
            })
    
    if invalid_words:
        details = [
            f"Position {w['position']}: '{w['word']}' (available words: {w['available_words']})"
            for w in invalid_words
        ]
        raise ValueError(f"Words not in list found:\n" + "\n".join(details))
    
    # Check index and word matching
    mismatches = []
    for i, (idx, word) in enumerate(zip(evidence_word_index, evidence)):
        if words[idx] != word:
            mismatches.append({
                "position": i,
                "index": idx,
                "expected": words[idx],
                "actual": word
            })
    
    if mismatches:
        details = [
            f"Position {m['position']}: Index {m['index']} should be '{m['expected']}' but got '{m['actual']}'"
            for m in mismatches
        ]
        raise ValueError(f"Index and word mismatches found:\n" + "\n".join(details))
    
    return evidence_word_index, evidence

def visualize_evidence(words, evidence_word_index, evidence, explanation):
    """Visualize evidence results"""
    highlighted_words = [
        f"<span style='background-color:#fff176; padding:2px'>{word}</span>"
        if i in evidence_word_index else word
        for i, word in enumerate(words)
    ]
    
    st.markdown("### Extracted Evidence:")
    st.markdown(" ".join(highlighted_words), unsafe_allow_html=True)
    
    # JSON ë°ì´í„°ë¥¼ ë¨¼ì € íŒŒì‹±í•˜ê³  ê²€ì¦
    try:
        json_data = {
            "evidence_word_index": evidence_word_index,
            "evidence": evidence,
            "explanation": explanation
        }
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆë‹¤ê°€ ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬
        json_str = json.dumps(json_data, ensure_ascii=False)
        validated_data = json.loads(json_str)
        st.json(validated_data)
    except json.JSONDecodeError as e:
        st.error(f"JSON ë°ì´í„° ì˜¤ë¥˜: {str(e)}")
        st.code(str(json_data), language="json")

def get_test_prompt(domain: str) -> str:
    """ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ origin í´ë”ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    origin_prompts = load_origin_prompts()
    
    if domain in origin_prompts and origin_prompts[domain]:
        return random.choice(origin_prompts[domain])
    else:
        # fallback í”„ë¡¬í”„íŠ¸
        fallback_prompts = {
            "Medical": [
                "What are the main side effects of this medication?",
                "What are the contraindications for this treatment?",
                "What are the recommended dosages for this drug?",
                "What are the potential complications of this procedure?",
                "What are the warning signs to watch for?"
            ],
            "Legal": [
                "What are the key clauses in this contract?",
                "What are the main obligations of the parties?",
                "What are the termination conditions?",
                "What are the dispute resolution procedures?",
                "What are the confidentiality requirements?"
            ],
            "Technical": [
                "What is the main functionality of this code?",
                "What are the key features of this system?",
                "What are the system requirements?",
                "What are the performance specifications?",
                "What are the security measures implemented?"
            ],
            "Economy": [
                "What is the main content of this document?",
                "What are the key points discussed?",
                "What are the main conclusions?",
                "What are the important findings?",
                "What are the main recommendations?"
            ]
        }
        return random.choice(fallback_prompts.get(domain, ["Please enter your prompt here..."]))

def extract_evidence_with_ollama(prompt, tokens, model_name, domain="economy"):
    """LLMì´ evidence í† í°ì„ ì¶”ì¶œí•˜ê³ , ì½”ë“œë¡œ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        # í† í°ì´ ë°”ì´íŠ¸ íƒ€ì…ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë””ì½”ë”©
        decoded_tokens = []
        for token in tokens:
            if isinstance(token, bytes):
                try:
                    decoded_tokens.append(token.decode('utf-8'))
                except UnicodeDecodeError:
                    try:
                        decoded_tokens.append(token.decode('latin-1'))
                    except:
                        decoded_tokens.append('')
            else:
                decoded_tokens.append(str(token))

        # LLMì—ê²Œ evidence í† í° ì¶”ì¶œ ìš”ì²­
        query = create_evidence_query("\n".join(decoded_tokens), prompt, domain)
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": query,
                "stream": False
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            if 'response' in result:
                response_text = result['response']
                
                # deepseek ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
                if "deepseek" in model_name.lower():
                    import re
                    # <think>...</think> íƒœê·¸ì™€ ë‚´ìš© ì œê±°
                    response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
                    # <think> íƒœê·¸ë§Œ ìˆëŠ” ê²½ìš° ì œê±°
                    response_text = re.sub(r'<think>\s*</think>', '', response_text)
                    response_text = response_text.strip()
                    print(f"Raw response from model (deepseek íƒœê·¸ ì œê±°): {response_text}")  # ë””ë²„ê¹…ìš©
                else:
                    print(f"Raw response from model: {response_text}")  # ë””ë²„ê¹…ìš©
                
                try:
                    # JSON ë¬¸ìì—´ ì •ë¦¬
                    response_text = response_text.strip()
                    
                    # ì´ìŠ¤ì¼€ì´í”„ëœ JSON ë¬¸ìì—´ ì²˜ë¦¬
                    if '\\"' in response_text:
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œë¥¼ ì¼ë°˜ ë”°ì˜´í‘œë¡œ ë³€í™˜
                        response_text = response_text.replace('\\"', '"')
                    
                    # JSON ê°ì²´ ì¶”ì¶œ
                    json_match = re.search(r'(\{.*\})', response_text, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(1)
                    else:
                        print("Could not find JSON object in response")
                        return [], []
                    
                    # JSON íŒŒì‹± ì‹œë„
                    evidence_data = json.loads(response_text)
                    
                    # í•„ë“œëª… ì •ê·œí™”
                    evidence_data = {k.lower().replace('_', ''): v for k, v in evidence_data.items()}
                    
                    # LLMì´ ì¶”ì¶œí•œ evidence í† í° ê°€ì ¸ì˜¤ê¸°
                    evidence_tokens = evidence_data.get('evidence', [])
                    
                    if not evidence_tokens:
                        print("No evidence tokens found in LLM response")
                        return [], []
                    
                    # ì½”ë“œë¡œ evidence í† í°ì˜ ì¸ë±ìŠ¤ ê³„ì‚°
                    evidence_indices = calculate_evidence_indices(evidence_tokens, decoded_tokens)
                    
                    # ê²°ê³¼ ê²€ì¦
                    if not evidence_indices:
                        print(f"Warning: Could not find indices for evidence tokens: {evidence_tokens}")
                        return [], []
                    
                    # ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                    if any(not isinstance(i, int) or i < 0 or i >= len(decoded_tokens) for i in evidence_indices):
                        print(f"Warning: Invalid indices found: {evidence_indices}")
                        return [], []
                    
                    print(f"LLM extracted {len(evidence_tokens)} evidence tokens: {evidence_tokens}")
                    print(f"Code calculated indices: {evidence_indices}")
                    return evidence_indices, evidence_tokens
                    
                except json.JSONDecodeError as e:
                    print(f"Error parsing response: {str(e)}\nResponse: {response_text}")
                    return [], []
            else:
                print("No response field in API result")
                return [], []
        else:
            print(f"API request failed with status code: {response.status_code}")
            return [], []
    except Exception as e:
        print(f"Error during evidence extraction: {str(e)}")
        return [], []

def load_tokenizer(model_key):
    """Load tokenizer for the given model with caching"""
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ìºì‹œëœ í† í¬ë‚˜ì´ì € í™•ì¸
    cache_key = f"tokenizer_{model_key}"
    if cache_key in st.session_state:
        return st.session_state[cache_key]
    
    try:
        # ëª¨ë¸ í‚¤ì—ì„œ ê¸°ë³¸ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: gemma:7b -> gemma, deepseek-r1:7b -> deepseek-r1)
        base_model = model_key.split(":")[0]
        
        # ë””ë²„ê¹… ì •ë³´
        st.info(f"ğŸ” ëª¨ë¸ í‚¤: {model_key}")
        st.info(f"ğŸ” ê¸°ë³¸ ëª¨ë¸ëª…: {base_model}")
        st.info(f"ğŸ” ì§€ì›ë˜ëŠ” ëª¨ë¸ë“¤: {list(MODEL_TOKENIZER_MAP.keys())}")
        
        # MODEL_TOKENIZER_MAPì—ì„œ í† í¬ë‚˜ì´ì € ì´ë¦„ ì°¾ê¸°
        # 1. ì „ì²´ ëª¨ë¸ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„
        tokenizer_name = MODEL_TOKENIZER_MAP.get(model_key)
        st.info(f"ğŸ” ì „ì²´ ëª¨ë¸ëª… ë§¤ì¹­ ì‹œë„: {model_key} -> {tokenizer_name}")
        
        if tokenizer_name:
            st.info(f"ğŸ” ì „ì²´ ëª¨ë¸ëª… ë§¤ì¹­ ì„±ê³µ: {model_key}")
        else:
            # 2. ê¸°ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ ì‹œë„ (íŒŒë¼ë¯¸í„° ìˆ˜ ë¬´ì‹œ)
            tokenizer_name = MODEL_TOKENIZER_MAP.get(base_model)
            st.info(f"ğŸ” ê¸°ë³¸ ëª¨ë¸ëª… ë§¤ì¹­ ì‹œë„: {base_model} -> {tokenizer_name}")
            
            if tokenizer_name:
                st.info(f"ğŸ” ê¸°ë³¸ ëª¨ë¸ëª… ë§¤ì¹­ ì„±ê³µ: {base_model}")
        
        st.info(f"ğŸ” ìµœì¢… ì°¾ì€ í† í¬ë‚˜ì´ì €: {tokenizer_name}")
        
        if tokenizer_name:
            # Hugging Face í† í° í™•ì¸
            hf_token = os.getenv('HUGGINGFACE_TOKEN')
            
            # deepseek-r1 ëª¨ë¸ì€ íŠ¹ë³„í•œ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            if "deepseek-r1" in model_key.lower():
                st.info(f"ğŸ” deepseek-r1 ëª¨ë¸ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            # íŠ¹ë³„í•œ ì„¤ì •ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
            elif "qwen" in model_key.lower():
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            elif "gemma" in model_key.lower():
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.padding_side = "right"
            elif "llama" in model_key.lower():
                # Llama ëª¨ë¸ì€ í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    st.warning("Llama ëª¨ë¸ì— ì ‘ê·¼í•˜ë ¤ë©´ HUGGINGFACE_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                    return None
            else:
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            
            # ì„¸ì…˜ ìƒíƒœì— í† í¬ë‚˜ì´ì € ìºì‹œ
            st.session_state[cache_key] = tokenizer
            return tokenizer
        
        st.error(f"Tokenizer not found for model {model_key}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
        return None
    except Exception as e:
        st.error(f"Error loading tokenizer: {str(e)}")
        return None

def generate_domain_prompt(domain: str, model_key: str) -> str:
    """ë„ë©”ì¸ë³„ë¡œ origin í´ë”ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    origin_prompts = load_origin_prompts()
    
    # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í™•ì¸
    if model_key in origin_prompts and domain in origin_prompts[model_key] and origin_prompts[model_key][domain]:
        return random.choice(origin_prompts[model_key][domain])
    else:
        # ë„ë©”ì¸ë³„ êµ¬ì²´ì ì¸ ìš”ì²­ í”„ë¡¬í”„íŠ¸
        domain_prompts = {
            "Medical": "Generate only one medical question a patient might ask a doctor. Respond with a single, clear question sentence only. Do not include explanations, lists, or any other text.",
            "Legal": "Generate only one legal question someone might ask a lawyer. Respond with a single, clear question sentence only. Do not include explanations, lists, or any other text.",
            "Technical": "Generate only one technical question about computers, software, or technology. Respond with a single, clear question sentence only. Do not include explanations, lists, or any other text.",
            "Economy": "Generate only one economic question about markets, finance, or business. Respond with a single, clear question sentence only. Do not include explanations, lists, or any other text."
        }
        
        request_prompt = domain_prompts.get(domain, f"Generate only one question about {domain}. Respond with a single, clear question sentence only. Do not include explanations, lists, or any other text.")
        
        try:
            response = get_model_response(model_key, request_prompt)
            
            # ì‘ë‹µì´ ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
            if not response:
                print(f"Empty or invalid response from model {model_key}")
                return ""
            
            # ì‘ë‹µ ì •ë¦¬ ë° ê²€ì¦
            if not response or not response.strip():
                print(f"Empty response from model {model_key}")
                return ""
            
            # ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì¶”ì¶œ
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            if not lines:
                print(f"No valid lines in response from model {model_key}")
                return ""
            
            prompt = lines[0]
            
            # í”„ë¡¬í”„íŠ¸ ê²€ì¦ (ë” ê´€ëŒ€í•œ ì¡°ê±´)
            if len(prompt) < 5:  # ìµœì†Œ ê¸¸ì´ë¥¼ 5ìë¡œ ì¤„ì„
                print(f"Prompt too short: {prompt}")
                return ""
            
            # ëª…í™•íˆ ì˜ëª»ëœ ì‘ë‹µë§Œ í•„í„°ë§
            invalid_starts = ('please enter', 'error', 'failed', 'i cannot', 'i am unable', 'i do not have')
            if prompt.lower().startswith(invalid_starts):
                print(f"Invalid prompt generated: {prompt}")
                return ""
            
            return prompt
        except Exception as e:
            print(f"Error generating prompt: {str(e)}")
            return ""

def show():
    st.title("ğŸ“ Domain Prompt Generator")
    st.markdown("ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ëª¨ë¸ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.")
    
    # ê°•ì œ ìºì‹œ ë¬´íš¨í™” (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
    if st.sidebar.button("ğŸ”„ ê°•ì œ ìƒˆë¡œê³ ì¹¨", key="force_refresh_dataset"):
        # ëª¨ë“  ìºì‹œ ë¬´íš¨í™”
        get_running_models.clear()
        get_available_models.clear()
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        keys_to_remove = [key for key in st.session_state.keys() if key.startswith(('tokenizer_', 'origin_prompts_cache', 'generated_prompts', 'prompt_generation_complete'))]
        for key in keys_to_remove:
            del st.session_state[key]
        st.sidebar.success("ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ!")
        st.rerun()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'dataset_generator_initialized' not in st.session_state:
        st.session_state.dataset_generator_initialized = True
    
    # ===== ëª¨ë¸ ì„ íƒ ì„¹ì…˜ =====
    st.markdown("---")
    st.subheader("ğŸ¤– Model Selection")
    
    # Ollama ëª¨ë¸ ëª©ë¡ (ìºì‹œë¨)
    with st.spinner("ğŸ”„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•˜ëŠ” ì¤‘..."):
        models = get_available_models()
    
    # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
    st.info(f"ğŸ” ë°œê²¬ëœ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
    if models:
        st.info(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡: {', '.join(models)}")
    
    if not models:
        st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Model Load íƒ­ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ í•´ê²° ë°©ë²•:")
        st.info("1. Model Load íƒ­ì—ì„œ 'ğŸ”„ ìƒˆë¡œê³ ì¹¨' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
        st.info("2. ë˜ëŠ” ì´ í˜ì´ì§€ì—ì„œ 'ğŸ”„ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
        return
    
    # ëª¨ë¸ ì„ íƒê³¼ ìƒíƒœ í‘œì‹œë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if models:
            st.markdown("**ğŸ”§ ëª¨ë¸ ì„ íƒ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)**")
            selected_models = st.multiselect(
                "ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ë“¤",
                models,
                default=[models[0]] if models else [],
                help="ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
            )
            
            if selected_models:
                st.info(f"ì„ íƒëœ ëª¨ë¸: {', '.join(selected_models)}")
                model_key = selected_models[0].lower()  # ì²« ë²ˆì§¸ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ
            else:
                st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                model_key = None
        else:
            st.warning("âš ï¸ ì„ íƒí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            model_key = None
            selected_models = []
    
    with col2:
        if selected_models:
            st.markdown("**ğŸ“Š ëª¨ë¸ ìƒíƒœ í™•ì¸**")
            
            # ì„ íƒëœ ëª¨ë¸ë“¤ì˜ ìƒíƒœ í™•ì¸
            for model in selected_models:
                model_lower = model.lower()
                model_status = check_ollama_model_status_fast(model_lower)
                
                # í† í¬ë‚˜ì´ì € ë¡œë“œ ìƒíƒœ í‘œì‹œ
                tokenizer = load_tokenizer(model_lower)
                
                if tokenizer:
                    if model_lower == model_key:  # í˜„ì¬ ì„ íƒëœ ëª¨ë¸
                        st.success(f"âœ… {model} (í† í¬ë‚˜ì´ì € ë¡œë“œë¨)")
                    else:
                        st.info(f"â„¹ï¸ {model} (í† í¬ë‚˜ì´ì € ë¡œë“œë¨)")
                else:
                    st.warning(f"âš ï¸ {model} (í† í¬ë‚˜ì´ì € ì—†ìŒ)")
                
                # ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ í‘œì‹œ
                if model_status:
                    st.success(f"ğŸŸ¢ {model} (ì‹¤í–‰ ì¤‘)")
                else:
                    st.warning(f"ğŸ”´ {model} (ë¯¸ì‹¤í–‰)")
        else:
            st.warning("âš ï¸ ëª¨ë¸ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")
            tokenizer = None
    
    # ===== ë„ë©”ì¸ ì„¤ì • ì„¹ì…˜ =====
    st.markdown("---")
    st.subheader("ğŸ¯ Domain Configuration")
    
    domains = ["Medical", "Legal", "Technical", "Economy"]
    
    # ìƒì„± ëª¨ë“œì™€ ë„ë©”ì¸ ì„ íƒì„ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜
    col3, col4 = st.columns([1, 1])
    
    with col3:
        generation_mode = st.radio(
            "Generation Mode",
            ["Single Domain", "All Domains"],
            help="ë‹¨ì¼ ë„ë©”ì¸ ë˜ëŠ” ëª¨ë“  ë„ë©”ì¸ì— ëŒ€í•´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤."
        )
    
    with col4:
        if generation_mode == "Single Domain":
            selected_domain = st.selectbox(
                "Select domain",
                domains,
                key="domain_selector"
            )
            selected_domains = [selected_domain]
        else:
            selected_domain = domains[0]  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ë„ë©”ì¸ ì„ íƒ
            selected_domains = domains
            st.info(f"ëª¨ë“  ë„ë©”ì¸ ì„ íƒë¨: {', '.join(domains)}")
    
    # ===== ì„¹ì…˜ 1: ë„ë©”ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± =====
    st.markdown("---")
    st.markdown("## ğŸ”¥ STEP 1: Domain Prompt Generation")
    st.markdown("### ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ëª¨ë¸ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„± ì„¤ì •
    col5, col6 = st.columns([1, 2])
    
    with col5:
        num_prompts = st.number_input(
            "Number of prompts per domain",
            min_value=1,
            max_value=10000,
            value=5,
            step=1,
            help="ê° ë„ë©”ì¸ë³„ë¡œ ìƒì„±í•  í”„ë¡¬í”„íŠ¸ì˜ ê°œìˆ˜ (ìµœëŒ€ 10000ê°œ)"
        )
    
    with col6:
        if generation_mode == "All Domains":
            total_prompts = len(domains) * num_prompts
            st.metric("ì´ ìƒì„±ë  í”„ë¡¬í”„íŠ¸", f"{total_prompts}ê°œ")
        else:
            st.metric("ìƒì„±ë  í”„ë¡¬í”„íŠ¸", f"{num_prompts}ê°œ")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„± ë²„íŠ¼
    col7, col8 = st.columns([2, 1])
    
    with col7:
        generate_disabled = not model_key or not tokenizer
        generate_prompts_button = st.button("ğŸ“ Generate Prompts", type="primary", key="generate_prompts", disabled=generate_disabled)
    
    with col8:
        col8_1, col8_2 = st.columns(2)
        with col8_1:
            if st.button("ğŸ”„ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", key="refresh_models_dataset"):
                get_available_models.clear()
                st.success("ëª¨ë¸ ëª©ë¡ì´ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
        with col8_2:
            if st.button("ğŸ” ëª¨ë¸ ìƒíƒœ í™•ì¸", key="check_model_status"):
                if model_key:
                    # ìºì‹œ ë¬´íš¨í™” í›„ ìƒíƒœ í™•ì¸
                    get_running_models.clear()
                    status = check_ollama_model_status_fast(model_key)
                    if status:
                        st.success(f"âœ… {model_key} ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!")
                    else:
                        st.warning(f"âš ï¸ {model_key} ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")
                        st.info("ğŸ’¡ Model Load íƒ­ì—ì„œ ëª¨ë¸ì„ ì‹œì‘í•œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    st.warning("ëª¨ë¸ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”", key="clear_cache_dataset"):
        # ìºì‹œ í•¨ìˆ˜ë“¤ ì´ˆê¸°í™”
        get_available_models.clear()
        # ì„¸ì…˜ ìƒíƒœ ìºì‹œ ì´ˆê¸°í™”
        keys_to_remove = [key for key in st.session_state.keys() if key.startswith(('tokenizer_', 'origin_prompts_cache'))]
        for key in keys_to_remove:
            del st.session_state[key]
        st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ===== í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤í–‰ =====
    if generate_prompts_button:
        if not selected_models:
            st.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì„ íƒëœ ëª¨ë¸ë“¤ì˜ í† í¬ë‚˜ì´ì €ì™€ ìƒíƒœ í™•ì¸
        valid_models = []
        for model in selected_models:
            model_lower = model.lower()
            tokenizer = load_tokenizer(model_lower)
            model_status = check_ollama_model_status_fast(model_lower)
            
            if not tokenizer:
                st.warning(f"âš ï¸ Tokenizer not found for model {model}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
                continue
            
            if not model_status:
                st.warning(f"âš ï¸ Model {model}ê°€ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")
                continue
            
            valid_models.append(model)
        
        if not valid_models:
            st.error("âŒ ìœ íš¨í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì™€ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            total_models = len(valid_models)
            total_domains = len(selected_domains)
            
            # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
            progress_text = st.empty()
            progress_counter = st.empty()
            time_info = st.empty()
            
            # ì „ì²´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            total_start_time = time.time()
            
            # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë“¤ì„ ì €ì¥í•  ì„ì‹œ ë°ì´í„° (ëª¨ë¸ë³„ë¡œ êµ¬ë¶„)
            generated_prompts = {}
            
            # ëª¨ë¸ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
            for model_idx, model in enumerate(valid_models, 1):
                model_lower = model.lower()
                model_start_time = time.time()
                
                progress_text.text(f"Processing model {model} ({model_idx}/{total_models})...")
                
                # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ì €ì¥ì†Œ ì´ˆê¸°í™”
                generated_prompts[model] = {}
                
                # ë„ë©”ì¸ë³„ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                for domain_idx, domain in enumerate(selected_domains, 1):
                    domain_start_time = time.time()
                    
                    progress_text.text(f"Generating prompts for {model}/{domain} ({model_idx}/{total_models}, {domain_idx}/{total_domains})...")
                    
                    with st.spinner(f"Generating {num_prompts} prompts for {model}/{domain}..."):
                        # ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                        generated_prompts[model][domain] = []
                        used_prompts = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
                        
                        i = 0
                        while len(generated_prompts[model][domain]) < num_prompts:
                            # ì§„í–‰ìƒí™© ì¹´ìš´í„° ì—…ë°ì´íŠ¸
                            current_count = len(generated_prompts[model][domain])
                            progress_counter.text(f"{model}/{domain}: {current_count}/{num_prompts} (attempt {i+1})")
                            
                            # ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
                            current_time = time.time()
                            elapsed_time = current_time - total_start_time
                            
                            # ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
                            total_prompts_to_generate = total_models * total_domains * num_prompts
                            completed_prompts = (model_idx - 1) * total_domains * num_prompts + (domain_idx - 1) * num_prompts + i
                            if completed_prompts > 0:
                                avg_time_per_prompt = elapsed_time / completed_prompts
                                remaining_prompts = total_prompts_to_generate - completed_prompts
                                estimated_remaining_time = avg_time_per_prompt * remaining_prompts
                                estimated_total_time = elapsed_time + estimated_remaining_time
                                
                                time_info.text(f"ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ | ì˜ˆìƒì™„ë£Œ: {estimated_total_time:.1f}ì´ˆ | ë‚¨ì€ì‹œê°„: {estimated_remaining_time:.1f}ì´ˆ")
                            else:
                                time_info.text(f"ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ | ì˜ˆìƒì™„ë£Œ: ê³„ì‚° ì¤‘...")
                            
                            # Generate new prompt for the domain
                            print(f"Generating prompt for {domain} with {model_lower}...")
                            prompt = generate_domain_prompt(domain, model_lower)
                            print(f"Generated prompt: {prompt}")
                            
                            # í”„ë¡¬í”„íŠ¸ê°€ Noneì´ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ì¬ì‹œë„
                            retry_count = 0
                            max_retries = 3
                            while (not prompt or prompt == "ERROR" or 
                                   prompt.lower().startswith(('please enter', 'error', 'failed', 'i cannot', 'i am unable'))) and retry_count < max_retries:
                                print(f"Invalid prompt generated, retrying... (attempt {retry_count + 1}/{max_retries})")
                                # ì¬ì‹œë„ ì‹œì—ëŠ” ë‹¤ë¥¸ ìš”ì²­ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                                if retry_count == 1:
                                    request_prompt = f"Ask a question about {domain.lower()} topics."
                                elif retry_count == 2:
                                    request_prompt = f"What would you like to know about {domain.lower()}?"
                                else:
                                    request_prompt = f"Generate a {domain.lower()} question."
                                
                                try:
                                    response = get_model_response(model_lower, request_prompt)
                                    if response and response.strip():
                                        lines = [line.strip() for line in response.split('\n') if line.strip()]
                                        if lines and len(lines[0]) >= 5:
                                            prompt = lines[0]
                                        else:
                                            prompt = ""
                                    else:
                                        prompt = ""
                                except:
                                    prompt = ""
                                retry_count += 1
                            
                            # ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬
                            if not prompt or prompt == "ERROR" or prompt.lower().startswith(('please enter', 'error', 'failed', 'i cannot', 'i am unable')):
                                print(f"Failed to generate valid prompt after {max_retries} attempts, marking as empty")
                                prompt = ""
                            
                            # DeepSeek ëª¨ë¸ì˜ ê²½ìš° <think> íƒœê·¸ ì œê±° ë° ê²€ì¦
                            if "deepseek" in model_lower:
                                original_prompt = prompt
                                prompt = clean_deepseek_response(prompt)
                                print(f"DeepSeek response cleaned: {prompt[:100]}...")  # ë””ë²„ê¹…ìš©
                                
                                # ê³µë°±ì´ê±°ë‚˜ <think> íƒœê·¸ë§Œ ë‚¨ì€ ê²½ìš° ë‹¤ì‹œ ìš”ì²­
                                if not prompt.strip() or prompt.strip().lower().startswith('<think>'):
                                    print(f"DeepSeek response is empty or only contains think tags, retrying...")
                                    prompt = ""
                            
                            # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (ë”°ì˜´í‘œ ì œê±°)
                            prompt = clean_prompt_text(prompt)
                            print(f"Final prompt cleaned: {prompt[:100]}...")  # ë””ë²„ê¹…ìš©
                            
                            # ì¤‘ë³µ ì œê±°: ì´ë¯¸ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸
                            if prompt in used_prompts:
                                print(f"Duplicate prompt detected: {prompt[:50]}...")
                                # ì¤‘ë³µëœ ê²½ìš° ë‹¤ì‹œ ìƒì„± ì‹œë„
                                retry_count = 0
                                while prompt in used_prompts and retry_count < 3:
                                    print(f"Generating alternative prompt (attempt {retry_count + 1})")
                                    new_prompt = generate_domain_prompt(domain, model_lower)
                                    if new_prompt and new_prompt not in used_prompts:
                                        prompt = clean_prompt_text(new_prompt)
                                        break
                                    retry_count += 1
                            
                            # ê³µë°±ì¸ ê²½ìš° ë‹¤ì‹œ ìš”ì²­ (ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ 10íšŒ)
                            retry_for_empty = 0
                            while (not prompt or not prompt.strip()) and retry_for_empty < 10:
                                print(f"Empty prompt generated, retrying... (attempt {retry_for_empty + 1}/10)")
                                prompt = generate_domain_prompt(domain, model_lower)
                                if prompt:
                                    # DeepSeek ëª¨ë¸ì˜ ê²½ìš° <think> íƒœê·¸ ì œê±° ë° ê²€ì¦
                                    if "deepseek" in model_lower:
                                        original_prompt = prompt
                                        prompt = clean_deepseek_response(prompt)
                                        print(f"DeepSeek response cleaned: {prompt[:100]}...")  # ë””ë²„ê¹…ìš©
                                        
                                        # ê³µë°±ì´ê±°ë‚˜ <think> íƒœê·¸ë§Œ ë‚¨ì€ ê²½ìš° ë‹¤ì‹œ ìš”ì²­
                                        if not prompt.strip() or prompt.strip().lower().startswith('<think>'):
                                            print(f"DeepSeek response is empty or only contains think tags, retrying...")
                                            prompt = ""
                                            retry_for_empty += 1
                                            continue
                                    
                                    # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (ë”°ì˜´í‘œ ì œê±°)
                                    prompt = clean_prompt_text(prompt)
                                    print(f"Final prompt cleaned: {prompt[:100]}...")  # ë””ë²„ê¹…ìš©
                                    
                                    # ì¤‘ë³µ ì œê±°: ì´ë¯¸ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸
                                    if prompt in used_prompts:
                                        print(f"Duplicate prompt detected: {prompt[:50]}...")
                                        # ì¤‘ë³µëœ ê²½ìš° ë‹¤ì‹œ ìƒì„± ì‹œë„
                                        retry_count = 0
                                        while prompt in used_prompts and retry_count < 3:
                                            print(f"Generating alternative prompt (attempt {retry_count + 1})")
                                            new_prompt = generate_domain_prompt(domain, model_lower)
                                            if new_prompt and new_prompt not in used_prompts:
                                                prompt = clean_prompt_text(new_prompt)
                                                break
                                            retry_count += 1
                                retry_for_empty += 1
                            
                            # ìµœì¢…ì ìœ¼ë¡œ ê³µë°±ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì €ì¥
                            if prompt.strip():
                                # í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©ëœ ëª©ë¡ì— ì¶”ê°€
                                used_prompts.add(prompt)
                                
                                # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì €ì¥
                                prompt_data = {
                                    "prompt": prompt,
                                    "model": model_lower,
                                    "domain": domain,
                                    "index": len(generated_prompts[model][domain]) + 1
                                }
                                
                                generated_prompts[model][domain].append(prompt_data)
                                print(f"Successfully added prompt: {prompt[:50]}...")
                            else:
                                print(f"Failed to generate valid prompt after all retries, skipping...")
                            
                            i += 1
                        
                        # ë„ë©”ì¸ë³„ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
                        domain_end_time = time.time()
                        domain_duration = domain_end_time - domain_start_time
                        print(f"{model}/{domain} prompts completed in {domain_duration:.2f} seconds")
                
                # ëª¨ë¸ë³„ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
                model_end_time = time.time()
                model_duration = model_end_time - model_start_time
                print(f"{model} model completed in {model_duration:.2f} seconds")
            
            # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
            total_end_time = time.time()
            total_duration = total_end_time - total_start_time
            
            # ì§„í–‰ìƒí™© í‘œì‹œ ì •ë¦¬
            progress_text.empty()
            progress_counter.empty()
            time_info.empty()
            
            # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¸ì…˜ì— ì €ì¥
            st.session_state.generated_prompts = generated_prompts
            st.session_state.prompt_generation_complete = True
            
            # ===== í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥ =====
            st.markdown("---")
            st.subheader("ğŸ’¾ Saving Generated Prompts")
            
            # ëª¨ë¸ë³„, ë„ë©”ì¸ë³„ë¡œ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            saved_files = []
            
            for model, model_prompts in generated_prompts.items():
                for domain, prompts in model_prompts.items():
                    if prompts:
                        # Create output directory (ëª¨ë¸ë³„ êµ¬ì¡°)
                        output_dir = Path(f"dataset/origin/{model.lower()}/{domain.lower()}")
                        output_dir.mkdir(parents=True, exist_ok=True)
                        
                        # Create output file
                        output_path = output_dir / f"{domain.lower()}_{len(prompts)}prompts_{timestamp}.jsonl"
                        
                        # Save to file
                        with open(output_path, "w", encoding="utf-8") as f:
                            for prompt_data in prompts:
                                f.write(json.dumps(prompt_data, ensure_ascii=False) + "\n")
                        
                        saved_files.append((model, domain, output_path, len(prompts)))
            
            # ===== ì €ì¥ ê²°ê³¼ í‘œì‹œ =====
            st.markdown("---")
            st.subheader("âœ… Prompt Generation Complete!")
            
            # ê²°ê³¼ ìš”ì•½ì„ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜
            col13, col14, col15 = st.columns(3)
            
            with col13:
                st.metric("ì´ ì†Œìš” ì‹œê°„", f"{total_duration:.1f}ì´ˆ")
            
            with col14:
                st.metric("ì²˜ë¦¬ëœ ëª¨ë¸", f"{total_models}ê°œ")
            
            with col15:
                total_generated = total_models * total_domains * num_prompts
                st.metric("ìƒì„±ëœ í”„ë¡¬í”„íŠ¸", f"{total_generated}ê°œ")
            
            # ìƒì„±ëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“‹ Generated Files")
            
            for model, domain, file_path, count in saved_files:
                with st.container():
                    col16, col17 = st.columns([3, 1])
                    with col16:
                        st.write(f"ğŸ“„ **{model}/{domain}**: `{file_path.name}` ({count}ê°œ í”„ë¡¬í”„íŠ¸)")
                    with col17:
                        st.write(f"ğŸ“ `{file_path.parent}`")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë³´
            st.info(f"ğŸ“ ëª¨ë“  íŒŒì¼ì´ `dataset/origin/[ëª¨ë¸ëª…]/[ë„ë©”ì¸ëª…]/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.success("ğŸ‰ í”„ë¡¬í”„íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ Evidence ì¶”ì¶œ í˜ì´ì§€ì—ì„œ evidenceë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.markdown("---")
            st.subheader("âŒ Prompt Generation Failed")
            
            # ì—ëŸ¬ ì •ë³´ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜
            col15, col16 = st.columns([2, 1])
            
            with col15:
                st.error(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:")
                st.code(str(e))
            
            with col16:
                st.warning("ğŸ’¡ í•´ê²° ë°©ë²•:")
                st.write("1. ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
                st.write("2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
                st.write("3. ìºì‹œë¥¼ ì´ˆê¸°í™”í•˜ê³  ì¬ì‹œë„")
            
            print(f"Prompt generation error: {str(e)}")
    
