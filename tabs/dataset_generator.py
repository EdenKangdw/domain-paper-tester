import streamlit as st
from pathlib import Path
import json
from transformers import AutoTokenizer
from utils import check_ollama_model_status, OLLAMA_API_BASE
import requests
import random
from typing import List, Tuple
import re
from datetime import datetime
import time
import os

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    st.warning("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

# Model tokenizer settings
MODEL_TOKENIZER_MAP = {
    "llama2": "meta-llama/Llama-2-7b-hf",
    "gemma:2b": "google/gemma-2b",
    "gemma:7b": "google/gemma-7b",
    "qwen": "Qwen/Qwen-7B",
    "deepseek": "deepseek-ai/deepseek-coder-7b-base",
    "mistral": "mistralai/Mistral-7B-v0.1",
    "mixtral": "mistralai/Mixtral-8x7B-v0.1",
    "yi": "01-ai/Yi-6B",
    "openchat": "openchat/openchat",
    "neural": "neural-chat/neural-chat-7b-v3-1",
    "phi": "microsoft/phi-2",
    "stable": "stabilityai/stable-code-3b"
}

def get_running_models():
    """Get list of currently running Ollama models"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            running_models = []
            for model in models:
                if check_ollama_model_status(model["name"]):
                    running_models.append(model["name"])
            return running_models
        return []
    except:
        return []

def get_model_response(model_name, prompt):
    """Get response from the model"""
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        response.raise_for_status()
        
        # ì‘ë‹µì´ JSONì¸ ê²½ìš°
        try:
            json_response = response.json()
            if isinstance(json_response, dict) and "response" in json_response:
                return json_response["response"]
        except json.JSONDecodeError:
            pass
            
        # ì‘ë‹µì´ ë°”ì´íŠ¸ì¸ ê²½ìš°
        if isinstance(response.content, bytes):
            return response.content.decode('utf-8')
            
        return str(response.text)
    except Exception as e:
        return f"Error occurred: {str(e)}"

def tokenize_and_extract_words(text, tokenizer):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ë‹¨ì–´ë¥¼ ì¶”ì¶œ"""
    print("\n=== Tokenization Debug ===")
    print(f"Original text: {text}")
    
    tokens = tokenizer.tokenize(text)
    print(f"Tokens: {tokens}")
    
    # êµ¬ë‘ì  ì œê±°ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def clean_word(word):
        # ë‹¨ì–´ ëì˜ êµ¬ë‘ì  ì œê±° (ì‰¼í‘œ, ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±)
        return word.rstrip(',.!?:;')
    
    # ë‹¨ì–´ ë¶„ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def split_into_words(text):
        # ê¸°ë³¸ì ì¸ ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
        raw_words = []
        current_word = ""
        for char in text:
            if char.isspace():
                if current_word:
                    raw_words.append(current_word)
                    current_word = ""
            else:
                current_word += char
        if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬
            raw_words.append(current_word)
        
        # êµ¬ë‘ì  ì œê±° ë° ì •ë¦¬
        words = [clean_word(word) for word in raw_words]
        # ë¹ˆ ë¬¸ìì—´ ì œê±°
        words = [word for word in words if word.strip()]
        return words
    
    # í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë‹¨ì–´ë¡œ ë¶„ë¦¬
    words = split_into_words(text)
    print(f"Split words: {words}")
    
    # ê° ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ ì°¾ê¸°
    word_positions = []
    current_pos = 0
    text_lower = text.lower()
    
    for word in words:
        word_lower = word.lower()
        # í˜„ì¬ ìœ„ì¹˜ë¶€í„° ë‹¨ì–´ ì°¾ê¸°
        while current_pos < len(text):
            pos = text_lower.find(word_lower, current_pos)
            if pos != -1:
                word_positions.append((pos, pos + len(word), word))
                current_pos = pos + len(word)
                break
            current_pos += 1
    
    # ë‹¨ì–´ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    word_positions.sort(key=lambda x: x[0])
    
    # ì •ë ¬ëœ ìˆœì„œë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¬êµ¬ì„±
    words = [wp[2] for wp in word_positions]
    
    # í† í°ê³¼ ë‹¨ì–´ ë§¤í•‘
    word_to_tokens = {}
    current_token_pos = 0
    current_token_idx = 0
    
    for start, end, word in word_positions:
        token_indices = []
        token_text = ""
        
        while current_token_idx < len(tokens) and current_token_pos <= end:
            token = tokens[current_token_idx].replace('â–', '')
            if token.strip():
                token_text += token
                if current_token_pos >= start and current_token_pos < end:
                    token_indices.append(current_token_idx)
                current_token_pos += len(token)
            current_token_idx += 1
        
        if token_indices:
            word_to_tokens[word] = token_indices
    
    print("\n=== Word Processing Results ===")
    print(f"Final words: {words}")
    print("Word to token mapping:")
    for word, token_indices in word_to_tokens.items():
        token_values = [tokens[idx] for idx in token_indices]
        print(f"Word: {word}")
        print(f"Token indices: {token_indices}")
        print(f"Token values: {token_values}\n")
    
    return tokens, words, word_to_tokens

def format_word_and_token_info(tokens, words, word_to_tokens):
    """Format token and word information"""
    token_entries = [
        f'Token[{i}] = >>>{token}<<< (raw: {repr(token)})'
        for i, token in enumerate(tokens)
    ]
    
    word_entries = [
        f'Word[{i}] = >>>{word}<<<'
        for i, word in enumerate(words)
    ]
    
    return "\n".join(token_entries), "\n".join(word_entries)

def create_evidence_query(word_list, prompt, domain):
    """Evidence ì¶”ì¶œì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    # ë‹¨ì–´ ëª©ë¡ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
    words = [word.strip() for word in word_list.split('\n') if word.strip()]
    
    # ë‹¨ì–´ ëª©ë¡ì„ í‘œ í˜•ì‹ìœ¼ë¡œ ìƒì„±
    word_table = []
    for i, word in enumerate(words):
        word_table.append(f"| {i} | {word} |")
    word_table = "| Index | Word |\n|-------|------|\n" + "\n".join(word_table)

    return f"""You are a JSON API that extracts evidence tokens from text. Follow these instructions exactly:

1. From the token list below, identify tokens related to the '{domain}' domain.
2. Return ONLY a JSON object with this exact format:
{{
    "evidence_token_index": [numbers],
    "evidence": [tokens]
}}

Token List:
{word_table}

Input Text: "{prompt}"

Rules:
- evidence_token_index must be an array of numbers
- evidence must be an array of exact tokens from the list
- arrays must have the same length
- do not add any explanation or text outside the JSON
- do not modify or format the tokens
- do not use markdown or code blocks
"""

def extract_json_from_response(response):
    """Extract JSON from response"""
    import re
    # ë¬¸ìì—´ë¡œ ë³€í™˜ ë³´ì¥
    if not isinstance(response, str):
        response = str(response)
    
    # ì‘ë‹µì´ ì´ë¯¸ JSON ê°ì²´ì¸ ê²½ìš°
    if isinstance(response, dict):
        return response

    try:
        # ë¨¼ì € ì „ì²´ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
        return json.loads(response)
    except json.JSONDecodeError:
        try:
            # JSON í˜•ì‹ì˜ ë¬¸ìì—´ì„ ì°¾ìŒ (ì•ë’¤ì˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°)
            json_match = re.search(r'(\{(?:[^{}]|(?:\{[^{}]*\}))*\})', response)
            if not json_match:
                raise ValueError("Could not find JSON format response")
            
            json_str = json_match.group(1)
            
            # JSON ë¬¸ìì—´ ì •ë¦¬
            # 1. ì¤„ë°”ê¿ˆê³¼ ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€ê²½
            json_str = re.sub(r'\s+', ' ', json_str).strip()
            
            # 2. ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì°¾ì•„ì„œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            # ë¨¼ì € ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œë¥¼ ì„ì‹œ ì¹˜í™˜
            json_str = json_str.replace('\\"', '___ESCAPED_QUOTE___')
            # ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            json_str = re.sub(r'(?<!\\)"([^"]*)"', r'"\1"', json_str)
            # ì„ì‹œ ì¹˜í™˜ëœ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œ ë³µì›
            json_str = json_str.replace('___ESCAPED_QUOTE___', '\\"')
            
            # 3. ì‘ì€ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë¬¸ìì—´ì„ í°ë”°ì˜´í‘œë¡œ ë³€ê²½
            json_str = re.sub(r"'([^']*)'", r'"\1"', json_str)
            
            # 4. í°ë”°ì˜´í‘œ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*"\s*', '"', json_str)
            
            # 5. ì½¤ë§ˆ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*,\s*', ',', json_str)
            
            # 6. ì¤‘ê´„í˜¸ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*{\s*', '{', json_str)
            json_str = re.sub(r'\s*}\s*', '}', json_str)
            
            return json.loads(json_str)
        except Exception as e:
            print(f"Failed to parse JSON: {response}")
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸ ì‘ë‹µ: {response}")

def validate_evidence(result, words):
    """Validate evidence results"""
    required_fields = ["evidence_word_index", "evidence", "explanation"]
    missing_fields = [field for field in required_fields if field not in result]
    if missing_fields:
        raise ValueError(f"Missing fields: {', '.join(missing_fields)}")
    
    evidence_word_index = result["evidence_word_index"]
    evidence = result["evidence"]
    
    if not isinstance(evidence_word_index, list):
        raise ValueError("evidence_word_index must be an array ([])")
    if not isinstance(evidence, list):
        raise ValueError("evidence must be an array ([])")
    
    # ë‹¨ì–´ ëª©ë¡ ì •ë¦¬ (ê³µë°± ì œê±°)
    words = [word.strip() for word in words if word.strip()]
    
    # Validate indices
    invalid_indices = []
    for i, idx in enumerate(evidence_word_index):
        if not isinstance(idx, int):
            invalid_indices.append({"position": i, "index": idx, "reason": "not an integer"})
        elif not (0 <= idx < len(words)):
            invalid_indices.append({"position": i, "index": idx, "reason": f"out of range (0-{len(words)-1})"})
    
    if invalid_indices:
        details = [
            f"Position {e['position']}: Index {e['index']} ({e['reason']})"
            for e in invalid_indices
        ]
        raise ValueError(f"Invalid indices found:\n" + "\n".join(details))
    
    # Check if evidence and evidence_word_index lengths match
    if len(evidence) != len(evidence_word_index):
        raise ValueError(f"Array lengths don't match (evidence: {len(evidence)}, index: {len(evidence_word_index)})")
    
    # Check for words not in the list
    invalid_words = []
    for i, word in enumerate(evidence):
        if word not in words:
            invalid_words.append({
                "position": i,
                "word": word,
                "available_words": words
            })
    
    if invalid_words:
        details = [
            f"Position {w['position']}: '{w['word']}' (available words: {w['available_words']})"
            for w in invalid_words
        ]
        raise ValueError(f"Words not in list found:\n" + "\n".join(details))
    
    # Check index and word matching
    mismatches = []
    for i, (idx, word) in enumerate(zip(evidence_word_index, evidence)):
        if words[idx] != word:
            mismatches.append({
                "position": i,
                "index": idx,
                "expected": words[idx],
                "actual": word
            })
    
    if mismatches:
        details = [
            f"Position {m['position']}: Index {m['index']} should be '{m['expected']}' but got '{m['actual']}'"
            for m in mismatches
        ]
        raise ValueError(f"Index and word mismatches found:\n" + "\n".join(details))
    
    return evidence_word_index, evidence

def visualize_evidence(words, evidence_word_index, evidence, explanation):
    """Visualize evidence results"""
    highlighted_words = [
        f"<span style='background-color:#fff176; padding:2px'>{word}</span>"
        if i in evidence_word_index else word
        for i, word in enumerate(words)
    ]
    
    st.markdown("### Extracted Evidence:")
    st.markdown(" ".join(highlighted_words), unsafe_allow_html=True)
    
    # JSON ë°ì´í„°ë¥¼ ë¨¼ì € íŒŒì‹±í•˜ê³  ê²€ì¦
    try:
        json_data = {
            "evidence_word_index": evidence_word_index,
            "evidence": evidence,
            "explanation": explanation
        }
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆë‹¤ê°€ ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬
        json_str = json.dumps(json_data, ensure_ascii=False)
        validated_data = json.loads(json_str)
        st.json(validated_data)
    except json.JSONDecodeError as e:
        st.error(f"JSON ë°ì´í„° ì˜¤ë¥˜: {str(e)}")
        st.code(str(json_data), language="json")

def get_test_prompt(domain: str) -> str:
    """ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    prompts = {
        "Medical": [
            "What are the main side effects of this medication?",
            "What are the contraindications for this treatment?",
            "What are the recommended dosages for this drug?",
            "What are the potential complications of this procedure?",
            "What are the warning signs to watch for?"
        ],
        "Legal": [
            "What are the key clauses in this contract?",
            "What are the main obligations of the parties?",
            "What are the termination conditions?",
            "What are the dispute resolution procedures?",
            "What are the confidentiality requirements?"
        ],
        "Technical": [
            "What is the main functionality of this code?",
            "What are the key features of this system?",
            "What are the system requirements?",
            "What are the performance specifications?",
            "What are the security measures implemented?"
        ],
        "General": [
            "What is the main content of this document?",
            "What are the key points discussed?",
            "What are the main conclusions?",
            "What are the important findings?",
            "What are the main recommendations?"
        ]
    }
    return random.choice(prompts.get(domain, ["Please enter your prompt here..."]))

def extract_evidence_with_ollama(prompt, tokens, model_name):
    """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦ê±° ì¶”ì¶œ"""
    try:
        # í† í°ì´ ë°”ì´íŠ¸ íƒ€ì…ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë””ì½”ë”©
        decoded_tokens = []
        for token in tokens:
            if isinstance(token, bytes):
                try:
                    decoded_tokens.append(token.decode('utf-8'))
                except UnicodeDecodeError:
                    try:
                        decoded_tokens.append(token.decode('latin-1'))
                    except:
                        decoded_tokens.append('')
            else:
                decoded_tokens.append(str(token))

        query = create_evidence_query("\n".join(decoded_tokens), prompt, "Medical")
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": query,
                "stream": False
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            if 'response' in result:
                response_text = result['response']
                print(f"Raw response from model: {response_text}")  # ë””ë²„ê¹…ìš©
                
                try:
                    # JSON ë¬¸ìì—´ ì •ë¦¬
                    response_text = response_text.strip()
                    
                    # ì´ìŠ¤ì¼€ì´í”„ëœ JSON ë¬¸ìì—´ ì²˜ë¦¬
                    if '\\"' in response_text:
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œë¥¼ ì¼ë°˜ ë”°ì˜´í‘œë¡œ ë³€í™˜
                        response_text = response_text.replace('\\"', '"')
                    
                    # JSON ê°ì²´ ì¶”ì¶œ
                    json_match = re.search(r'(\{.*\})', response_text, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(1)
                    else:
                        st.error("Could not find JSON object in response")
                        return [], []
                    
                    # JSON íŒŒì‹± ì‹œë„
                    evidence_data = json.loads(response_text)
                    
                    # í•„ë“œëª… ì •ê·œí™”
                    evidence_data = {k.lower().replace('_', ''): v for k, v in evidence_data.items()}
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ì •ê·œí™”ëœ í•„ë“œëª…ìœ¼ë¡œ)
                    indices = evidence_data.get('evidencetokenindex', evidence_data.get('evidenceindices', []))
                    evidence = evidence_data.get('evidence', [])
                    
                    if not indices or not evidence:
                        st.error("Missing required fields in evidence data")
                        return [], []
                    
                    # ë¬¸ì¥ë¶€í˜¸ ì œê±° ë° ì¸ë±ìŠ¤ ì¡°ì •
                    punctuation_pattern = re.compile(r'[^\w\s]')
                    filtered_indices = []
                    filtered_evidence = []
                    removed_count = 0
                    
                    for i, (idx, token) in enumerate(zip(indices, evidence)):
                        # ë¬¸ì¥ë¶€í˜¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                        if not punctuation_pattern.search(token):
                            # ì´ì „ì— ì œê±°ëœ í† í° ìˆ˜ë§Œí¼ ì¸ë±ìŠ¤ ì¡°ì •
                            adjusted_idx = idx - removed_count
                            filtered_indices.append(adjusted_idx)
                            filtered_evidence.append(token)
                        else:
                            removed_count += 1
                    
                    indices = filtered_indices
                    evidence = filtered_evidence
                    
                    # ì¸ë±ìŠ¤ì™€ í† í° ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(indices) != len(evidence):
                        print(f"Debug - Indices length: {len(indices)}, Evidence length: {len(evidence)}")
                        print(f"Debug - Indices: {indices}")
                        print(f"Debug - Evidence: {evidence}")
                        st.error(f"Number of indices ({len(indices)}) and tokens ({len(evidence)}) do not match")
                        # ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš° ë” ì§§ì€ ìª½ì— ë§ì¶° ìë¥´ê¸°
                        min_length = min(len(indices), len(evidence))
                        indices = indices[:min_length]
                        evidence = evidence[:min_length]
                    
                    # ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                    if any(not isinstance(i, int) or i < 0 or i >= len(tokens) for i in indices):
                        st.error("Invalid indices found in response")
                        return [], []
                    
                    return indices, evidence
                except json.JSONDecodeError as e:
                    print(f"Error parsing response: {str(e)}\nResponse: {response_text}")
                    st.error(f"Evidence extraction failed: Invalid JSON format")
                    return [], []
            else:
                st.error("No response field in API result")
                return [], []
        else:
            st.error(f"API request failed with status code: {response.status_code}")
            return [], []
    except Exception as e:
        st.error(f"Error during evidence extraction: {str(e)}")
        return [], []

def load_tokenizer(model_key):
    """Load tokenizer for the given model"""
    try:
        # ëª¨ë¸ í‚¤ì—ì„œ ê¸°ë³¸ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: gemma:7b -> gemma)
        base_model = model_key.split(":")[0]
        
        # MODEL_TOKENIZER_MAPì—ì„œ í† í¬ë‚˜ì´ì € ì´ë¦„ ì°¾ê¸°
        tokenizer_name = MODEL_TOKENIZER_MAP.get(model_key)  # ì „ì²´ ëª¨ë¸ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„
        if not tokenizer_name:
            tokenizer_name = MODEL_TOKENIZER_MAP.get(base_model)  # ê¸°ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ ì‹œë„
        
        if tokenizer_name:
            # Hugging Face í† í° í™•ì¸
            hf_token = os.getenv('HUGGINGFACE_TOKEN')
            
            # íŠ¹ë³„í•œ ì„¤ì •ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
            if "qwen" in model_key.lower():
                if hf_token:
                    return AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True, token=hf_token)
                else:
                    return AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            elif "gemma" in model_key.lower():
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.padding_side = "right"
                return tokenizer
            elif "llama" in model_key.lower():
                # Llama ëª¨ë¸ì€ í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
                if hf_token:
                    return AutoTokenizer.from_pretrained(tokenizer_name, token=hf_token)
                else:
                    st.warning("Llama ëª¨ë¸ì— ì ‘ê·¼í•˜ë ¤ë©´ HUGGINGFACE_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                    return None
            else:
                if hf_token:
                    return AutoTokenizer.from_pretrained(tokenizer_name, token=hf_token)
                else:
                    return AutoTokenizer.from_pretrained(tokenizer_name)
        
        st.error(f"Tokenizer not found for model {model_key}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
        return None
    except Exception as e:
        st.error(f"Error loading tokenizer: {str(e)}")
        return None

def generate_domain_prompt(domain: str, model_key: str) -> str:
    """ë„ë©”ì¸ë³„ë¡œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"""Generate a new question or prompt related to the {domain} domain.
The prompt should be:
1. Specific to the {domain} domain
2. Clear and concise
3. Focused on extracting key information
4. Natural and professional

Please provide only the prompt without any additional text or explanation."""

    try:
        response = get_model_response(model_key, prompt)
        # ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì¶”ì¶œ
        prompt = response.split('\n')[0].strip()
        return prompt
    except Exception as e:
        print(f"Error generating prompt: {str(e)}")
        return f"Please enter your {domain} domain prompt here..."

def show():
    st.title("Dataset Generator")
    
    # Model selection
    st.subheader("ğŸ¤– Model")
    
    # ëª¨ë¸ ì„ íƒ ë°©ì‹
    model_selection_method = st.radio(
        "ëª¨ë¸ ì„ íƒ ë°©ì‹",
        ["Hugging Face ëª¨ë¸ (í† í¬ë‚˜ì´ì € ì‚¬ìš©)", "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)"],
        horizontal=True,
        key="model_selection_method"
    )
    
    if model_selection_method == "Hugging Face ëª¨ë¸ (í† í¬ë‚˜ì´ì € ì‚¬ìš©)":
        # Hugging Face ëª¨ë¸ ëª©ë¡
        hf_models = list(MODEL_TOKENIZER_MAP.keys())
        selected_model = st.selectbox(
            "Hugging Face ëª¨ë¸ ì„ íƒ",
            hf_models,
            key="hf_model_selector"
        )
        model_key = selected_model
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = load_tokenizer(model_key)
        if not tokenizer:
            st.error(f"í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_key}")
            return
            
        st.success(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_TOKENIZER_MAP[model_key]}")
        
    else:
        # Ollama ëª¨ë¸ ëª©ë¡
        models = get_running_models()
        if not models:
            st.error("ì‹¤í–‰ ì¤‘ì¸ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Model Load íƒ­ì—ì„œ ëª¨ë¸ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            return
        
        selected_model = st.selectbox(
            "ì‹¤í–‰ ì¤‘ì¸ Ollama ëª¨ë¸ ì„ íƒ",
            models,
            key="ollama_model_selector"
        )
        model_key = selected_model.lower()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = load_tokenizer(model_key)
        if not tokenizer:
            st.warning(f"âš ï¸ í† í¬ë‚˜ì´ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_key}")
            st.info("ì§€ì›ë˜ëŠ” ëª¨ë¸: " + ", ".join(MODEL_TOKENIZER_MAP.keys()))
            return
    
    # Domain selection
    st.subheader("ğŸ¯ Domain")
    domains = ["Medical", "Legal", "Technical", "General"]
    
    # Dataset generation settings
    st.subheader("ğŸ“Š Dataset Generation")
    generation_mode = st.radio(
        "Generation Mode",
        ["Single Domain", "All Domains"],
        help="ë‹¨ì¼ ë„ë©”ì¸ ë˜ëŠ” ëª¨ë“  ë„ë©”ì¸ì— ëŒ€í•´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    if generation_mode == "Single Domain":
        selected_domain = st.selectbox(
            "Select domain",
            domains,
            key="domain_selector"
        )
        selected_domains = [selected_domain]
    else:
        selected_domain = domains[0]  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ë„ë©”ì¸ ì„ íƒ
        selected_domains = domains
    
    num_datasets = st.number_input(
        "Number of datasets per domain",
        min_value=1,
        max_value=100000,
        value=5,
        step=1,
        help="ê° ë„ë©”ì¸ë³„ë¡œ ìƒì„±í•  ë°ì´í„°ì…‹ì˜ ê°œìˆ˜"
    )
    
    # Generate dataset button
    if st.button("ğŸ”„ Generate Dataset", key="generate_dataset"):
        if not tokenizer:
            st.warning(f"âš ï¸ Tokenizer not found for model {model_key}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
            return
        
        # Ollama ëª¨ë¸ì¸ ê²½ìš° ì‹¤í–‰ ìƒíƒœ í™•ì¸
        if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
            if not check_ollama_model_status(model_key):
                st.error(f"âŒ Model {model_key} is not running. Please start it in the Model Load tab.")
                return
        
        try:
            total_domains = len(selected_domains)
            
            # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
            progress_text = st.empty()
            progress_counter = st.empty()
            time_info = st.empty()
            
            # ì „ì²´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            total_start_time = time.time()
            
            for domain_idx, domain in enumerate(selected_domains, 1):
                # ë„ë©”ì¸ë³„ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                domain_start_time = time.time()
                
                progress_text.text(f"Generating datasets for {domain} domain...")
                
                with st.spinner(f"Generating {num_datasets} datasets for {domain} domain ({domain_idx}/{total_domains})..."):
                    # Create output directory
                    output_dir = Path(f"dataset/{model_key}/{domain.lower()}")
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Create output file
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    output_path = output_dir / f"{model_key}_{num_datasets}prompts_{timestamp}.jsonl"
                    
                    # Generate datasets
                    with open(output_path, "w", encoding="utf-8") as f:
                        for i in range(num_datasets):
                            # ì§„í–‰ìƒí™© ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (ë„ë©”ì¸ ì •ë³´ í¬í•¨)
                            progress_counter.text(f"{domain} domain: {i+1}/{num_datasets}")
                            
                            # ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
                            current_time = time.time()
                            elapsed_time = current_time - total_start_time
                            
                            # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (í˜„ì¬ ì§„í–‰ë¥  ê¸°ì¤€)
                            total_datasets = total_domains * num_datasets
                            completed_datasets = (domain_idx - 1) * num_datasets + i
                            if completed_datasets > 0:
                                avg_time_per_dataset = elapsed_time / completed_datasets
                                remaining_datasets = total_datasets - completed_datasets
                                estimated_remaining_time = avg_time_per_dataset * remaining_datasets
                                estimated_total_time = elapsed_time + estimated_remaining_time
                                
                                time_info.text(f"ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ | ì˜ˆìƒì™„ë£Œ: {estimated_total_time:.1f}ì´ˆ | ë‚¨ì€ì‹œê°„: {estimated_remaining_time:.1f}ì´ˆ")
                            else:
                                time_info.text(f"ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ | ì˜ˆìƒì™„ë£Œ: ê³„ì‚° ì¤‘...")
                            
                            # Generate new prompt for the domain
                            if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
                                prompt = generate_domain_prompt(domain, model_key)
                                response = get_model_response(model_key, prompt)
                            else:
                                # Hugging Face ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                                prompt = get_test_prompt(domain)
                                response = "Generated using Hugging Face tokenizer"
                            
                            # Tokenize and extract evidence
                            tokens = tokenizer.tokenize(prompt)
                            
                            if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
                                query = f"""Given the following text and prompt, extract evidence tokens that support the answer to the prompt.
Text: {prompt}
Prompt: {prompt}

Please provide the evidence in the following JSON format:
{{
    "evidence_token_index": [list of token indices],
    "evidence": [list of evidence tokens]
}}

Rules:
1. Only include tokens that directly support the answer
2. Maintain the original order of tokens
3. Include complete phrases or sentences
4. Do not include words unrelated to the domain"""

                                evidence_response = get_model_response(model_key, query)
                            else:
                                # Hugging Face ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ evidence ì¶”ì¶œ
                                evidence_indices, evidence_tokens = extract_evidence_with_ollama(prompt, tokens, "default")
                                evidence_response = json.dumps({
                                    "evidence_token_index": evidence_indices,
                                    "evidence": evidence_tokens
                                })
                            
                            try:
                                # Extract JSON part from response
                                if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
                                    json_match = re.search(r'(\{.*\})', evidence_response, re.DOTALL)
                                    if json_match:
                                        evidence_data = json.loads(json_match.group(1))
                                    else:
                                        evidence_data = {"evidence_token_index": [], "evidence": []}
                                else:
                                    evidence_data = json.loads(evidence_response)
                                
                                evidence_indices = evidence_data.get("evidence_token_index", [])
                                evidence_tokens = evidence_data.get("evidence", [])
                                
                                # Create output data
                                output = {
                                    "prompt": prompt,
                                    "response": response,
                                    "evidence_indices": evidence_indices,
                                    "evidence_tokens": evidence_tokens,
                                    "model": model_key,
                                    "domain": domain,
                                    "timestamp": timestamp,
                                    "index": i + 1
                                }
                                
                                # Write to file
                                f.write(json.dumps(output, ensure_ascii=False) + "\n")
                                
                            except json.JSONDecodeError as e:
                                st.warning(f"Error parsing evidence response for dataset {i+1}: {str(e)}")
                                continue
                    
                    # ë„ë©”ì¸ë³„ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
                    domain_elapsed_time = time.time() - domain_start_time
                    st.success(f"âœ… Generated {num_datasets} datasets for {domain} domain (ì†Œìš”ì‹œê°„: {domain_elapsed_time:.1f}ì´ˆ)")
                    st.success(f"Dataset saved to {output_path}")
                
                # Add a small delay between domains
                if domain_idx < total_domains:
                    time.sleep(1)
            
            # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
            total_elapsed_time = time.time() - total_start_time
            
            # ì§„í–‰ìƒí™© ì»¨í…Œì´ë„ˆë“¤ ì •ë¦¬
            progress_text.empty()
            progress_counter.empty()
            time_info.empty()
            
            st.success(f"ğŸ‰ Completed generating datasets for all {total_domains} domains! (ì´ ì†Œìš”ì‹œê°„: {total_elapsed_time:.1f}ì´ˆ)")
                
        except Exception as e:
            st.error(f"Error during dataset generation: {str(e)}")
            return
    
    # Prompt input (for manual testing)
    st.subheader("âœï¸ Manual Testing")
    if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
        prompt = st.text_area(
            "Enter your prompt",
            value=generate_domain_prompt(selected_domain, model_key),
            height=150,
            key="prompt_input"
        )
    else:
        prompt = st.text_area(
            "Enter your prompt",
            value=get_test_prompt(selected_domain),
            height=150,
            key="prompt_input"
        )
    
    # Extract evidence button
    if st.button("ğŸ¯ Extract Evidence", key="extract_evidence"):
        if not tokenizer:
            st.warning(f"âš ï¸ Tokenizer not found for model {model_key}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
            return
        
        tokens = tokenizer.tokenize(prompt)
        with st.spinner("Extracting evidence..."):
            if model_selection_method == "Ollama ëª¨ë¸ (ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)":
                evidence_indices, evidence_tokens = extract_evidence_with_ollama(prompt, tokens, model_key)
            else:
                evidence_indices, evidence_tokens = extract_evidence_with_ollama(prompt, tokens, "default")
            
            if evidence_indices and evidence_tokens:
                st.markdown("### Extracted Evidence:")
                for idx, token in zip(evidence_indices, evidence_tokens):
                    st.markdown(f"- **{idx}**: {token}")