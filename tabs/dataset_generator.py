import streamlit as st
from pathlib import Path
import json
from transformers import AutoTokenizer
from utils import check_ollama_model_status, OLLAMA_API_BASE
import requests

def get_running_models():
    """í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            running_models = []
            for model in models:
                if check_ollama_model_status(model["name"]):
                    running_models.append(model["name"])
            return running_models
        return []
    except:
        return []

def get_model_response(model_name, prompt):
    """ëª¨ë¸ì˜ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        response.raise_for_status()
        return response.json()["response"]
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def tokenize_and_extract_words(text, tokenizer):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ë‹¨ì–´ë¥¼ ì¶”ì¶œ"""
    tokens = tokenizer.tokenize(text)
    words = []
    word_to_tokens = {}
    current_word = []
    current_word_tokens = []
    
    for i, token in enumerate(tokens):
        if token.startswith('â–'):
            if current_word:
                word = ''.join(current_word).replace('â–', '')
                if word.strip():  # ë¹ˆ ë¬¸ìžì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                    words.append(word)
                    word_to_tokens[word] = current_word_tokens
                current_word = []
                current_word_tokens = []
        current_word.append(token)
        current_word_tokens.append(i)
    
    if current_word:
        word = ''.join(current_word).replace('â–', '')
        if word.strip():  # ë¹ˆ ë¬¸ìžì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            words.append(word)
            word_to_tokens[word] = current_word_tokens
    
    # ì¤‘ë³µ ë‹¨ì–´ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    unique_words = []
    seen = set()
    for word in words:
        if word not in seen:
            unique_words.append(word)
            seen.add(word)
    
    return tokens, unique_words, word_to_tokens

def format_word_and_token_info(tokens, words, word_to_tokens):
    """í† í°ê³¼ ë‹¨ì–´ ì •ë³´ë¥¼ í¬ë§·íŒ…"""
    token_entries = [
        f'í† í°[{i}] = >>>{token}<<< (ì›ë¬¸ìž: {repr(token)})'
        for i, token in enumerate(tokens)
    ]
    
    word_entries = [
        f'ë‹¨ì–´[{i}] = >>>{word}<<<'
        for i, word in enumerate(words)
    ]
    
    return "\n".join(token_entries), "\n".join(word_entries)

def create_evidence_query(word_list, prompt, domain):
    """Evidence ì¶”ì¶œì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    return f"""ì•„ëž˜ ë‹¨ì–´ ëª©ë¡ì—ì„œë§Œ '{domain}' ë¶„ì•¼ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.
í”„ë¡¬í”„íŠ¸ì— ì—†ëŠ” ë‹¨ì–´ë‚˜ ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

í”„ë¡¬í”„íŠ¸: "{prompt}"

ì„ íƒ ê°€ëŠ¥í•œ ë‹¨ì–´ ëª©ë¡ (ì´ ëª©ë¡ì— ìžˆëŠ” ë‹¨ì–´ë§Œ ì„ íƒ ê°€ëŠ¥):
{word_list}

ì£¼ì˜ì‚¬í•­:
- ìœ„ ë‹¨ì–´ ëª©ë¡ì— ìžˆëŠ” ë‹¨ì–´ë§Œ ì„ íƒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
- ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì ˆëŒ€ë¡œ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ë‹¨ì–´ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë³€í˜•í•˜ì§€ ë§ˆì„¸ìš”
- ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”

ì‘ë‹µ ê·œì¹™:
1. evidence_word_indexì—ëŠ” ì„ íƒí•œ ë‹¨ì–´ì˜ ë²ˆí˜¸ë§Œ ë„£ìœ¼ì„¸ìš” (ë‹¨ì–´ ëª©ë¡ì˜ ë²ˆí˜¸)
2. evidenceì—ëŠ” ì„ íƒí•œ ë‹¨ì–´ë¥¼ ì •í™•í•˜ê²Œ ë³µì‚¬í•´ì„œ ë„£ìœ¼ì„¸ìš”
3. ê´€ë ¨ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”
4. ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

ì‘ë‹µ í˜•ì‹:
{{
    "evidence_word_index": [ë‹¨ì–´ ë²ˆí˜¸1, ë‹¨ì–´ ë²ˆí˜¸2, ...],
    "evidence": ["ë‹¨ì–´1", "ë‹¨ì–´2", ...],
    "explanation": "ì„ íƒí•œ ë‹¨ì–´ë“¤ì´ {domain} ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê´€ë ¨ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ 'ê´€ë ¨ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ìž‘ì„±í•˜ì„¸ìš”."
}}

ê²€ì¦ ì‚¬í•­:
1. evidence_word_indexì˜ ê° ë²ˆí˜¸ëŠ” 0ë¶€í„° {len(word_list.split(chr(10))) - 1} ì‚¬ì´ì˜ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤
2. evidenceì˜ ê° ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë‹¨ì–´ ëª©ë¡ì— ìžˆì–´ì•¼ í•©ë‹ˆë‹¤
3. ë‹¨ì–´ëŠ” >>> <<< ì‚¬ì´ì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë³µì‚¬í•´ì•¼ í•©ë‹ˆë‹¤
4. ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë§Œë“¤ê±°ë‚˜ ê¸°ì¡´ ë‹¨ì–´ë¥¼ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”"""

def extract_json_from_response(response):
    """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
    import re
    json_match = re.search(r'(\{[^{]*\})', response)
    if not json_match:
        raise ValueError("JSON í˜•ì‹ì˜ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return json.loads(json_match.group(1))

def validate_evidence(result, words):
    """Evidence ê²°ê³¼ ê²€ì¦"""
    required_fields = ["evidence_word_index", "evidence", "explanation"]
    missing_fields = [field for field in required_fields if field not in result]
    if missing_fields:
        raise ValueError(f"ëˆ„ë½ëœ í•„ë“œê°€ ìžˆìŠµë‹ˆë‹¤: {', '.join(missing_fields)}")
    
    evidence_word_index = result["evidence_word_index"]
    evidence = result["evidence"]
    
    if not isinstance(evidence_word_index, list):
        raise ValueError("evidence_word_indexëŠ” ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    if not isinstance(evidence, list):
        raise ValueError("evidenceëŠ” ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    # ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
    invalid_indices = []
    for i, idx in enumerate(evidence_word_index):
        if not isinstance(idx, int):
            invalid_indices.append({"ìœ„ì¹˜": i, "ì¸ë±ìŠ¤": idx, "ì´ìœ ": "ì •ìˆ˜ê°€ ì•„ë‹˜"})
        elif not (0 <= idx < len(words)):
            invalid_indices.append({"ìœ„ì¹˜": i, "ì¸ë±ìŠ¤": idx, "ì´ìœ ": f"ë²”ìœ„ ì´ˆê³¼ (0-{len(words)-1})"})
    
    if invalid_indices:
        details = [
            f"ìœ„ì¹˜ {e['ìœ„ì¹˜']}: ì¸ë±ìŠ¤ {e['ì¸ë±ìŠ¤']} ({e['ì´ìœ ']})"
            for e in invalid_indices
        ]
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ê°€ ìžˆìŠµë‹ˆë‹¤:\n" + "\n".join(details))
    
    # evidenceì™€ evidence_word_index ê¸¸ì´ ì¼ì¹˜ ê²€ì‚¬
    if len(evidence) != len(evidence_word_index):
        raise ValueError(f"ë°°ì—´ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (evidence: {len(evidence)}, index: {len(evidence_word_index)})")
    
    # ë‹¨ì–´ ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ ê²€ì‚¬
    invalid_words = []
    for i, word in enumerate(evidence):
        if word not in words:
            invalid_words.append({
                "ìœ„ì¹˜": i,
                "ë‹¨ì–´": word,
                "ê°€ëŠ¥í•œ_ë‹¨ì–´": words
            })
    
    if invalid_words:
        details = [
            f"ìœ„ì¹˜ {w['ìœ„ì¹˜']}: '{w['ë‹¨ì–´']}' (ì„ íƒ ê°€ëŠ¥í•œ ë‹¨ì–´: {w['ê°€ëŠ¥í•œ_ë‹¨ì–´']})"
            for w in invalid_words
        ]
        raise ValueError(f"ë‹¨ì–´ ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤:\n" + "\n".join(details))
    
    # ì¸ë±ìŠ¤ì™€ ë‹¨ì–´ ë§¤ì¹­ ê²€ì‚¬
    mismatches = []
    for i, (idx, word) in enumerate(zip(evidence_word_index, evidence)):
        if words[idx] != word:
            mismatches.append({
                "ìœ„ì¹˜": i,
                "ì¸ë±ìŠ¤": idx,
                "ì˜ˆìƒ": words[idx],
                "ì‹¤ì œ": word
            })
    
    if mismatches:
        details = [
            f"ìœ„ì¹˜ {m['ìœ„ì¹˜']}: ì¸ë±ìŠ¤ {m['ì¸ë±ìŠ¤']}ëŠ” '{m['ì˜ˆìƒ']}'ì´ì§€ë§Œ '{m['ì‹¤ì œ']}'ê°€ ì‚¬ìš©ë¨"
            for m in mismatches
        ]
        raise ValueError(f"ì¸ë±ìŠ¤ì™€ ë‹¨ì–´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:\n" + "\n".join(details))
    
    return evidence_word_index, evidence

def visualize_evidence(words, evidence_word_index, evidence, explanation):
    """Evidence ê²°ê³¼ ì‹œê°í™”"""
    highlighted_words = [
        f"<span style='background-color:#fff176; padding:2px'>{word}</span>"
        if i in evidence_word_index else word
        for i, word in enumerate(words)
    ]
    
    st.markdown("### ì¶”ì¶œëœ Evidence:")
    st.markdown(" ".join(highlighted_words), unsafe_allow_html=True)
    st.json({
        "evidence_word_index": evidence_word_index,
        "evidence": evidence,
        "explanation": explanation
    })

def show():
    st.title("ðŸ§ª Evidence ì¶”ì¶œ + ì €ìž¥ (Ollama ê¸°ë°˜)")

    # í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í‘œì‹œ
    running_models = get_running_models()
    if running_models:
        st.success(f"âœ… í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸: {', '.join(running_models)}")
    else:
        st.warning("âš ï¸ í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë¡œë“œ íƒ­ì—ì„œ ëª¨ë¸ì„ ì‹œìž‘í•´ì£¼ì„¸ìš”.")
        st.stop()

    domains = ["ì˜ë£Œ", "ë²•ë¥ ", "ë³´í—˜", "ê¸ˆìœµ", "íšŒê³„"]
    models = running_models if running_models else ["llama2", "gemma", "qwen", "deepseek"]
    
    # ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì„¤ì •
    MODEL_TOKENIZER_MAP = {
        "llama2": "meta-llama/Llama-2-7b-hf",
        "gemma:2b": "google/gemma-2b",
        "gemma:7b": "google/gemma-7b",
        "qwen": "Qwen/Qwen-7B",
        "deepseek": "deepseek-ai/deepseek-coder-7b-base"
    }

    # ìž…ë ¥ ì„¹ì…˜
    st.subheader("ðŸ“ ìž…ë ¥")
    prompt = st.text_area("í”„ë¡¬í”„íŠ¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”", height=100)
    col1, col2 = st.columns(2)
    with col1:
        domain = st.selectbox("ë„ë©”ì¸ì„ ì„ íƒí•˜ì„¸ìš”", domains)
    with col2:
        selected_model = st.selectbox(
            "ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", 
            models,
            help="ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ì¤‘ì—ì„œ ì„ íƒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
        )
    
    # ì„ íƒí•œ ëª¨ë¸ì— ë§žëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = None
    if prompt.strip():
        try:
            # ëª¨ë¸ ì´ë¦„ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
            model_key = selected_model.lower()
            tokenizer_name = MODEL_TOKENIZER_MAP.get(model_key)
            
            if not tokenizer_name:
                # ë²„ì „ ì •ë³´ê°€ ì—†ëŠ” ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
                base_model = model_key.split(":")[0]
                tokenizer_name = MODEL_TOKENIZER_MAP.get(base_model)
            
            if tokenizer_name:
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                tokens = tokenizer.tokenize(prompt)
                if st.button("ðŸ” í† í¬ë‚˜ì´ì € ê²°ê³¼ ë³´ê¸°", key="show_tokenizer_result"):
                    st.markdown("### í† í¬ë‚˜ì´ì € ê²°ê³¼")
                    # í† í°ê³¼ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ í‘œì‹œ
                    token_data = [{"ì¸ë±ìŠ¤": i, "í† í°": token} for i, token in enumerate(tokens)]
                    st.table(token_data)
            else:
                st.warning(f"âš ï¸ {selected_model} ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ëª¨ë¸: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
                st.stop()
        except Exception as e:
            st.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.stop()

    # í”„ë¦¬ë·° ì„¹ì…˜
    st.subheader("ðŸ‘€ í”„ë¦¬ë·°")
    if prompt.strip() and tokenizer:
        if st.button("ðŸŽ¯ Evidence ì¶”ì¶œ ë¯¸ë¦¬ë³´ê¸°", key="show_evidence_preview"):
            with st.spinner("Evidence ì¶”ì¶œ ì¤‘..."):
                # í† í°í™” ë° ë‹¨ì–´ ì¶”ì¶œ
                tokens, words, word_to_tokens = tokenize_and_extract_words(prompt, tokenizer)
                
                # ì •ë³´ í¬ë§·íŒ…
                token_list, word_list = format_word_and_token_info(tokens, words, word_to_tokens)
                
                # ì¿¼ë¦¬ ìƒì„± ë° ëª¨ë¸ í˜¸ì¶œ
                query = create_evidence_query(word_list, prompt, domain)
                evidence_response = get_model_response(selected_model, query)
                
                try:
                    # JSON ì¶”ì¶œ ë° ê²€ì¦
                    result = extract_json_from_response(evidence_response)
                    evidence_word_index, evidence = validate_evidence(result, words)
                    explanation = result.get("explanation", "")
                    
                    # ê²°ê³¼ ì‹œê°í™”
                    visualize_evidence(words, evidence_word_index, evidence, explanation)
                    
                except Exception as e:
                    st.error(str(e))
                    st.code(evidence_response, language="text")

    # ì €ìž¥ ì„¹ì…˜
    st.subheader("ðŸ’¾ ì €ìž¥")
    if st.button("ðŸ“¦ Evidence ì¶”ì¶œ ê²°ê³¼ ì €ìž¥"):
        if not prompt.strip():
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ì„ íƒí•œ ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
            if not check_ollama_model_status(selected_model):
                st.error(f"âŒ {selected_model} ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ ë¡œë“œ íƒ­ì—ì„œ ëª¨ë¸ì„ ì‹œìž‘í•´ì£¼ì„¸ìš”.")
                st.stop()

            try:
                with st.spinner("Evidence ì¶”ì¶œ ë° ì €ìž¥ ì¤‘..."):
                    # ì¼ë°˜ ì‘ë‹µ ì–»ê¸°
                    response = get_model_response(selected_model, prompt)

                    # Evidence ì¶”ì¶œ
                    query = f"""ìž…ë ¥ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ '{domain}' ë¶„ì•¼ì™€ ê´€ë ¨ëœ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.

í”„ë¡¬í”„íŠ¸: "{prompt}"

ë‹¨ì–´ ëª©ë¡:
{word_list}

í† í° ì •ë³´:
{token_list}

ì£¼ì˜ì‚¬í•­:
- í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œë§Œ ë‹¨ì–´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”
- ë‹¨ì–´ëŠ” ì •í™•ížˆ ì œì‹œëœ í˜•íƒœë¡œë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
- ë‹¨ì–´ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë³€í˜•í•˜ì§€ ë§ˆì„¸ìš”
- evidence ë°°ì—´ì˜ ê° ë‹¨ì–´ëŠ” ë‹¨ì–´ ëª©ë¡ì—ì„œ ë³µì‚¬í•œ ê²ƒê³¼ ì •í™•ížˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤

ì‘ë‹µ ê·œì¹™:
1. í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ '{domain}' ë¶„ì•¼ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹¨ì–´ë§Œ ì°¾ìœ¼ì„¸ìš”
2. ê´€ë ¨ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”
3. evidence_word_indexì—ëŠ” ì„ íƒí•œ ë‹¨ì–´ì˜ ë²ˆí˜¸ë§Œ ë„£ìœ¼ì„¸ìš”
4. evidenceì—ëŠ” í•´ë‹¹ ë²ˆí˜¸ì˜ ë‹¨ì–´ë¥¼ ì •í™•í•˜ê²Œ ë³µì‚¬í•´ì„œ ë„£ìœ¼ì„¸ìš”
5. evidence_word_indexì™€ evidence ë°°ì—´ì˜ ê¸¸ì´ëŠ” ê°™ì•„ì•¼ í•©ë‹ˆë‹¤

ì‘ë‹µ í˜•ì‹:
{{
    "evidence_word_index": [ë‹¨ì–´ ë²ˆí˜¸1, ë‹¨ì–´ ë²ˆí˜¸2, ...],
    "evidence": ["ë‹¨ì–´1", "ë‹¨ì–´2", ...],
    "explanation": "ì„ íƒí•œ ë‹¨ì–´ë“¤ì´ {domain} ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê´€ë ¨ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ 'ê´€ë ¨ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ìž‘ì„±í•˜ì„¸ìš”."
}}

ê²€ì¦ ì‚¬í•­:
1. evidence_word_indexì˜ ê° ë²ˆí˜¸ëŠ” ì‹¤ì œ ë‹¨ì–´ ëª©ë¡ì˜ ì¸ë±ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤
2. evidenceì˜ ê° ë‹¨ì–´ëŠ” í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë‹¨ì–´ì™€ ì •í™•ížˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤
3. ë‹¨ì–´ëŠ” >>> <<< ì‚¬ì´ì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì•¼ í•©ë‹ˆë‹¤
4. ë„ë©”ì¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‹¨ì–´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""

                    evidence_response = get_model_response(selected_model, query)
                    try:
                        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        import re
                        json_match = re.search(r'(\{[^{]*\})', evidence_response)
                        if not json_match:
                            raise ValueError("JSON í˜•ì‹ì˜ ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        
                        evidence_response = json_match.group(1)
                        result = json.loads(evidence_response)
                        
                        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                        required_fields = ["evidence_word_index", "evidence", "explanation"]
                        missing_fields = [field for field in required_fields if field not in result]
                        if missing_fields:
                            raise ValueError(f"ëˆ„ë½ëœ í•„ë“œê°€ ìžˆìŠµë‹ˆë‹¤: {', '.join(missing_fields)}")
                            
                        evidence_word_index = result["evidence_word_index"]
                        evidence = result["evidence"]
                        explanation = result.get("explanation", "")

                        # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ê²€ì¦
                        if not isinstance(evidence_word_index, list):
                            raise ValueError("evidence_word_indexëŠ” ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
                        if not isinstance(evidence, list):
                            raise ValueError("evidenceëŠ” ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

                        # ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
                        invalid_indices = [i for i in evidence_word_index if not (isinstance(i, int) and 0 <= i < len(words))]
                        if invalid_indices:
                            raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ê°€ ìžˆìŠµë‹ˆë‹¤: {invalid_indices}")

                        # evidenceì™€ evidence_word_index ê¸¸ì´ ì¼ì¹˜ ê²€ì‚¬
                        if len(evidence) != len(evidence_word_index):
                            raise ValueError(f"ë°°ì—´ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (evidence: {len(evidence)}, index: {len(evidence_word_index)})")

                        # evidenceê°€ ì‹¤ì œ ë‹¨ì–´ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì‚¬
                        mismatches = []
                        for i, idx in enumerate(evidence_word_index):
                            expected_word = words[idx]
                            actual_word = evidence[i]
                            if expected_word != actual_word:
                                mismatches.append({
                                    "ìœ„ì¹˜": i,
                                    "ì¸ë±ìŠ¤": idx,
                                    "ì˜ˆìƒ": repr(expected_word),
                                    "ì‹¤ì œ": repr(actual_word)
                                })
                        
                        if mismatches:
                            mismatch_details = [
                                f"ìœ„ì¹˜ {m['ìœ„ì¹˜']}: ì¸ë±ìŠ¤ {m['ì¸ë±ìŠ¤']}ì˜ ë‹¨ì–´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ (ì˜ˆìƒ: {m['ì˜ˆìƒ']}, ì‹¤ì œ: {m['ì‹¤ì œ']})"
                                for m in mismatches
                            ]
                            raise ValueError(f"ë‹¨ì–´ ë¶ˆì¼ì¹˜:\n" + "\n".join(mismatch_details))

                        # ì €ìž¥
                        output = {
                            "input": prompt,
                            "domain": domain,
                            "model_response": response,
                            "words": words,
                            "evidence_word_index": evidence_word_index,
                            "evidence": evidence,
                            "explanation": explanation
                        }

                        output_dir = Path("dataset_output")
                        output_dir.mkdir(exist_ok=True)
                        output_path = output_dir / f"{selected_model}_{domain}.jsonl"
                        with open(output_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps(output, ensure_ascii=False) + "\n")

                        # ê²°ê³¼ í‘œì‹œ
                        st.success(f"ðŸŽ‰ ì €ìž¥ ì™„ë£Œ: {output_path}")
                        
                        # ì €ìž¥ëœ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                        with st.expander("ðŸ“‹ ì €ìž¥ëœ ê²°ê³¼ ë³´ê¸°"):
                            st.markdown("### ëª¨ë¸ ì‘ë‹µ:")
                            st.markdown(response)
                            
                            st.markdown("### ì¶”ì¶œëœ Evidence:")
                            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ê²°ê³¼ í‘œì‹œ
                            word_results = []
                            for i, word in enumerate(words):
                                is_evidence = i in evidence_word_index
                                word_results.append({
                                    "ì¸ë±ìŠ¤": i,
                                    "ë‹¨ì–´": word,
                                    "Evidence ì—¬ë¶€": "âœ…" if is_evidence else ""
                                })
                            st.table(word_results)
                            
                            st.markdown("### Evidence ì„¤ëª…:")
                            st.markdown(explanation)
                            
                            st.markdown("### ì „ì²´ ê²°ê³¼:")
                            st.json({
                                "evidence_word_index": evidence_word_index,
                                "evidence": evidence,
                                "explanation": explanation
                            })

                    except json.JSONDecodeError as e:
                        st.error(f"Evidence ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                        st.code(evidence_response, language="text")
                    except ValueError as e:
                        st.error(f"Evidence ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
                        st.code(evidence_response, language="text")
                    except Exception as e:
                        st.error(f"Evidence ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                        st.code(evidence_response, language="text")

            except Exception as e:
                st.error(f"âŒ Ollama ìš”ì²­ ì‹¤íŒ¨: {e}") 