import streamlit as st
from pathlib import Path
import json
from transformers import AutoTokenizer
from utils import check_ollama_model_status, OLLAMA_API_BASE
import requests
import random
from typing import List, Tuple
import re

# Model tokenizer settings
MODEL_TOKENIZER_MAP = {
    "llama2": "meta-llama/Llama-2-7b-hf",
    "gemma:2b": "google/gemma-2b",
    "gemma:7b": "google/gemma-7b",
    "qwen": "Qwen/Qwen-7B",
    "deepseek": "deepseek-ai/deepseek-coder-7b-base",
    "mistral": "mistralai/Mistral-7B-v0.1",
    "mixtral": "mistralai/Mixtral-8x7B-v0.1",
    "yi": "01-ai/Yi-6B",
    "openchat": "openchat/openchat",
    "neural": "neural-chat/neural-chat-7b-v3-1",
    "phi": "microsoft/phi-2",
    "stable": "stabilityai/stable-code-3b"
}

def get_running_models():
    """Get list of currently running Ollama models"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            running_models = []
            for model in models:
                if check_ollama_model_status(model["name"]):
                    running_models.append(model["name"])
            return running_models
        return []
    except:
        return []

def get_model_response(model_name, prompt):
    """Get response from the model"""
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        response.raise_for_status()
        
        # ì‘ë‹µì´ JSONì¸ ê²½ìš°
        try:
            json_response = response.json()
            if isinstance(json_response, dict) and "response" in json_response:
                return json_response["response"]
        except json.JSONDecodeError:
            pass
            
        # ì‘ë‹µì´ ë°”ì´íŠ¸ì¸ ê²½ìš°
        if isinstance(response.content, bytes):
            return response.content.decode('utf-8')
            
        return str(response.text)
    except Exception as e:
        return f"Error occurred: {str(e)}"

def tokenize_and_extract_words(text, tokenizer):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ë‹¨ì–´ë¥¼ ì¶”ì¶œ"""
    print("\n=== Tokenization Debug ===")
    print(f"Original text: {text}")
    
    tokens = tokenizer.tokenize(text)
    print(f"Tokens: {tokens}")
    
    # êµ¬ë‘ì  ì œê±°ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def clean_word(word):
        # ë‹¨ì–´ ëì˜ êµ¬ë‘ì  ì œê±° (ì‰¼í‘œ, ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±)
        return word.rstrip(',.!?:;')
    
    # ë‹¨ì–´ ë¶„ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
    def split_into_words(text):
        # ê¸°ë³¸ì ì¸ ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
        raw_words = []
        current_word = ""
        for char in text:
            if char.isspace():
                if current_word:
                    raw_words.append(current_word)
                    current_word = ""
            else:
                current_word += char
        if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬
            raw_words.append(current_word)
        
        # êµ¬ë‘ì  ì œê±° ë° ì •ë¦¬
        words = [clean_word(word) for word in raw_words]
        # ë¹ˆ ë¬¸ìì—´ ì œê±°
        words = [word for word in words if word.strip()]
        return words
    
    # í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë‹¨ì–´ë¡œ ë¶„ë¦¬
    words = split_into_words(text)
    print(f"Split words: {words}")
    
    # ê° ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ ì°¾ê¸°
    word_positions = []
    current_pos = 0
    text_lower = text.lower()
    
    for word in words:
        word_lower = word.lower()
        # í˜„ì¬ ìœ„ì¹˜ë¶€í„° ë‹¨ì–´ ì°¾ê¸°
        while current_pos < len(text):
            pos = text_lower.find(word_lower, current_pos)
            if pos != -1:
                word_positions.append((pos, pos + len(word), word))
                current_pos = pos + len(word)
                break
            current_pos += 1
    
    # ë‹¨ì–´ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    word_positions.sort(key=lambda x: x[0])
    
    # ì •ë ¬ëœ ìˆœì„œë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¬êµ¬ì„±
    words = [wp[2] for wp in word_positions]
    
    # í† í°ê³¼ ë‹¨ì–´ ë§¤í•‘
    word_to_tokens = {}
    current_token_pos = 0
    current_token_idx = 0
    
    for start, end, word in word_positions:
        token_indices = []
        token_text = ""
        
        while current_token_idx < len(tokens) and current_token_pos <= end:
            token = tokens[current_token_idx].replace('â–', '')
            if token.strip():
                token_text += token
                if current_token_pos >= start and current_token_pos < end:
                    token_indices.append(current_token_idx)
                current_token_pos += len(token)
            current_token_idx += 1
        
        if token_indices:
            word_to_tokens[word] = token_indices
    
    print("\n=== Word Processing Results ===")
    print(f"Final words: {words}")
    print("Word to token mapping:")
    for word, token_indices in word_to_tokens.items():
        token_values = [tokens[idx] for idx in token_indices]
        print(f"Word: {word}")
        print(f"Token indices: {token_indices}")
        print(f"Token values: {token_values}\n")
    
    return tokens, words, word_to_tokens

def format_word_and_token_info(tokens, words, word_to_tokens):
    """Format token and word information"""
    token_entries = [
        f'Token[{i}] = >>>{token}<<< (raw: {repr(token)})'
        for i, token in enumerate(tokens)
    ]
    
    word_entries = [
        f'Word[{i}] = >>>{word}<<<'
        for i, word in enumerate(words)
    ]
    
    return "\n".join(token_entries), "\n".join(word_entries)

def create_evidence_query(word_list, prompt, domain):
    """Evidence ì¶”ì¶œì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    # ë‹¨ì–´ ëª©ë¡ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
    words = [word.strip() for word in word_list.split('\n') if word.strip()]
    
    # ë‹¨ì–´ ëª©ë¡ì„ í‘œ í˜•ì‹ìœ¼ë¡œ ìƒì„±
    word_table = []
    for i, word in enumerate(words):
        word_table.append(f"| {i} | {word} |")
    word_table = "| Index | Word |\n|-------|------|\n" + "\n".join(word_table)

    return f"""You are a JSON API that extracts evidence tokens from text. Follow these instructions exactly:

1. From the token list below, identify tokens related to the '{domain}' domain.
2. Return ONLY a JSON object with this exact format:
{{
    "evidence_token_index": [numbers],
    "evidence": [tokens]
}}

Token List:
{word_table}

Input Text: "{prompt}"

Rules:
- evidence_token_index must be an array of numbers
- evidence must be an array of exact tokens from the list
- arrays must have the same length
- do not add any explanation or text outside the JSON
- do not modify or format the tokens
- do not use markdown or code blocks
"""

def extract_json_from_response(response):
    """Extract JSON from response"""
    import re
    # ë¬¸ìì—´ë¡œ ë³€í™˜ ë³´ì¥
    if not isinstance(response, str):
        response = str(response)
    
    # ì‘ë‹µì´ ì´ë¯¸ JSON ê°ì²´ì¸ ê²½ìš°
    if isinstance(response, dict):
        return response

    try:
        # ë¨¼ì € ì „ì²´ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
        return json.loads(response)
    except json.JSONDecodeError:
        try:
            # JSON í˜•ì‹ì˜ ë¬¸ìì—´ì„ ì°¾ìŒ (ì•ë’¤ì˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°)
            json_match = re.search(r'(\{(?:[^{}]|(?:\{[^{}]*\}))*\})', response)
            if not json_match:
                raise ValueError("Could not find JSON format response")
            
            json_str = json_match.group(1)
            
            # JSON ë¬¸ìì—´ ì •ë¦¬
            # 1. ì¤„ë°”ê¿ˆê³¼ ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€ê²½
            json_str = re.sub(r'\s+', ' ', json_str).strip()
            
            # 2. ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì°¾ì•„ì„œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            # ë¨¼ì € ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œë¥¼ ì„ì‹œ ì¹˜í™˜
            json_str = json_str.replace('\\"', '___ESCAPED_QUOTE___')
            # ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ í°ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            json_str = re.sub(r'(?<!\\)"([^"]*)"', r'"\1"', json_str)
            # ì„ì‹œ ì¹˜í™˜ëœ ì´ìŠ¤ì¼€ì´í”„ëœ í°ë”°ì˜´í‘œ ë³µì›
            json_str = json_str.replace('___ESCAPED_QUOTE___', '\\"')
            
            # 3. ì‘ì€ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë¬¸ìì—´ì„ í°ë”°ì˜´í‘œë¡œ ë³€ê²½
            json_str = re.sub(r"'([^']*)'", r'"\1"', json_str)
            
            # 4. í°ë”°ì˜´í‘œ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*"\s*', '"', json_str)
            
            # 5. ì½¤ë§ˆ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*,\s*', ',', json_str)
            
            # 6. ì¤‘ê´„í˜¸ ì£¼ë³€ì˜ ê³µë°± ì œê±°
            json_str = re.sub(r'\s*{\s*', '{', json_str)
            json_str = re.sub(r'\s*}\s*', '}', json_str)
            
            return json.loads(json_str)
        except Exception as e:
            print(f"Failed to parse JSON: {response}")
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸ ì‘ë‹µ: {response}")

def validate_evidence(result, words):
    """Validate evidence results"""
    required_fields = ["evidence_word_index", "evidence", "explanation"]
    missing_fields = [field for field in required_fields if field not in result]
    if missing_fields:
        raise ValueError(f"Missing fields: {', '.join(missing_fields)}")
    
    evidence_word_index = result["evidence_word_index"]
    evidence = result["evidence"]
    
    if not isinstance(evidence_word_index, list):
        raise ValueError("evidence_word_index must be an array ([])")
    if not isinstance(evidence, list):
        raise ValueError("evidence must be an array ([])")
    
    # ë‹¨ì–´ ëª©ë¡ ì •ë¦¬ (ê³µë°± ì œê±°)
    words = [word.strip() for word in words if word.strip()]
    
    # Validate indices
    invalid_indices = []
    for i, idx in enumerate(evidence_word_index):
        if not isinstance(idx, int):
            invalid_indices.append({"position": i, "index": idx, "reason": "not an integer"})
        elif not (0 <= idx < len(words)):
            invalid_indices.append({"position": i, "index": idx, "reason": f"out of range (0-{len(words)-1})"})
    
    if invalid_indices:
        details = [
            f"Position {e['position']}: Index {e['index']} ({e['reason']})"
            for e in invalid_indices
        ]
        raise ValueError(f"Invalid indices found:\n" + "\n".join(details))
    
    # Check if evidence and evidence_word_index lengths match
    if len(evidence) != len(evidence_word_index):
        raise ValueError(f"Array lengths don't match (evidence: {len(evidence)}, index: {len(evidence_word_index)})")
    
    # Check for words not in the list
    invalid_words = []
    for i, word in enumerate(evidence):
        if word not in words:
            invalid_words.append({
                "position": i,
                "word": word,
                "available_words": words
            })
    
    if invalid_words:
        details = [
            f"Position {w['position']}: '{w['word']}' (available words: {w['available_words']})"
            for w in invalid_words
        ]
        raise ValueError(f"Words not in list found:\n" + "\n".join(details))
    
    # Check index and word matching
    mismatches = []
    for i, (idx, word) in enumerate(zip(evidence_word_index, evidence)):
        if words[idx] != word:
            mismatches.append({
                "position": i,
                "index": idx,
                "expected": words[idx],
                "actual": word
            })
    
    if mismatches:
        details = [
            f"Position {m['position']}: Index {m['index']} should be '{m['expected']}' but got '{m['actual']}'"
            for m in mismatches
        ]
        raise ValueError(f"Index and word mismatches found:\n" + "\n".join(details))
    
    return evidence_word_index, evidence

def visualize_evidence(words, evidence_word_index, evidence, explanation):
    """Visualize evidence results"""
    highlighted_words = [
        f"<span style='background-color:#fff176; padding:2px'>{word}</span>"
        if i in evidence_word_index else word
        for i, word in enumerate(words)
    ]
    
    st.markdown("### Extracted Evidence:")
    st.markdown(" ".join(highlighted_words), unsafe_allow_html=True)
    
    # JSON ë°ì´í„°ë¥¼ ë¨¼ì € íŒŒì‹±í•˜ê³  ê²€ì¦
    try:
        json_data = {
            "evidence_word_index": evidence_word_index,
            "evidence": evidence,
            "explanation": explanation
        }
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜í–ˆë‹¤ê°€ ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬
        json_str = json.dumps(json_data, ensure_ascii=False)
        validated_data = json.loads(json_str)
        st.json(validated_data)
    except json.JSONDecodeError as e:
        st.error(f"JSON ë°ì´í„° ì˜¤ë¥˜: {str(e)}")
        st.code(str(json_data), language="json")

def get_test_prompt(domain: str) -> str:
    """ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    prompts = {
        "Medical": [
            "What are the main side effects of this medication?",
            "What are the contraindications for this treatment?",
            "What are the recommended dosages for this drug?",
            "What are the potential complications of this procedure?",
            "What are the warning signs to watch for?"
        ],
        "Legal": [
            "What are the key clauses in this contract?",
            "What are the main obligations of the parties?",
            "What are the termination conditions?",
            "What are the dispute resolution procedures?",
            "What are the confidentiality requirements?"
        ],
        "Technical": [
            "What is the main functionality of this code?",
            "What are the key features of this system?",
            "What are the system requirements?",
            "What are the performance specifications?",
            "What are the security measures implemented?"
        ],
        "General": [
            "What is the main content of this document?",
            "What are the key points discussed?",
            "What are the main conclusions?",
            "What are the important findings?",
            "What are the main recommendations?"
        ]
    }
    return random.choice(prompts.get(domain, ["Please enter your prompt here..."]))

def extract_evidence_with_ollama(prompt, tokens, model_name):
    """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦ê±° ì¶”ì¶œ"""
    try:
        # í† í°ì´ ë°”ì´íŠ¸ íƒ€ì…ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë””ì½”ë”©
        decoded_tokens = []
        for token in tokens:
            if isinstance(token, bytes):
                try:
                    decoded_tokens.append(token.decode('utf-8'))
                except UnicodeDecodeError:
                    try:
                        decoded_tokens.append(token.decode('latin-1'))
                    except:
                        decoded_tokens.append('')
            else:
                decoded_tokens.append(str(token))

        query = create_evidence_query("\n".join(decoded_tokens), prompt, "Medical")
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": query,
                "stream": False
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            if 'response' in result:
                response_text = result['response']
                print(f"Raw response from model: {response_text}")  # ë””ë²„ê¹…ìš©
                
                try:
                    # JSON ë¬¸ìì—´ ì •ë¦¬
                    response_text = response_text.strip()
                    
                    # ì´ìŠ¤ì¼€ì´í”„ëœ JSON ë¬¸ìì—´ ì²˜ë¦¬
                    if '\\"' in response_text:
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œë¥¼ ì¼ë°˜ ë”°ì˜´í‘œë¡œ ë³€í™˜
                        response_text = response_text.replace('\\"', '"')
                    
                    # JSON ê°ì²´ ì¶”ì¶œ
                    json_match = re.search(r'(\{.*\})', response_text, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(1)
                    else:
                        st.error("Could not find JSON object in response")
                        return [], []
                    
                    # JSON íŒŒì‹± ì‹œë„
                    evidence_data = json.loads(response_text)
                    
                    # í•„ë“œëª… ì •ê·œí™”
                    evidence_data = {k.lower().replace('_', ''): v for k, v in evidence_data.items()}
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ì •ê·œí™”ëœ í•„ë“œëª…ìœ¼ë¡œ)
                    indices = evidence_data.get('evidencetokenindex', evidence_data.get('evidenceindices', []))
                    evidence = evidence_data.get('evidence', [])
                    
                    if not indices or not evidence:
                        st.error("Missing required fields in evidence data")
                        return [], []
                    
                    # ë¬¸ì¥ë¶€í˜¸ ì œê±° ë° ì¸ë±ìŠ¤ ì¡°ì •
                    punctuation_pattern = re.compile(r'[^\w\s]')
                    filtered_indices = []
                    filtered_evidence = []
                    removed_count = 0
                    
                    for i, (idx, token) in enumerate(zip(indices, evidence)):
                        # ë¬¸ì¥ë¶€í˜¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                        if not punctuation_pattern.search(token):
                            # ì´ì „ì— ì œê±°ëœ í† í° ìˆ˜ë§Œí¼ ì¸ë±ìŠ¤ ì¡°ì •
                            adjusted_idx = idx - removed_count
                            filtered_indices.append(adjusted_idx)
                            filtered_evidence.append(token)
                        else:
                            removed_count += 1
                    
                    indices = filtered_indices
                    evidence = filtered_evidence
                    
                    # ì¸ë±ìŠ¤ì™€ í† í° ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(indices) != len(evidence):
                        print(f"Debug - Indices length: {len(indices)}, Evidence length: {len(evidence)}")
                        print(f"Debug - Indices: {indices}")
                        print(f"Debug - Evidence: {evidence}")
                        st.error(f"Number of indices ({len(indices)}) and tokens ({len(evidence)}) do not match")
                        # ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš° ë” ì§§ì€ ìª½ì— ë§ì¶° ìë¥´ê¸°
                        min_length = min(len(indices), len(evidence))
                        indices = indices[:min_length]
                        evidence = evidence[:min_length]
                    
                    # ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                    if any(not isinstance(i, int) or i < 0 or i >= len(tokens) for i in indices):
                        st.error("Invalid indices found in response")
                        return [], []
                    
                    return indices, evidence
                except json.JSONDecodeError as e:
                    print(f"Error parsing response: {str(e)}\nResponse: {response_text}")
                    st.error(f"Evidence extraction failed: Invalid JSON format")
                    return [], []
            else:
                st.error("No response field in API result")
                return [], []
        else:
            st.error(f"API request failed with status code: {response.status_code}")
            return [], []
    except Exception as e:
        st.error(f"Error during evidence extraction: {str(e)}")
        return [], []

def load_tokenizer(model_key):
    """Load tokenizer for the given model"""
    try:
        tokenizer_name = MODEL_TOKENIZER_MAP.get(model_key.split(":")[0])
        if tokenizer_name:
            # Qwen ëª¨ë¸ì˜ ê²½ìš° trust_remote_code=True ì˜µì…˜ ì¶”ê°€
            if "qwen" in model_key.lower():
                return AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            else:
                return AutoTokenizer.from_pretrained(tokenizer_name)
        return None
    except Exception as e:
        st.error(f"Error loading tokenizer: {str(e)}")
        return None

def show():
    st.title("Dataset Generator")
    
    # Model selection
    st.subheader("ğŸ¤– Model")
    models = get_running_models()
    if not models:
        st.error("No running models found. Please start Ollama first.")
        return
    
    selected_model = st.selectbox(
        "Select a model",
        models,
        key="model_selector"
    )
    model_key = selected_model.lower()
    
    # Get tokenizer
    tokenizer = load_tokenizer(model_key)
    
    # Domain selection
    st.subheader("ğŸ¯ Domain")
    domain = st.selectbox(
        "Select domain",
        ["Medical", "Legal", "Technical", "General"],
        key="domain_selector"
    )
    
    # Prompt input
    prompt = st.text_area(
        "Enter your prompt",
        value=get_test_prompt(domain),
        height=150,
        key="prompt_input"
    )
    
    # Preview section
    st.subheader("ğŸ‘€ Preview")
    if prompt.strip():
        # Extract evidence using Ollama
        if st.button("ğŸ¯ Extract Evidence", key="extract_evidence"):
            if not tokenizer:
                st.warning(f"âš ï¸ Tokenizer not found for model {model_key}. Supported models: {', '.join(MODEL_TOKENIZER_MAP.keys())}")
                # í† í¬ë‚˜ì´ì € ì¶”ê°€ ë²„íŠ¼
                if st.button("â• Add Tokenizer", help="í˜„ì¬ ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤"):
                    base_model = model_key.split(":")[0]
                    default_tokenizers = {
                        "mistral": "mistralai/Mistral-7B-v0.1",
                        "mixtral": "mistralai/Mixtral-8x7B-v0.1",
                        "llama2": "meta-llama/Llama-2-7b-hf",
                        "gemma": "google/gemma-7b",
                        "qwen": "Qwen/Qwen-7B",
                        "yi": "01-ai/Yi-6B",
                        "deepseek": "deepseek-ai/deepseek-coder-7b-base",
                        "openchat": "openchat/openchat",
                        "neural": "neural-chat/neural-chat-7b-v3-1",
                        "phi": "microsoft/phi-2",
                        "stable": "stabilityai/stable-code-3b"
                    }
                    if base_model in default_tokenizers:
                        if "MODEL_TOKENIZER_MAP" not in st.session_state:
                            st.session_state.MODEL_TOKENIZER_MAP = MODEL_TOKENIZER_MAP.copy()
                        st.session_state.MODEL_TOKENIZER_MAP[base_model] = default_tokenizers[base_model]
                        st.success(f"âœ… Added tokenizer for {base_model}: {default_tokenizers[base_model]}")
                    else:
                        st.error(f"âŒ No default tokenizer found for {base_model}")
            else:
                # Tokenize text
                tokens = tokenizer.tokenize(prompt)
                with st.spinner("Extracting evidence..."):
                    evidence_indices, evidence_tokens = extract_evidence_with_ollama(prompt, tokens, model_key)
                    if evidence_indices and evidence_tokens:
                        st.markdown("### Extracted Evidence:")
                        evidence_data = [
                            {"Index": idx, "Token": token, "Is Evidence": "âœ…"}
                            for idx, token in zip(evidence_indices, evidence_tokens)
                        ]
                        st.table(evidence_data)
                        
                        # ì „ì²´ í† í° ëª©ë¡ì—ì„œ ì¦ê±° í† í° í•˜ì´ë¼ì´íŠ¸
                        st.markdown("### All Tokens:")
                        all_tokens_data = [
                            {"Index": i, "Token": token, "Is Evidence": "âœ…" if i in evidence_indices else ""}
                            for i, token in enumerate(tokens)
                        ]
                        st.table(all_tokens_data)
                    else:
                        st.warning("No evidence tokens found.")

    # Save section
    st.subheader("ğŸ’¾ Save")
    if st.button("ğŸ“¦ Save Evidence Extraction Results"):
        if not prompt.strip():
            st.warning("Please enter a prompt.")
        else:
            # Double check if selected model is running
            if not check_ollama_model_status(model_key):
                st.error(f"âŒ Model {model_key} is not running. Please start it in the Model Load tab.")
                st.stop()

            try:
                with st.spinner("Extracting and saving evidence..."):
                    # Get general response
                    response = get_model_response(model_key, prompt)

                    # Extract evidence
                    query = f"""Find words from the input prompt that are related to the '{domain}' domain.

Prompt: "{prompt}"

Word list:
{word_list}

Token information:
{token_list}

Important notes:
- Only find words from within the prompt
- Return empty arrays if no domain-related words are found
- Words must be used exactly as shown
- Do not modify or transform words
- Each word in the evidence array must exactly match a word from the word list

Response rules:
1. Only find words directly related to the '{domain}' domain from the prompt
2. Return empty arrays if no related words are found
3. evidence_word_index should only contain word numbers
4. evidence should contain exact copies of the words at those numbers
5. evidence_word_index and evidence arrays must have the same length

Response format:
{{
    "evidence_word_index": [word_number1, word_number2, ...],
    "evidence": ["word1", "word2", ...],
    "explanation": "Please explain why the selected words are related to the {domain} domain. If no related words are found, write 'No related words found.'"
}}

Validation:
1. Each number in evidence_word_index must be a valid word list index
2. Each word in evidence must match the word at its index
3. Words must be exact copies of the content between >>> and <<<
4. Do not include words unrelated to the domain"""

                    evidence_response = get_model_response(model_key, query)
                    try:
                        # Extract JSON part from response
                        import re
                        json_match = re.search(r'(\{[^{]*\})', evidence_response)
                        if not json_match:
                            raise ValueError("Could not find JSON format response")
                        
                        evidence_response = json_match.group(1)
                        result = json.loads(evidence_response)
                        
                        # Validate required fields
                        required_fields = ["evidence_word_index", "evidence", "explanation"]
                        missing_fields = [field for field in required_fields if field not in result]
                        if missing_fields:
                            raise ValueError(f"Missing fields: {', '.join(missing_fields)}")
                            
                        evidence_word_index = result["evidence_word_index"]
                        evidence = result["evidence"]
                        explanation = result.get("explanation", "")

                        # Validate list format
                        if not isinstance(evidence_word_index, list):
                            raise ValueError("evidence_word_index must be an array ([])")
                        if not isinstance(evidence, list):
                            raise ValueError("evidence must be an array ([])")

                        # Validate indices
                        invalid_indices = [i for i in evidence_word_index if not (isinstance(i, int) and 0 <= i < len(words))]
                        if invalid_indices:
                            raise ValueError(f"Invalid indices found: {invalid_indices}")

                        # Check if evidence and evidence_word_index lengths match
                        if len(evidence) != len(evidence_word_index):
                            raise ValueError(f"Array lengths don't match (evidence: {len(evidence)}, index: {len(evidence_word_index)})")

                        # Check if evidence matches actual words
                        mismatches = []
                        for i, idx in enumerate(evidence_word_index):
                            expected_word = words[idx]
                            actual_word = evidence[i]
                            if expected_word != actual_word:
                                mismatches.append({
                                    "position": i,
                                    "index": idx,
                                    "expected": repr(expected_word),
                                    "actual": repr(actual_word)
                                })
                        
                        if mismatches:
                            mismatch_details = [
                                f"Position {m['position']}: Index {m['index']} word mismatch (expected: {m['expected']}, got: {m['actual']})"
                                for m in mismatches
                            ]
                            raise ValueError(f"Word mismatches:\n" + "\n".join(mismatch_details))

                        # Save
                        output = {
                            "input": prompt,
                            "domain": domain,
                            "model_response": response,
                            "words": words,
                            "evidence_word_index": evidence_word_index,
                            "evidence": evidence,
                            "explanation": explanation
                        }

                        output_dir = Path("dataset_output")
                        output_dir.mkdir(exist_ok=True)
                        output_path = output_dir / f"{model_key}_{domain}.jsonl"
                        with open(output_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps(output, ensure_ascii=False) + "\n")

                        # Display results
                        st.success(f"ğŸ‰ Save complete: {output_path}")
                        
                        # Preview saved results
                        with st.expander("ğŸ“‹ View Saved Results"):
                            st.markdown("### Model Response:")
                            st.markdown(response)
                            
                            st.markdown("### Extracted Evidence:")
                            # Display results word by word
                            word_results = []
                            for i, word in enumerate(words):
                                is_evidence = i in evidence_word_index
                                word_results.append({
                                    "Index": i,
                                    "Word": word,
                                    "Is Evidence": "âœ…" if is_evidence else ""
                                })
                            st.table(word_results)
                            
                            st.markdown("### Evidence Explanation:")
                            st.markdown(explanation)
                            
                            st.markdown("### Complete Results:")
                            try:
                                json_data = {
                                    "evidence_word_index": evidence_word_index,
                                    "evidence": evidence,
                                    "explanation": explanation
                                }
                                json_str = json.dumps(json_data, ensure_ascii=False)
                                validated_data = json.loads(json_str)
                                st.json(validated_data)
                            except json.JSONDecodeError as e:
                                st.error(f"JSON ë°ì´í„° ì˜¤ë¥˜: {str(e)}")
                                st.code(str(json_data), language="json")

                    except json.JSONDecodeError as e:
                        st.error(f"Evidence extraction failed. JSON parsing error: {str(e)}")
                        st.code(evidence_response, language="text")
                    except ValueError as e:
                        st.error(f"Evidence extraction failed. Data validation error: {str(e)}")
                        st.code(evidence_response, language="text")
                    except Exception as e:
                        st.error(f"Error during evidence extraction: {str(e)}")
                        st.code(evidence_response, language="text")

            except Exception as e:
                st.error(f"âŒ Ollama request failed: {e}")