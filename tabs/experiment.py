import streamlit as st
import os
import json
import requests
from utils import get_available_models
import pandas as pd
from datetime import datetime
import pickle

# Huggingface ê´€ë ¨ ì¶”ê°€
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

DATASET_ROOT = "dataset"
MODEL_CACHE_FILE = "model_cache.pkl"

# ëª¨ë¸ë³„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
def get_model_experiment_path(model_name):
    """ëª¨ë¸ë³„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    safe_model_name = model_name.replace(':', '_').replace('/', '_')
    return f"experiment_results/{safe_model_name}"

def get_model_dataset_path(model_name):
    """ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    safe_model_name = model_name.replace(':', '_').replace('/', '_')
    return f"dataset/{safe_model_name}"

# ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ
MODEL_CACHE = {
    "model": None,
    "tokenizer": None,
    "model_name": None
}

def save_model_cache():
    """ëª¨ë¸ ìºì‹œ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥"""
    try:
        cache_info = {
            "model_name": MODEL_CACHE["model_name"],
            "timestamp": datetime.now().isoformat()
        }
        with open(MODEL_CACHE_FILE, "wb") as f:
            pickle.dump(cache_info, f)
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_model_cache():
    """íŒŒì¼ì—ì„œ ëª¨ë¸ ìºì‹œ ì •ë³´ë¥¼ ë¡œë“œ"""
    try:
        if os.path.exists(MODEL_CACHE_FILE):
            with open(MODEL_CACHE_FILE, "rb") as f:
                cache_info = pickle.load(f)
            return cache_info
    except Exception as e:
        print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(MODEL_CACHE_FILE):
            os.remove(MODEL_CACHE_FILE)
    except Exception as e:
        print(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

def create_model_directories(model_name):
    """ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    dataset_path = get_model_dataset_path(model_name)
    
    os.makedirs(experiment_path, exist_ok=True)
    os.makedirs(dataset_path, exist_ok=True)
    
    # ë„ë©”ì¸ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
    domains = ["economy", "technical", "legal", "medical"]
    for domain in domains:
        os.makedirs(os.path.join(dataset_path, domain), exist_ok=True)
    
    return experiment_path, dataset_path

def get_model_dataset_files(model_name):
    """íŠ¹ì • ëª¨ë¸ì˜ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    dataset_path = get_model_dataset_path(model_name)
    domains = ["economy", "technical", "legal", "medical"]
    files = {}
    
    for domain in domains:
        domain_path = os.path.join(dataset_path, domain)
        if os.path.exists(domain_path):
            files[domain] = [f for f in os.listdir(domain_path) if f.endswith(".jsonl")]
        else:
            files[domain] = []
    
    return files

def get_model_prompts(model_name, domain, filename, max_count=10000):
    """íŠ¹ì • ëª¨ë¸ì˜ ë°ì´í„°ì…‹ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    dataset_path = get_model_dataset_path(model_name)
    path = os.path.join(dataset_path, domain, filename)
    prompts = []
    
    try:
        with open(path, "r") as f:
            for i, line in enumerate(f):
                if i >= max_count:
                    break
                try:
                    data = json.loads(line)
                    prompt = data.get("prompt", "(no prompt)")
                    prompts.append(prompt)
                except Exception:
                    continue
    except Exception:
        pass
    return prompts

def copy_original_dataset_to_model(model_name):
    """ì›ë³¸ ë°ì´í„°ì…‹ì„ ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤."""
    original_dataset_path = DATASET_ROOT
    model_dataset_path = get_model_dataset_path(model_name)
    
    if not os.path.exists(original_dataset_path):
        st.error("ì›ë³¸ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ì›ë³¸ ë°ì´í„°ì…‹ì˜ ëª¨ë“  íŒŒì¼ì„ ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        domains = ["economy", "technical", "legal", "medical"]
        for domain in domains:
            original_domain_path = os.path.join(original_dataset_path, domain)
            model_domain_path = os.path.join(model_dataset_path, domain)
            
            if os.path.exists(original_domain_path):
                # ë„ë©”ì¸ ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(model_domain_path, exist_ok=True)
                
                # íŒŒì¼ ë³µì‚¬
                for filename in os.listdir(original_domain_path):
                    if filename.endswith('.jsonl'):
                        original_file = os.path.join(original_domain_path, filename)
                        model_file = os.path.join(model_domain_path, filename)
                        
                        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë³µì‚¬
                        if not os.path.exists(model_file):
                            import shutil
                            shutil.copy2(original_file, model_file)
        
        st.success(f"{model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# ê° ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
def get_dataset_files():
    domains = ["economy", "technical", "legal", "medical"]
    files = {}
    for domain in domains:
        domain_path = os.path.join(DATASET_ROOT, domain)
        if os.path.exists(domain_path):
            files[domain] = [f for f in os.listdir(domain_path) if f.endswith(".jsonl")]
        else:
            files[domain] = []
    return files

# ì„ íƒí•œ ë°ì´í„°ì…‹ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
def get_prompts(domain, filename, max_count=10000):
    path = os.path.join(DATASET_ROOT, domain, filename)
    prompts = []
    try:
        with open(path, "r") as f:
            for i, line in enumerate(f):
                if i >= max_count:
                    break
                try:
                    data = json.loads(line)
                    prompt = data.get("prompt", "(no prompt)")
                    prompts.append(prompt)
                except Exception:
                    continue
    except Exception:
        pass
    return prompts

def load_model_to_session(model_name):
    """
    ëª¨ë¸ì„ ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì— ì°¸ì¡°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    import torch
    import os
    
    # Qwen ëª¨ë¸ì˜ ê²½ìš°ì—ë§Œ íŠ¹ë³„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    if 'qwen' in model_name.lower():
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
        # scaled_dot_product_attention ë¹„í™œì„±í™” (Qwen ëª¨ë¸ë§Œ)
        os.environ["PYTORCH_DISABLE_SCALED_DOT_PRODUCT_ATTENTION"] = "1"
    # ê¸°ë³¸ ëª¨ë¸ëª… ì¶”ì¶œ (íŒŒë¼ë¯¸í„° ìˆ˜ ë¬´ì‹œ)
    base_model = model_name.split(":")[0]
    
    model_map = {
        'mistral': 'mistralai/Mistral-7B-v0.1',
        'llama2': 'meta-llama/Llama-2-7b-hf',
        'gemma': 'google/gemma-7b',
        'qwen': 'Qwen/Qwen-7B',
        'deepseek': 'deepseek-ai/deepseek-llm-7b-base',
        'deepseek-r1': 'deepseek-ai/deepseek-llm-7b-base',
        'deepseek-r1-distill-llama': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',
        'yi': '01-ai/Yi-6B',
        'openchat': 'openchat/openchat-3.5-7b',
        'neural': 'microsoft/DialoGPT-medium',
        'phi': 'microsoft/phi-2',
        'stable': 'stabilityai/stablelm-base-alpha-7b',
    }
    
    # 1. ì „ì²´ ëª¨ë¸ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„
    hf_model = model_map.get(model_name)
    if not hf_model:
        # 2. ê¸°ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ ì‹œë„ (íŒŒë¼ë¯¸í„° ìˆ˜ ë¬´ì‹œ)
        hf_model = model_map.get(base_model)
    
    if not hf_model:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ëª…ì…ë‹ˆë‹¤: {model_name} (ê¸°ë³¸ ëª¨ë¸ëª…: {base_model})")
        st.info(f"ì§€ì›ë˜ëŠ” ëª¨ë¸ë“¤: {', '.join(model_map.keys())}")
        return False
    try:
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(hf_model, trust_remote_code=True)
        
        # ëª¨ë¸ë³„ íŠ¹ë³„í•œ í† í¬ë‚˜ì´ì € ì„¤ì •
        if 'gemma' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        elif 'llama' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        elif 'qwen' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        
        # Qwen ëª¨ë¸ì˜ ê²½ìš° attention êµ¬í˜„ì„ ê°•ì œë¡œ eagerë¡œ ì„¤ì •
        if 'qwen' in model_name.lower():
            import os
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
            # Qwen ëª¨ë¸ì˜ configë¥¼ ìˆ˜ì •í•˜ì—¬ eager attention ì‚¬ìš©
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(hf_model, trust_remote_code=True)
            config.attn_implementation = "eager"
            config.use_flash_attention_2 = False
            config.use_cache = True
        
        # ëª¨ë¸ ë¡œë”© ì‹œë„
        try:
            quant_config = BitsAndBytesConfig(load_in_8bit=True)
            if 'qwen' in model_name.lower():
                # Qwen ëª¨ë¸ì˜ ê²½ìš° configë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ
                model = AutoModelForCausalLM.from_pretrained(
                    hf_model,
                    config=config,
                    quantization_config=quant_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    hf_model,
                    quantization_config=quant_config,
                    device_map="auto",
                    trust_remote_code=True,
                    attn_implementation="eager"
                )
        except Exception as quant_error:
            # 8ë¹„íŠ¸ ì–‘ìí™” ì‹¤íŒ¨ ì‹œ 16ë¹„íŠ¸ë¡œ ì‹œë„
            st.warning("8ë¹„íŠ¸ ì–‘ìí™” ë¡œë”© ì‹¤íŒ¨, 16ë¹„íŠ¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
            try:
                if 'qwen' in model_name.lower():
                    model = AutoModelForCausalLM.from_pretrained(
                        hf_model,
                        config=config,
                        device_map="auto",
                        trust_remote_code=True
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        hf_model,
                        device_map="auto",
                        trust_remote_code=True,
                        attn_implementation="eager"
                    )
            except Exception as device_error:
                # device_map ì‹¤íŒ¨ ì‹œ CPUë¡œ ì‹œë„
                st.warning("ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘ ì‹¤íŒ¨, CPUë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
                if 'qwen' in model_name.lower():
                    model = AutoModelForCausalLM.from_pretrained(
                        hf_model,
                        config=config,
                        trust_remote_code=True
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        hf_model,
                        trust_remote_code=True,
                        attn_implementation="eager"
                    )
        
        # 8ë¹„íŠ¸ ì–‘ìí™”ëœ ëª¨ë¸ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ
        # device_map="auto"ë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì¶”ê°€ ë””ë°”ì´ìŠ¤ ì´ë™ ë¶ˆí•„ìš”
        try:
            # ëª¨ë¸ì˜ í˜„ì¬ ë””ë°”ì´ìŠ¤ í™•ì¸
            device = next(model.parameters()).device
            st.info(f"ëª¨ë¸ì´ {device} ë””ë°”ì´ìŠ¤ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as device_error:
            st.warning(f"ë””ë°”ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {str(device_error)}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ CPU ì„¤ì •
            device = torch.device("cpu")
        
        # ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì— ì°¸ì¡° ì €ì¥
        st.session_state['model'] = model
        st.session_state['tokenizer'] = tokenizer
        st.session_state['model_name'] = model_name
        MODEL_CACHE["model"] = model
        MODEL_CACHE["tokenizer"] = tokenizer
        MODEL_CACHE["model_name"] = model_name
        # ìºì‹œ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥
        save_model_cache()
        return True
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        st.error("ëª¨ë¸ëª…ì„ í™•ì¸í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def unload_model_from_session():
    """
    ì„œë²„ ë©”ëª¨ë¦¬ì—ì„œ ëª¨ë¸ì„ í•´ì œí•©ë‹ˆë‹¤.
    ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì˜ ì°¸ì¡°ë¥¼ ì œê±°í•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ë¹„ì›ë‹ˆë‹¤.
    """
    import torch
    if 'model' in st.session_state:
        del st.session_state['model']
    if 'tokenizer' in st.session_state:
        del st.session_state['tokenizer']
    if 'model_name' in st.session_state:
        del st.session_state['model_name']
    # ê¸€ë¡œë²Œ ìºì‹œë„ ë¹„ì›€
    MODEL_CACHE["model"] = None
    MODEL_CACHE["tokenizer"] = None
    MODEL_CACHE["model_name"] = None
    # ìºì‹œ íŒŒì¼ ì‚­ì œ
    clear_model_cache()
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    st.success("ëª¨ë¸ì´ ì„œë²„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_attention_from_session(prompt):
    import torch
    model = st.session_state.get('model', None)
    tokenizer = st.session_state.get('tokenizer', None)
    if model is None or tokenizer is None:
        return None, None, None
    inputs = tokenizer(prompt, return_tensors="pt")
    # ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì…ë ¥ ì´ë™
    try:
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
    except Exception as device_error:
        # ë””ë°”ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨ ì‹œ CPU ì‚¬ìš©
        inputs = {k: v.to('cpu') for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    attentions = tuple(attn.cpu().numpy() for attn in outputs.attentions)
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    token_ids = inputs['input_ids'][0].cpu().tolist()
    return attentions, tokens, token_ids

def plot_attention_head_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í—¤ë“œë³„ë¡œ evidence í† í°(to_token)ì— ëŒ€í•œ ì–´í…ì…˜ í‰ê·  ê³„ì‚°
    # attn shape: (head, from_token, to_token)
    head_count = attn.shape[0]
    avg_evidence_attention = []
    for h in range(head_count):
        # from_token ì „ì²´ì—ì„œ evidence í† í°(to_token)ìœ¼ë¡œ ê°€ëŠ” ì–´í…ì…˜ í‰ê· 
        if evidence_indices:  # evidence_indicesê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
            try:
                avg = attn[h, :, evidence_indices].mean()
            except (IndexError, ValueError):
                avg = 0.0  # ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ ì‹œ 0 ë°˜í™˜
        else:
            avg = 0.0  # evidence_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ 0 ë°˜í™˜
        avg_evidence_attention.append(avg)
    avg_evidence_attention = np.array(avg_evidence_attention)

    # íˆíŠ¸ë§µ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 1.5))
    sns.heatmap(avg_evidence_attention[None, :], annot=True, fmt=".2f", cmap="YlOrRd", cbar=True, ax=ax)
    ax.set_xlabel("Head index")
    ax.set_yticks([])
    ax.set_title("Average Evidence Attention by Head")
    plt.tight_layout()
    return fig, avg_evidence_attention

def plot_attention_head_token_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í—¤ë“œë³„ë¡œ to_token(ëª¨ë“  í† í°)ì— ëŒ€í•œ ì–´í…ì…˜ í‰ê·  (from_token ì „ì²´ í‰ê· )
    # shape: (head, to_token)
    avg_attn = attn.mean(axis=1)  # (head, to_token)

    fig, ax = plt.subplots(figsize=(min(1.2+0.4*len(tokens), 16), 1.5+0.3*avg_attn.shape[0]))
    sns.heatmap(avg_attn, annot=False, cmap="YlGnBu", cbar=True, ax=ax)
    ax.set_xlabel("Token index")
    ax.set_ylabel("Head index")
    ax.set_title("Average Token Attention by Head")
    # xì¶•ì— í† í° ë¼ë²¨ í‘œì‹œ (ê¸¸ë©´ ì˜ë¦¼)
    token_labels = [t if i not in evidence_indices else f"*{t}*" for i, t in enumerate(tokens)]
    ax.set_xticks(range(len(tokens)))
    ax.set_xticklabels(token_labels, rotation=90, fontsize=8)
    # evidence í† í° ê°•ì¡° (xì¶• ë¼ë²¨ ìƒ‰ìƒ)
    for idx in evidence_indices:
        if idx < len(ax.get_xticklabels()):  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
            ax.get_xticklabels()[idx].set_color("red")
            ax.get_xticklabels()[idx].set_fontweight("bold")
    plt.tight_layout()
    return fig, avg_attn

def plot_token_head_attention_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í† í°ë³„ë¡œ í—¤ë“œ ì–´í…ì…˜ í‰ê·  (from_token ì „ì²´ í‰ê· )
    # shape: (head, to_token)
    avg_attn = attn.mean(axis=1)  # (head, to_token)
    # shapeë¥¼ (to_token, head)ë¡œ transpose
    avg_attn_t = avg_attn.T  # (to_token, head)
    fig, ax = plt.subplots(figsize=(min(1.2+0.4*len(tokens), 16), 1.5+0.3*avg_attn_t.shape[1]))
    sns.heatmap(avg_attn_t, annot=False, cmap="YlGnBu", cbar=True, ax=ax)
    ax.set_ylabel("Token index")
    ax.set_xlabel("Head index")
    ax.set_title("Average Head Attention by Token")
    # yì¶•ì— í† í° ë¼ë²¨ í‘œì‹œ (ê¸¸ë©´ ì˜ë¦¼)
    token_labels = [t if i not in evidence_indices else f"*{t}*" for i, t in enumerate(tokens)]
    ax.set_yticks(range(len(tokens)))
    ax.set_yticklabels(token_labels, rotation=0, fontsize=8)
    # evidence í† í° ê°•ì¡° (yì¶• ë¼ë²¨ ìƒ‰ìƒ)
    for idx in evidence_indices:
        if idx < len(ax.get_yticklabels()):  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
            ax.get_yticklabels()[idx].set_color("red")
            ax.get_yticklabels()[idx].set_fontweight("bold")
    plt.tight_layout()
    return fig, avg_attn_t

def analyze_head_attention_pattern(attn, tokens, evidence_indices, target_head=27):
    """
    íŠ¹ì • í—¤ë“œì˜ attention íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ evidence í† í°ì—ë§Œ íŠ¹ë³„íˆ ë°˜ì‘í•˜ëŠ”ì§€ í™•ì¸
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    target_head: ë¶„ì„í•  í—¤ë“œ ë²ˆí˜¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    if target_head >= attn.shape[0]:
        return None, None, None
    
    # íŠ¹ì • í—¤ë“œì˜ attention íŒ¨í„´ (from_token ì „ì²´ í‰ê· )
    head_attention = attn[target_head].mean(axis=0)  # (to_token,)
    
    # evidence í† í°ê³¼ non-evidence í† í° ë¶„ë¦¬
    evidence_attention = head_attention[evidence_indices] if evidence_indices else np.array([])
    non_evidence_indices = [i for i in range(len(tokens)) if i not in evidence_indices]
    non_evidence_attention = head_attention[non_evidence_indices] if non_evidence_indices else np.array([])
    
    # í†µê³„ ê³„ì‚°
    stats = {
        'evidence_mean': float(evidence_attention.mean()) if len(evidence_attention) > 0 else 0.0,
        'evidence_std': float(evidence_attention.std()) if len(evidence_attention) > 0 else 0.0,
        'non_evidence_mean': float(non_evidence_attention.mean()) if len(non_evidence_attention) > 0 else 0.0,
        'non_evidence_std': float(non_evidence_attention.std()) if len(non_evidence_attention) > 0 else 0.0,
        'evidence_count': len(evidence_attention),
        'non_evidence_count': len(non_evidence_attention),
        'attention_ratio': float(evidence_attention.mean() / non_evidence_attention.mean()) if len(non_evidence_attention) > 0 and non_evidence_attention.mean() > 0 else 0.0
    }
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # 1. ì „ì²´ attention íŒ¨í„´
    ax1.bar(range(len(tokens)), head_attention, alpha=0.7, color='lightblue')
    # evidence í† í° ê°•ì¡°
    for idx in evidence_indices:
        ax1.bar(idx, head_attention[idx], color='red', alpha=0.8)
    ax1.set_xlabel('Token Index')
    ax1.set_ylabel('Attention Weight')
    ax1.set_title(f'Head {target_head} Attention Pattern')
    ax1.set_xticks(range(len(tokens)))
    ax1.set_xticklabels([t[:10] + '...' if len(t) > 10 else t for t in tokens], rotation=45, ha='right')
    
    # 2. Evidence vs Non-evidence ë¹„êµ
    categories = ['Evidence Tokens', 'Non-Evidence Tokens']
    means = [stats['evidence_mean'], stats['non_evidence_mean']]
    stds = [stats['evidence_std'], stats['non_evidence_std']]
    
    bars = ax2.bar(categories, means, yerr=stds, capsize=5, alpha=0.7, color=['red', 'lightblue'])
    ax2.set_ylabel('Average Attention Weight')
    ax2.set_title(f'Head {target_head}: Evidence vs Non-Evidence Attention')
    
    # ê°’ í‘œì‹œ
    for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.001, 
                f'{mean:.4f}\nÂ±{std:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    return fig, stats, head_attention

def batch_domain_experiment_multi_models(model_names, files, num_prompts=20):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì— ëŒ€í•´ evidence ì–´í…ì…˜ ì‹¤í—˜ì„ ì¼ê´„ ìˆ˜í–‰í•˜ê³  í†µê³„ ì§‘ê³„
    model_names: ì‚¬ìš©í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸
    files: {domain: filename} í˜•íƒœì˜ ì„ íƒëœ íŒŒì¼ë“¤
    num_prompts: ë„ë©”ì¸ë³„ ìƒ˜í”Œë§í•  í”„ë¡¬í”„íŠ¸ ê°œìˆ˜
    """
    import time
    from datetime import datetime, timedelta
    
    all_results = []
    error_logs = []  # ì—ëŸ¬ ë¡œê·¸ ì €ì¥ìš©
    
    # ì „ì²´ ì‘ì—…ëŸ‰ ê³„ì‚° (ëª¨ë¸ ìˆ˜ * ë„ë©”ì¸ ìˆ˜ * í”„ë¡¬í”„íŠ¸ ìˆ˜)
    total_tasks = len(model_names) * len(files) * num_prompts
    completed_tasks = 0
    start_time = time.time()
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ëª¨ë¸ë³„ ì§„í–‰ ìƒí™© ì¶”ì 
    model_progress = {}
    for model_name in model_names:
        model_progress[model_name] = {}
        for domain in files.keys():
            model_progress[model_name][domain] = 0
    
    for model_name in model_names:
        st.info(f"ğŸ”„ {model_name} ëª¨ë¸ ì‹¤í—˜ ì‹œì‘...")
        
        # ê° ëª¨ë¸ì— ëŒ€í•´ ì‹¤í—˜ ìˆ˜í–‰
        model_results = batch_domain_experiment_single_model(model_name, files, num_prompts, 
                                                           progress_bar, status_text, 
                                                           model_progress, completed_tasks, 
                                                           total_tasks, start_time)
        
        all_results.extend(model_results)
        
        # ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
        if model_results:
            save_experiment_result(model_results, model_name)
            st.success(f"âœ… {model_name} ëª¨ë¸ ì‹¤í—˜ ì™„ë£Œ! {len(model_results)}ê°œ ê²°ê³¼")
        
        # ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        unload_model_from_session()
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if all_results:
        st.success(f"ğŸ‰ ëª¨ë“  ëª¨ë¸ ì‹¤í—˜ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ê²°ê³¼")
        
        # ëª¨ë¸ë³„ ê²°ê³¼ ìš”ì•½
        model_summary = {}
        for result in all_results:
            model_name = result['model_name']
            if model_name not in model_summary:
                model_summary[model_name] = {'count': 0, 'domains': set()}
            model_summary[model_name]['count'] += 1
            model_summary[model_name]['domains'].add(result['domain'])
        
        st.subheader("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        for model_name, summary in model_summary.items():
            domains_str = ', '.join(sorted(summary['domains']))
            st.info(f"**{model_name}**: {summary['count']}ê°œ ê²°ê³¼ ({domains_str} ë„ë©”ì¸)")
    
    return all_results

def batch_domain_experiment_single_model(model_name, files, num_prompts=20, 
                                       progress_bar=None, status_text=None, 
                                       model_progress=None, completed_tasks=0, 
                                       total_tasks=0, start_time=None):
    """
    ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´ evidence ì–´í…ì…˜ ì‹¤í—˜ì„ ìˆ˜í–‰
    """
    import time
    from datetime import datetime, timedelta
    
    results = []
    error_logs = []  # ì—ëŸ¬ ë¡œê·¸ ì €ì¥ìš©
    
    # ëª¨ë¸ ë¡œë“œ
    with st.spinner(f"{model_name} ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘..."):
        success = load_model_to_session(model_name)
        if not success:
            st.error(f"âŒ {model_name} ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return results
    
    # ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'model' in st.session_state and 'tokenizer' in st.session_state:
        model = st.session_state['model']
        tokenizer = st.session_state['tokenizer']
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì‹¤ì œë¡œ Noneì´ ì•„ë‹Œì§€ í™•ì¸
        if model is None or tokenizer is None:
            st.error("ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ Noneì…ë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return results
    else:
        st.error("ëª¨ë¸ì´ ì„¸ì…˜ì— ë¡œë“œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì§„í–‰í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return results

    # ë„ë©”ì¸ë³„ ì§„í–‰ ìƒí™© ì¶”ì 
    domain_progress = {}
    for domain in files.keys():
        domain_progress[domain] = 0
    
    for domain, selected_file in files.items():
        if not selected_file:
            continue
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 10000ê°œ)
        prompts = get_model_prompts(model_name, domain, selected_file, max_count=10000)
        if not prompts:
            continue
        
        # í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œë§
        sampled = prompts[:num_prompts]
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ê²½ë¡œ ì‚¬ìš©
        path = os.path.join(get_model_dataset_path(model_name), domain, selected_file)
        try:
            with open(path, "r") as f:
                lines = f.readlines()
        except Exception as e:
            continue
            
        domain_results = 0
        for i, prompt in enumerate(sampled):
            try:
                idx = prompts.index(prompt)
                data = json.loads(lines[idx])
                evidence_indices = data.get("evidence_indices", [])
                
                # ì–´í…ì…˜ ì¶”ì¶œ
                inputs = tokenizer(prompt, return_tensors="pt")
                # ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì…ë ¥ ì´ë™
                try:
                    device = next(model.parameters()).device
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                except Exception as device_error:
                    # ë””ë°”ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨ ì‹œ CPU ì‚¬ìš©
                    inputs = {k: v.to('cpu') for k, v in inputs.items()}
                
                with torch.no_grad():
                    try:
                        outputs = model(**inputs, output_attentions=True)
                        attentions = tuple(attn.cpu().numpy() for attn in outputs.attentions)
                    except Exception as attn_error:
                        continue
                tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
                
                # evidence_indices íƒ€ì… í™•ì¸ ë° ì•ˆì „í•œ ì²˜ë¦¬
                if isinstance(evidence_indices, list):
                    evidence_indices = [i for i in evidence_indices if isinstance(i, (int, float)) and i < len(tokens)]
                else:
                    # evidence_indicesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
                    evidence_indices = []
                
                last_attn = attentions[-1][0]  # (head, from_token, to_token)
                # í—¤ë“œë³„ evidence ì–´í…ì…˜ í‰ê· 
                head_count = last_attn.shape[0]
                avg_evidence_attention_whole = []
                for h in range(head_count):
                    if evidence_indices:  # evidence_indicesê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
                        try:
                            avg = last_attn[h, :, evidence_indices].mean()
                        except (IndexError, ValueError):
                            avg = 0.0  # ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ ì‹œ 0 ë°˜í™˜
                    else:
                        avg = 0.0  # evidence_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ 0 ë°˜í™˜
                    avg_evidence_attention_whole.append(avg)
                max_head = int(np.argmax(avg_evidence_attention_whole))
                results.append({
                    "domain": domain,
                    "prompt": prompt,
                    "max_head": max_head,
                    "avg_evidence_attention": avg_evidence_attention_whole[max_head],  # ê¸°ì¡´ maxê°’ (í˜¸í™˜ì„± ìœ ì§€)
                    "avg_evidence_attention_whole": avg_evidence_attention_whole,  # 32ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì €ì¥
                    "evidence_indices": evidence_indices,
                    "tokens": tokens,
                    "model_name": model_name,
                    "tokenizer_name": tokenizer.name_or_path if hasattr(tokenizer, 'name_or_path') else "unknown"
                })
                domain_results += 1
                domain_progress[domain] = domain_results
                if model_progress and model_name in model_progress:
                    model_progress[model_name][domain] = domain_results
                completed_tasks += 1
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if progress_bar and status_text and start_time:
                    elapsed_time = time.time() - start_time
                    if completed_tasks > 0:
                        avg_time_per_task = elapsed_time / completed_tasks
                        remaining_tasks = total_tasks - completed_tasks
                        estimated_remaining_time = remaining_tasks * avg_time_per_task
                        
                        # ì‹œê°„ í¬ë§·íŒ…
                        elapsed_str = str(timedelta(seconds=int(elapsed_time)))
                        remaining_str = str(timedelta(seconds=int(estimated_remaining_time)))
                        
                        # í”„ë¡œê·¸ë ˆìŠ¤ë°”ì™€ ìƒíƒœ ì •ë³´ ì—…ë°ì´íŠ¸
                        progress_bar.progress(completed_tasks / total_tasks)
                        
                        # ëª¨ë“  ëª¨ë¸ê³¼ ë„ë©”ì¸ì˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                        progress_info = []
                        for m, domains in model_progress.items():
                            for d, progress in domains.items():
                                progress_info.append(f"{m}-{d}: {progress}/{num_prompts}")
                        
                        status_text.write(f"**ì†Œìš”ì‹œê°„: {elapsed_str} / ë‚¨ì€ ì‹œê°„: {remaining_str}**  \n**{' | '.join(progress_info)}**")
                
            except Exception as e:
                completed_tasks += 1
                # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                error_log = {
                    "model_name": model_name,
                    "domain": domain,
                    "prompt_index": i+1,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                error_logs.append(error_log)
                
                # ì—ëŸ¬ ë¡œê·¸ë¥¼ ìƒíƒœ í…ìŠ¤íŠ¸ì— ì¶”ê°€
                if progress_bar and status_text and start_time:
                    elapsed_time = time.time() - start_time
                    if completed_tasks > 0:
                        avg_time_per_task = elapsed_time / completed_tasks
                        remaining_tasks = total_tasks - completed_tasks
                        estimated_remaining_time = remaining_tasks * avg_time_per_task
                        elapsed_str = str(timedelta(seconds=int(elapsed_time)))
                        remaining_str = str(timedelta(seconds=int(estimated_remaining_time)))
                    else:
                        elapsed_str = "0:00:00"
                        remaining_str = "0:00:00"
                    
                    error_msg = f"âŒ {model_name}-{domain} ë„ë©”ì¸ {i+1}ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                    
                    # ëª¨ë“  ëª¨ë¸ê³¼ ë„ë©”ì¸ì˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                    progress_info = []
                    for m, domains in model_progress.items():
                        for d, progress in domains.items():
                            progress_info.append(f"{m}-{d}: {progress}/{num_prompts}")
                    
                    status_text.write(f"**ì†Œìš”ì‹œê°„: {elapsed_str} / ë‚¨ì€ ì‹œê°„: {remaining_str}**  \n**{' | '.join(progress_info)}**  \n{error_msg}")
                continue

    # ì—ëŸ¬ ë¡œê·¸ê°€ ìˆìœ¼ë©´ íŒŒì¼ì— ì €ì¥
    if error_logs:
        experiment_path = get_model_experiment_path(model_name)
        os.makedirs(experiment_path, exist_ok=True)
        now = datetime.now().strftime("%Y%m%d_%H%M%S")
        error_log_file = f"{experiment_path}/{now}_{model_name.replace(':','_')}_errors.json"
        with open(error_log_file, "w", encoding="utf-8") as f:
            json.dump(error_logs, f, ensure_ascii=False, indent=2)
        
        # ì—ëŸ¬ ê°œìˆ˜ í‘œì‹œ
        if status_text:
            status_text.write(f"âœ… {model_name} ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ê²°ê³¼, {len(error_logs)}ê°œ ì—ëŸ¬ ë°œìƒ  \nì—ëŸ¬ ë¡œê·¸: {error_log_file}")
    
    return results

def batch_domain_experiment(model_name, files, num_prompts=20):
    """
    ì—¬ëŸ¬ ë„ë©”ì¸ì— ëŒ€í•´ evidence ì–´í…ì…˜ ì‹¤í—˜ì„ ì¼ê´„ ìˆ˜í–‰í•˜ê³  í†µê³„ ì§‘ê³„ (ë‹¨ì¼ ëª¨ë¸ìš©)
    model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
    files: {domain: filename} í˜•íƒœì˜ ì„ íƒëœ íŒŒì¼ë“¤
    num_prompts: ë„ë©”ì¸ë³„ ìƒ˜í”Œë§í•  í”„ë¡¬í”„íŠ¸ ê°œìˆ˜
    """
    import time
    from datetime import datetime, timedelta
    
    results = []
    error_logs = []  # ì—ëŸ¬ ë¡œê·¸ ì €ì¥ìš©
    
    # ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'model' in st.session_state and 'tokenizer' in st.session_state:
        model = st.session_state['model']
        tokenizer = st.session_state['tokenizer']
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì‹¤ì œë¡œ Noneì´ ì•„ë‹Œì§€ í™•ì¸
        if model is None or tokenizer is None:
            st.error("ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ Noneì…ë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return results
    else:
        st.error("ëª¨ë¸ì´ ì„¸ì…˜ì— ë¡œë“œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì§„í–‰í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return results

    # ì „ì²´ ì‘ì—…ëŸ‰ ê³„ì‚°
    total_tasks = len(files) * num_prompts
    completed_tasks = 0
    start_time = time.time()
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ë„ë©”ì¸ë³„ ì§„í–‰ ìƒí™© ì¶”ì 
    domain_progress = {}
    for domain in files.keys():
        domain_progress[domain] = 0
    
    for domain, selected_file in files.items():
        if not selected_file:
            continue
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 10000ê°œ)
        prompts = get_model_prompts(model_name, domain, selected_file, max_count=10000)
        if not prompts:
            continue
        
        # í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œë§
        sampled = prompts[:num_prompts]
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ê²½ë¡œ ì‚¬ìš©
        path = os.path.join(get_model_dataset_path(model_name), domain, selected_file)
        try:
            with open(path, "r") as f:
                lines = f.readlines()
        except Exception as e:
            continue
            
        domain_results = 0
        for i, prompt in enumerate(sampled):
            try:
                idx = prompts.index(prompt)
                data = json.loads(lines[idx])
                evidence_indices = data.get("evidence_indices", [])
                
                # ì–´í…ì…˜ ì¶”ì¶œ
                inputs = tokenizer(prompt, return_tensors="pt")
                # ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì…ë ¥ ì´ë™
                try:
                    device = next(model.parameters()).device
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                except Exception as device_error:
                    # ë””ë°”ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨ ì‹œ CPU ì‚¬ìš©
                    inputs = {k: v.to('cpu') for k, v in inputs.items()}
                
                with torch.no_grad():
                    try:
                        outputs = model(**inputs, output_attentions=True)
                        attentions = tuple(attn.cpu().numpy() for attn in outputs.attentions)
                    except Exception as attn_error:
                        continue
                tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
                
                # evidence_indices íƒ€ì… í™•ì¸ ë° ì•ˆì „í•œ ì²˜ë¦¬
                if isinstance(evidence_indices, list):
                    evidence_indices = [i for i in evidence_indices if isinstance(i, (int, float)) and i < len(tokens)]
                else:
                    # evidence_indicesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
                    evidence_indices = []
                
                last_attn = attentions[-1][0]  # (head, from_token, to_token)
                # í—¤ë“œë³„ evidence ì–´í…ì…˜ í‰ê· 
                head_count = last_attn.shape[0]
                avg_evidence_attention_whole = []
                for h in range(head_count):
                    if evidence_indices:  # evidence_indicesê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
                        try:
                            avg = last_attn[h, :, evidence_indices].mean()
                        except (IndexError, ValueError):
                            avg = 0.0  # ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ ì‹œ 0 ë°˜í™˜
                    else:
                        avg = 0.0  # evidence_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ 0 ë°˜í™˜
                    avg_evidence_attention_whole.append(avg)
                max_head = int(np.argmax(avg_evidence_attention_whole))
                results.append({
                    "domain": domain,
                    "prompt": prompt,
                    "max_head": max_head,
                    "avg_evidence_attention": avg_evidence_attention_whole[max_head],  # ê¸°ì¡´ maxê°’ (í˜¸í™˜ì„± ìœ ì§€)
                    "avg_evidence_attention_whole": avg_evidence_attention_whole,  # 32ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì €ì¥
                    "evidence_indices": evidence_indices,
                    "tokens": tokens,
                    "model_name": model_name,
                    "tokenizer_name": tokenizer.name_or_path if hasattr(tokenizer, 'name_or_path') else "unknown"
                })
                domain_results += 1
                domain_progress[domain] = domain_results
                completed_tasks += 1
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                elapsed_time = time.time() - start_time
                if completed_tasks > 0:
                    avg_time_per_task = elapsed_time / completed_tasks
                    remaining_tasks = total_tasks - completed_tasks
                    estimated_remaining_time = remaining_tasks * avg_time_per_task
                    
                    # ì‹œê°„ í¬ë§·íŒ…
                    elapsed_str = str(timedelta(seconds=int(elapsed_time)))
                    remaining_str = str(timedelta(seconds=int(estimated_remaining_time)))
                    
                    # í”„ë¡œê·¸ë ˆìŠ¤ë°”ì™€ ìƒíƒœ ì •ë³´ ì—…ë°ì´íŠ¸
                    progress_bar.progress(completed_tasks / total_tasks)
                    
                    # ëª¨ë“  ë„ë©”ì¸ì˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                    progress_info = []
                    for d, progress in domain_progress.items():
                        progress_info.append(f"{d}: {progress}/{num_prompts}")
                    
                    status_text.write(f"**ì†Œìš”ì‹œê°„: {elapsed_str} / ë‚¨ì€ ì‹œê°„: {remaining_str}**  \n**{' | '.join(progress_info)}**")
                
            except Exception as e:
                completed_tasks += 1
                # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                error_log = {
                    "domain": domain,
                    "prompt_index": i+1,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                error_logs.append(error_log)
                
                # ì—ëŸ¬ ë¡œê·¸ë¥¼ ìƒíƒœ í…ìŠ¤íŠ¸ì— ì¶”ê°€
                elapsed_time = time.time() - start_time
                if completed_tasks > 0:
                    avg_time_per_task = elapsed_time / completed_tasks
                    remaining_tasks = total_tasks - completed_tasks
                    estimated_remaining_time = remaining_tasks * avg_time_per_task
                    elapsed_str = str(timedelta(seconds=int(elapsed_time)))
                    remaining_str = str(timedelta(seconds=int(estimated_remaining_time)))
                else:
                    elapsed_str = "0:00:00"
                    remaining_str = "0:00:00"
                
                error_msg = f"âŒ {domain} ë„ë©”ì¸ {i+1}ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                
                # ëª¨ë“  ë„ë©”ì¸ì˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_info = []
                for d, progress in domain_progress.items():
                    progress_info.append(f"{d}: {progress}/{num_prompts}")
                
                status_text.write(f"**ì†Œìš”ì‹œê°„: {elapsed_str} / ë‚¨ì€ ì‹œê°„: {remaining_str}**  \n**{' | '.join(progress_info)}**  \n{error_msg}")
                continue

    # ì—ëŸ¬ ë¡œê·¸ê°€ ìˆìœ¼ë©´ íŒŒì¼ì— ì €ì¥
    if error_logs:
        experiment_path = get_model_experiment_path(model_name)
        os.makedirs(experiment_path, exist_ok=True)
        now = datetime.now().strftime("%Y%m%d_%H%M%S")
        error_log_file = f"{experiment_path}/{now}_{model_name.replace(':','_')}_errors.json"
        with open(error_log_file, "w", encoding="utf-8") as f:
            json.dump(error_logs, f, ensure_ascii=False, indent=2)
        
        # ì—ëŸ¬ ê°œìˆ˜ í‘œì‹œ
        status_text.write(f"âœ… ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ê²°ê³¼, {len(error_logs)}ê°œ ì—ëŸ¬ ë°œìƒ  \nì—ëŸ¬ ë¡œê·¸: {error_log_file}")
    
    return results

def save_experiment_result(results, model_name):
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ experiment_results í´ë”ì— ì €ì¥ (numpy íƒ€ì…ì„ ëª¨ë‘ ë³€í™˜)"""
    # ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    experiment_path = get_model_experiment_path(model_name)
    os.makedirs(experiment_path, exist_ok=True)
    
    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{experiment_path}/{now}_{model_name.replace(':','_')}.json"
    
    # numpy íƒ€ì…ì„ ëª¨ë‘ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    def convert(o):
        if isinstance(o, np.generic):
            return o.item()
        if isinstance(o, (list, tuple)):
            return [convert(x) for x in o]
        if isinstance(o, dict):
            return {k: convert(v) for k, v in o.items()}
        return o
    results_converted = [convert(r) for r in results]
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results_converted, f, ensure_ascii=False, indent=2)
    return filename

def list_model_experiment_results(model_name):
    """íŠ¹ì • ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    if not os.path.exists(experiment_path):
        return []
    
    files = [f for f in os.listdir(experiment_path) if f.endswith(".json")]
    files.sort(reverse=True)
    return files

def load_model_experiment_result(model_name, filename):
    """íŠ¹ì • ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    path = os.path.join(experiment_path, filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def get_all_model_experiments():
    """ëª¨ë“  ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    experiment_root = "experiment_results"
    if not os.path.exists(experiment_root):
        return {}
    
    model_experiments = {}
    for model_dir in os.listdir(experiment_root):
        model_path = os.path.join(experiment_root, model_dir)
        if os.path.isdir(model_path):
            # ë””ë ‰í† ë¦¬ëª…ì„ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€í™˜
            model_name = model_dir.replace('_', '/').replace('_', ':')
            files = [f for f in os.listdir(model_path) if f.endswith(".json")]
            if files:
                model_experiments[model_name] = files
    
    return model_experiments

def check_model_loaded():
    """
    ì„œë²„ ë©”ëª¨ë¦¬ì— ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì„¸ì…˜ì— ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ ìºì‹œì—ì„œ ë³µêµ¬í•˜ê³ , ê·¸ê²ƒë„ ì—†ìœ¼ë©´ íŒŒì¼ ìºì‹œì—ì„œ ë³µêµ¬í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ë¨¼ì € ì„¸ì…˜ ìƒíƒœ í™•ì¸
        if 'model' in st.session_state and 'tokenizer' in st.session_state:
            model = st.session_state['model']
            if model is not None:
                return True, st.session_state.get('model_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸')
        
        # 2. ì„¸ì…˜ì— ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ ìºì‹œì—ì„œ ë³µêµ¬
        if MODEL_CACHE["model"] is not None and MODEL_CACHE["tokenizer"] is not None:
            st.session_state['model'] = MODEL_CACHE["model"]
            st.session_state['tokenizer'] = MODEL_CACHE["tokenizer"]
            st.session_state['model_name'] = MODEL_CACHE["model_name"]
            return True, MODEL_CACHE["model_name"]
        
        # 3. ê¸€ë¡œë²Œ ìºì‹œì—ë„ ì—†ìœ¼ë©´ íŒŒì¼ ìºì‹œì—ì„œ ë³µêµ¬ ì‹œë„
        cache_info = load_model_cache()
        if cache_info and cache_info.get('model_name'):
            model_name = cache_info['model_name']
            # ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œ
            if load_model_to_session(model_name):
                return True, model_name
        
        # 4. ì„¸ì…˜ì— ëª¨ë¸ ì´ë¦„ë§Œ ìˆê³  ì‹¤ì œ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° (ìƒˆë¡œê³ ì¹¨ í›„)
        if 'model_name' in st.session_state and st.session_state['model_name']:
            model_name = st.session_state['model_name']
            # ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œ
            if load_model_to_session(model_name):
                return True, model_name
        
        return False, None
    except Exception as e:
        print(f"ëª¨ë¸ ì²´í¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return False, None

def show():
    # í˜ì´ì§€ ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ëª¨ë¸ ìƒíƒœ ë³µêµ¬ ì‹œë„
    if 'model_loaded' not in st.session_state:
        st.session_state.model_loaded = False
    if 'current_model_name' not in st.session_state:
        st.session_state.current_model_name = None
    if 'dataset_files_cache' not in st.session_state:
        st.session_state.dataset_files_cache = {}
    if 'model_dataset_files_cache' not in st.session_state:
        st.session_state.model_dataset_files_cache = {}
    
    st.title("ğŸ”¬ Attention Pattern Experiment")
    
    # ëª¨ë¸ ì„ íƒ ì„¹ì…˜
    st.subheader("ğŸ¤– Model Selection")
    
    # ëª¨ë¸ ëª©ë¡ (ìºì‹œë¨)
    available_models = get_available_models()
    
    if not available_models:
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Model Load íƒ­ì—ì„œ ëª¨ë¸ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    # ì‹¤í—˜ ëª¨ë“œ ì„ íƒ
    experiment_mode = st.radio(
        "ì‹¤í—˜ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”",
        ["ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜", "ë‹¤ì¤‘ ëª¨ë¸ ì‹¤í—˜"],
        key="experiment_mode_selector"
    )
    
    if experiment_mode == "ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜":
        # ë‹¨ì¼ ëª¨ë¸ ì„ íƒ
        selected_model = st.selectbox(
            "ì‹¤í—˜í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
            available_models,
            key="experiment_model_selector"
        )
        
        # ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ë²„íŠ¼
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“¥ ëª¨ë¸ ë¡œë“œ", type="primary", key="load_model_btn"):
                if selected_model != st.session_state.current_model_name:
                    # ë‹¤ë¥¸ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê²½ìš°
                    if st.session_state.model_loaded:
                        unload_model_from_session()
                    
                    with st.spinner(f"{selected_model} ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘..."):
                        success = load_model_to_session(selected_model)
                        if success:
                            st.session_state.model_loaded = True
                            st.session_state.current_model_name = selected_model
                            st.success(f"âœ… {selected_model} ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            st.error(f"âŒ {selected_model} ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.info("ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì…ë‹ˆë‹¤.")
        
        with col2:
            if st.button("ğŸ“¤ ëª¨ë¸ ì–¸ë¡œë“œ", type="secondary", key="unload_model_btn"):
                if st.session_state.model_loaded:
                    unload_model_from_session()
                    st.session_state.model_loaded = False
                    st.session_state.current_model_name = None
                    st.success("âœ… ëª¨ë¸ì´ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.info("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        with col3:
            if st.button("ğŸ”„ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", type="secondary", key="refresh_models_btn"):
                get_available_models.clear()
                st.success("ëª¨ë¸ ëª©ë¡ì´ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ í‘œì‹œ
        if st.session_state.model_loaded:
            st.success(f"âœ… í˜„ì¬ ë¡œë“œëœ ëª¨ë¸: {st.session_state.current_model_name}")
        else:
            st.warning("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        selected_models = [selected_model]
        
    else:
        # ë‹¤ì¤‘ ëª¨ë¸ ì„ íƒ
        st.markdown("**ğŸ”§ ì‹¤í—˜í•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)**")
        selected_models = st.multiselect(
            "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤",
            available_models,
            default=[available_models[0]] if available_models else [],
            help="ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
        )
        
        if selected_models:
            st.info(f"ì„ íƒëœ ëª¨ë¸: {', '.join(selected_models)}")
            
            # ëª¨ë¸ë³„ ìƒíƒœ í™•ì¸
            st.markdown("**ğŸ“Š ëª¨ë¸ ìƒíƒœ í™•ì¸**")
            for model in selected_models:
                # ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸ (ì‹¤ì œ ë¡œë“œí•˜ì§€ ì•Šê³ )
                try:
                    response = requests.post(
                        f"http://localhost:11434/api/generate",
                        json={
                            "model": model,
                            "prompt": "test",
                            "stream": False
                        },
                        timeout=5
                    )
                    if response.status_code == 200:
                        st.success(f"âœ… {model} (ì‚¬ìš© ê°€ëŠ¥)")
                    else:
                        st.warning(f"âš ï¸ {model} (ì‘ë‹µ ì˜¤ë¥˜)")
                except:
                    st.warning(f"âš ï¸ {model} (ì—°ê²° ì‹¤íŒ¨)")
        else:
            st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            selected_models = []
        
        # ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
        if st.button("ğŸ”„ ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", type="secondary", key="refresh_models_multi_btn"):
            get_available_models.clear()
            st.success("ëª¨ë¸ ëª©ë¡ì´ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
    
    # ë°ì´í„°ì…‹ ì„ íƒ ì„¹ì…˜
    st.subheader("ğŸ“Š Dataset Selection")
    
    # ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡ (ìºì‹œëœ ê²½ìš° ì‚¬ìš©)
    cache_key = f"dataset_files_{selected_model}"
    if cache_key not in st.session_state.dataset_files_cache:
        st.session_state.dataset_files_cache[cache_key] = get_model_dataset_files(selected_model)
    
    dataset_files = st.session_state.dataset_files_cache[cache_key]
    
    # ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ íŒŒì¼ ì„ íƒ
    domains = ["economy", "technical", "legal", "medical"]
    selected_files = {}
    
    for domain in domains:
        files = dataset_files.get(domain, [])
        if files:
            selected_file = st.selectbox(
                f"{domain.capitalize()} ë„ë©”ì¸ ë°ì´í„°ì…‹",
                files,
                key=f"file_selector_{domain}"
            )
            selected_files[domain] = selected_file
        else:
            st.warning(f"{domain.capitalize()} ë„ë©”ì¸ì— ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì‹¤í—˜ ì„¤ì •
    st.subheader("âš™ï¸ Experiment Settings")
    
    col4, col5 = st.columns(2)
    
    with col4:
        num_prompts = st.number_input(
            "ì‹¤í—˜í•  í”„ë¡¬í”„íŠ¸ ìˆ˜",
            min_value=1,
            max_value=10000,
            value=20,
            help="ê° ë„ë©”ì¸ë³„ë¡œ ì‹¤í—˜í•  í”„ë¡¬í”„íŠ¸ì˜ ê°œìˆ˜ (ìµœëŒ€ 10000ê°œ, ê¶Œì¥: 20-50ê°œ)"
        )
    
    with col5:
        batch_mode = st.checkbox(
            "ë°°ì¹˜ ëª¨ë“œ",
            value=True,
            help="ëª¨ë“  ë„ë©”ì¸ì— ëŒ€í•´ í•œ ë²ˆì— ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
        )
    
    # ì‹¤í—˜ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸš€ ì‹¤í—˜ ì‹œì‘", type="primary", key="start_experiment_btn"):
        if not selected_models:
            st.error("ëª¨ë¸ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        if not selected_files:
            st.error("ì‹¤í—˜í•  ë°ì´í„°ì…‹ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì‹¤í—˜ ì‹¤í–‰
        try:
            if experiment_mode == "ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜":
                # ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜
                if not st.session_state.model_loaded:
                    st.error("ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
                    return
                
                if batch_mode:
                    results = batch_domain_experiment(selected_models[0], selected_files, num_prompts)
                    if results:
                        save_experiment_result(results, selected_models[0])
                        st.success(f"âœ… ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ê²°ê³¼")
                    else:
                        st.warning("âš ï¸ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.info("ë‹¨ì¼ ë„ë©”ì¸ ì‹¤í—˜ ëª¨ë“œëŠ” ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                # ë‹¤ì¤‘ ëª¨ë¸ ì‹¤í—˜
                st.info(f"ğŸ”„ {len(selected_models)}ê°œ ëª¨ë¸ì— ëŒ€í•´ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                results = batch_domain_experiment_multi_models(selected_models, selected_files, num_prompts)
                if results:
                    st.success(f"âœ… ëª¨ë“  ëª¨ë¸ ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ê²°ê³¼")
                else:
                    st.warning("âš ï¸ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ì‹¤í—˜ ê²°ê³¼ í™•ì¸
    st.subheader("ğŸ“‹ Experiment Results")
    
    if experiment_mode == "ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜":
        # ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜ ê²°ê³¼
        experiment_results = list_model_experiment_results(selected_models[0])
        
        if experiment_results:
            selected_result = st.selectbox(
                "í™•ì¸í•  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì„ íƒí•˜ì„¸ìš”",
                experiment_results,
                key="result_selector"
            )
            
            if selected_result:
                result_data = load_model_experiment_result(selected_models[0], selected_result)
                if result_data:
                    st.json(result_data)
        else:
            st.info("ì•„ì§ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ë‹¤ì¤‘ ëª¨ë¸ ì‹¤í—˜ ê²°ê³¼
        st.markdown("**ğŸ“Š ëª¨ë“  ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼**")
        
        for model_name in selected_models:
            with st.expander(f"ğŸ“‹ {model_name} ì‹¤í—˜ ê²°ê³¼", expanded=False):
                experiment_results = list_model_experiment_results(model_name)
                
                if experiment_results:
                    selected_result = st.selectbox(
                        f"{model_name} ê²°ê³¼ ì„ íƒ",
                        experiment_results,
                        key=f"result_selector_{model_name}"
                    )
                    
                    if selected_result:
                        result_data = load_model_experiment_result(model_name, selected_result)
                        if result_data:
                            st.json(result_data)
                else:
                    st.info(f"{model_name}ì˜ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
        st.markdown("**ğŸ“ˆ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½**")
        all_experiments = get_all_model_experiments()
        if all_experiments:
            # ìµœê·¼ ì‹¤í—˜ ê²°ê³¼ë“¤ í‘œì‹œ
            recent_experiments = []
            for model_name, experiments in all_experiments.items():
                if experiments:
                    recent_experiments.append({
                        'model': model_name,
                        'latest': experiments[0],  # ê°€ì¥ ìµœê·¼ ê²°ê³¼
                        'count': len(experiments)
                    })
            
            # ìµœê·¼ ì‹¤í—˜ ê²°ê³¼ë“¤ì„ í…Œì´ë¸”ë¡œ í‘œì‹œ
            if recent_experiments:
                st.markdown("**ìµœê·¼ ì‹¤í—˜ ê²°ê³¼**")
                for exp in recent_experiments:
                    st.info(f"**{exp['model']}**: {exp['count']}ê°œ ê²°ê³¼ (ìµœê·¼: {exp['latest']})")
        else:
            st.info("ì•„ì§ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼ (ë””ë²„ê¹…ìš©)
    if st.sidebar.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”", help="ëª¨ë“  ìºì‹œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤", key="clear_cache_experiment"):
        # ìºì‹œ í•¨ìˆ˜ë“¤ ì´ˆê¸°í™”
        get_available_models.clear()
        # ì„¸ì…˜ ìƒíƒœ ìºì‹œ ì´ˆê¸°í™”
        keys_to_remove = ['model_loaded', 'current_model_name', 'dataset_files_cache', 
                         'model_dataset_files_cache', 'available_models']
        for key in keys_to_remove:
            if key in st.session_state:
                del st.session_state[key]
        st.sidebar.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        # st.rerun() ì œê±° - í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ ë°©ì§€