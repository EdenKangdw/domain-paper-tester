import streamlit as st
import os
import json
from utils import get_available_models
import pandas as pd
from datetime import datetime
import pickle

# Huggingface ê´€ë ¨ ì¶”ê°€
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

DATASET_ROOT = "dataset"
MODEL_CACHE_FILE = "model_cache.pkl"

# ëª¨ë¸ë³„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
def get_model_experiment_path(model_name):
    """ëª¨ë¸ë³„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    safe_model_name = model_name.replace(':', '_').replace('/', '_')
    return f"experiment_results/{safe_model_name}"

def get_model_dataset_path(model_name):
    """ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    safe_model_name = model_name.replace(':', '_').replace('/', '_')
    return f"dataset/{safe_model_name}"

# ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ
MODEL_CACHE = {
    "model": None,
    "tokenizer": None,
    "model_name": None
}

def save_model_cache():
    """ëª¨ë¸ ìºì‹œ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥"""
    try:
        cache_info = {
            "model_name": MODEL_CACHE["model_name"],
            "timestamp": datetime.now().isoformat()
        }
        with open(MODEL_CACHE_FILE, "wb") as f:
            pickle.dump(cache_info, f)
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_model_cache():
    """íŒŒì¼ì—ì„œ ëª¨ë¸ ìºì‹œ ì •ë³´ë¥¼ ë¡œë“œ"""
    try:
        if os.path.exists(MODEL_CACHE_FILE):
            with open(MODEL_CACHE_FILE, "rb") as f:
                cache_info = pickle.load(f)
            return cache_info
    except Exception as e:
        print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(MODEL_CACHE_FILE):
            os.remove(MODEL_CACHE_FILE)
    except Exception as e:
        print(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

def create_model_directories(model_name):
    """ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    dataset_path = get_model_dataset_path(model_name)
    
    os.makedirs(experiment_path, exist_ok=True)
    os.makedirs(dataset_path, exist_ok=True)
    
    # ë„ë©”ì¸ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
    domains = ["general", "technical", "legal", "medical"]
    for domain in domains:
        os.makedirs(os.path.join(dataset_path, domain), exist_ok=True)
    
    return experiment_path, dataset_path

def get_model_dataset_files(model_name):
    """íŠ¹ì • ëª¨ë¸ì˜ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    dataset_path = get_model_dataset_path(model_name)
    domains = ["general", "technical", "legal", "medical"]
    files = {}
    
    for domain in domains:
        domain_path = os.path.join(dataset_path, domain)
        if os.path.exists(domain_path):
            files[domain] = [f for f in os.listdir(domain_path) if f.endswith(".jsonl")]
        else:
            files[domain] = []
    
    return files

def get_model_prompts(model_name, domain, filename, max_count=10000):
    """íŠ¹ì • ëª¨ë¸ì˜ ë°ì´í„°ì…‹ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    dataset_path = get_model_dataset_path(model_name)
    path = os.path.join(dataset_path, domain, filename)
    prompts = []
    
    try:
        with open(path, "r") as f:
            for i, line in enumerate(f):
                if i >= max_count:
                    break
                try:
                    data = json.loads(line)
                    prompt = data.get("prompt", "(no prompt)")
                    prompts.append(prompt)
                except Exception:
                    continue
    except Exception:
        pass
    return prompts

def copy_original_dataset_to_model(model_name):
    """ì›ë³¸ ë°ì´í„°ì…‹ì„ ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤."""
    original_dataset_path = DATASET_ROOT
    model_dataset_path = get_model_dataset_path(model_name)
    
    if not os.path.exists(original_dataset_path):
        st.error("ì›ë³¸ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ì›ë³¸ ë°ì´í„°ì…‹ì˜ ëª¨ë“  íŒŒì¼ì„ ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        domains = ["general", "technical", "legal", "medical"]
        for domain in domains:
            original_domain_path = os.path.join(original_dataset_path, domain)
            model_domain_path = os.path.join(model_dataset_path, domain)
            
            if os.path.exists(original_domain_path):
                # ë„ë©”ì¸ ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(model_domain_path, exist_ok=True)
                
                # íŒŒì¼ ë³µì‚¬
                for filename in os.listdir(original_domain_path):
                    if filename.endswith('.jsonl'):
                        original_file = os.path.join(original_domain_path, filename)
                        model_file = os.path.join(model_domain_path, filename)
                        
                        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë³µì‚¬
                        if not os.path.exists(model_file):
                            import shutil
                            shutil.copy2(original_file, model_file)
        
        st.success(f"{model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# ê° ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
def get_dataset_files():
    domains = ["general", "technical", "legal", "medical"]
    files = {}
    for domain in domains:
        domain_path = os.path.join(DATASET_ROOT, domain)
        if os.path.exists(domain_path):
            files[domain] = [f for f in os.listdir(domain_path) if f.endswith(".jsonl")]
        else:
            files[domain] = []
    return files

# ì„ íƒí•œ ë°ì´í„°ì…‹ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
def get_prompts(domain, filename, max_count=10000):
    path = os.path.join(DATASET_ROOT, domain, filename)
    prompts = []
    try:
        with open(path, "r") as f:
            for i, line in enumerate(f):
                if i >= max_count:
                    break
                try:
                    data = json.loads(line)
                    prompt = data.get("prompt", "(no prompt)")
                    prompts.append(prompt)
                except Exception:
                    continue
    except Exception:
        pass
    return prompts

def load_model_to_session(model_name):
    """
    ëª¨ë¸ì„ ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì— ì°¸ì¡°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    import torch
    model_map = {
        'mistral:7b': 'mistralai/Mistral-7B-v0.1',
        'llama2:7b': 'meta-llama/Llama-2-7b-hf',
        'gemma:7b': 'google/gemma-7b',
        'gemma:2b': 'google/gemma-2b',
        'qwen:7b': 'Qwen/Qwen-7B',
        'qwen:14b': 'Qwen/Qwen-14B',
        'deepseek:7b': 'deepseek-ai/deepseek-llm-7b-base',
        'yi:6b': '01-ai/Yi-6B',
        'yi:34b': '01-ai/Yi-34B',
        'openchat:7b': 'openchat/openchat-3.5-7b',
        'neural:7b': 'microsoft/DialoGPT-medium',
        'phi:2.7b': 'microsoft/phi-2',
        'stable:7b': 'stabilityai/stablelm-base-alpha-7b',
    }
    hf_model = model_map.get(model_name)
    if not hf_model:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ëª…ì…ë‹ˆë‹¤.")
        return False
    try:
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(hf_model)
        
        # ëª¨ë¸ë³„ íŠ¹ë³„í•œ í† í¬ë‚˜ì´ì € ì„¤ì •
        if 'gemma' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        elif 'llama' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        elif 'qwen' in model_name.lower():
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
        
        # ëª¨ë¸ ë¡œë”© ì‹œë„
        try:
            quant_config = BitsAndBytesConfig(load_in_8bit=True)
            model = AutoModelForCausalLM.from_pretrained(
                hf_model,
                quantization_config=quant_config,
                device_map="auto",
                torch_dtype=torch.float16
            )
        except Exception as quant_error:
            # 8ë¹„íŠ¸ ì–‘ìí™” ì‹¤íŒ¨ ì‹œ 16ë¹„íŠ¸ë¡œ ì‹œë„
            st.warning("8ë¹„íŠ¸ ì–‘ìí™” ë¡œë”© ì‹¤íŒ¨, 16ë¹„íŠ¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
            model = AutoModelForCausalLM.from_pretrained(
                hf_model,
                device_map="auto",
                torch_dtype=torch.float16
            )
        
        # ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì— ì°¸ì¡° ì €ì¥
        st.session_state['model'] = model
        st.session_state['tokenizer'] = tokenizer
        st.session_state['model_name'] = model_name
        MODEL_CACHE["model"] = model
        MODEL_CACHE["tokenizer"] = tokenizer
        MODEL_CACHE["model_name"] = model_name
        # ìºì‹œ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥
        save_model_cache()
        return True
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        st.error("ëª¨ë¸ëª…ì„ í™•ì¸í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def unload_model_from_session():
    """
    ì„œë²„ ë©”ëª¨ë¦¬ì—ì„œ ëª¨ë¸ì„ í•´ì œí•©ë‹ˆë‹¤.
    ì„¸ì…˜ ìƒíƒœì™€ ê¸€ë¡œë²Œ ìºì‹œì˜ ì°¸ì¡°ë¥¼ ì œê±°í•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ë¹„ì›ë‹ˆë‹¤.
    """
    import torch
    if 'model' in st.session_state:
        del st.session_state['model']
    if 'tokenizer' in st.session_state:
        del st.session_state['tokenizer']
    if 'model_name' in st.session_state:
        del st.session_state['model_name']
    # ê¸€ë¡œë²Œ ìºì‹œë„ ë¹„ì›€
    MODEL_CACHE["model"] = None
    MODEL_CACHE["tokenizer"] = None
    MODEL_CACHE["model_name"] = None
    # ìºì‹œ íŒŒì¼ ì‚­ì œ
    clear_model_cache()
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    st.success("ëª¨ë¸ì´ ì„œë²„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_attention_from_session(prompt):
    import torch
    model = st.session_state.get('model', None)
    tokenizer = st.session_state.get('tokenizer', None)
    if model is None or tokenizer is None:
        return None, None, None
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    attentions = tuple(attn.cpu().numpy() for attn in outputs.attentions)
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    token_ids = inputs['input_ids'][0].cpu().tolist()
    return attentions, tokens, token_ids

def plot_attention_head_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í—¤ë“œë³„ë¡œ evidence í† í°(to_token)ì— ëŒ€í•œ ì–´í…ì…˜ í‰ê·  ê³„ì‚°
    # attn shape: (head, from_token, to_token)
    head_count = attn.shape[0]
    avg_evidence_attention = []
    for h in range(head_count):
        # from_token ì „ì²´ì—ì„œ evidence í† í°(to_token)ìœ¼ë¡œ ê°€ëŠ” ì–´í…ì…˜ í‰ê· 
        if evidence_indices:  # evidence_indicesê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
            try:
                avg = attn[h, :, evidence_indices].mean()
            except (IndexError, ValueError):
                avg = 0.0  # ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ ì‹œ 0 ë°˜í™˜
        else:
            avg = 0.0  # evidence_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ 0 ë°˜í™˜
        avg_evidence_attention.append(avg)
    avg_evidence_attention = np.array(avg_evidence_attention)

    # íˆíŠ¸ë§µ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 1.5))
    sns.heatmap(avg_evidence_attention[None, :], annot=True, fmt=".2f", cmap="YlOrRd", cbar=True, ax=ax)
    ax.set_xlabel("Head index")
    ax.set_yticks([])
    ax.set_title("Average Evidence Attention by Head")
    plt.tight_layout()
    return fig, avg_evidence_attention

def plot_attention_head_token_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í—¤ë“œë³„ë¡œ to_token(ëª¨ë“  í† í°)ì— ëŒ€í•œ ì–´í…ì…˜ í‰ê·  (from_token ì „ì²´ í‰ê· )
    # shape: (head, to_token)
    avg_attn = attn.mean(axis=1)  # (head, to_token)

    fig, ax = plt.subplots(figsize=(min(1.2+0.4*len(tokens), 16), 1.5+0.3*avg_attn.shape[0]))
    sns.heatmap(avg_attn, annot=False, cmap="YlGnBu", cbar=True, ax=ax)
    ax.set_xlabel("Token index")
    ax.set_ylabel("Head index")
    ax.set_title("Average Token Attention by Head")
    # xì¶•ì— í† í° ë¼ë²¨ í‘œì‹œ (ê¸¸ë©´ ì˜ë¦¼)
    token_labels = [t if i not in evidence_indices else f"*{t}*" for i, t in enumerate(tokens)]
    ax.set_xticks(range(len(tokens)))
    ax.set_xticklabels(token_labels, rotation=90, fontsize=8)
    # evidence í† í° ê°•ì¡° (xì¶• ë¼ë²¨ ìƒ‰ìƒ)
    for idx in evidence_indices:
        if idx < len(ax.get_xticklabels()):  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
            ax.get_xticklabels()[idx].set_color("red")
            ax.get_xticklabels()[idx].set_fontweight("bold")
    plt.tight_layout()
    return fig, avg_attn

def plot_token_head_attention_heatmap(attn, tokens, evidence_indices):
    """
    attn: (head, from_token, to_token) numpy array (ë§ˆì§€ë§‰ ë ˆì´ì–´)
    tokens: í† í° ë¦¬ìŠ¤íŠ¸
    evidence_indices: evidence í† í°ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # evidence_indices ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
    if not evidence_indices:
        evidence_indices = []
    else:
        # í† í° ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¸ë±ìŠ¤ ì œê±°
        evidence_indices = [i for i in evidence_indices if 0 <= i < len(tokens)]
    
    # ê° í† í°ë³„ë¡œ í—¤ë“œ ì–´í…ì…˜ í‰ê·  (from_token ì „ì²´ í‰ê· )
    # shape: (head, to_token)
    avg_attn = attn.mean(axis=1)  # (head, to_token)
    # shapeë¥¼ (to_token, head)ë¡œ transpose
    avg_attn_t = avg_attn.T  # (to_token, head)
    fig, ax = plt.subplots(figsize=(min(1.2+0.4*len(tokens), 16), 1.5+0.3*avg_attn_t.shape[1]))
    sns.heatmap(avg_attn_t, annot=False, cmap="YlGnBu", cbar=True, ax=ax)
    ax.set_ylabel("Token index")
    ax.set_xlabel("Head index")
    ax.set_title("Average Head Attention by Token")
    # yì¶•ì— í† í° ë¼ë²¨ í‘œì‹œ (ê¸¸ë©´ ì˜ë¦¼)
    token_labels = [t if i not in evidence_indices else f"*{t}*" for i, t in enumerate(tokens)]
    ax.set_yticks(range(len(tokens)))
    ax.set_yticklabels(token_labels, rotation=0, fontsize=8)
    # evidence í† í° ê°•ì¡° (yì¶• ë¼ë²¨ ìƒ‰ìƒ)
    for idx in evidence_indices:
        if idx < len(ax.get_yticklabels()):  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
            ax.get_yticklabels()[idx].set_color("red")
            ax.get_yticklabels()[idx].set_fontweight("bold")
    plt.tight_layout()
    return fig, avg_attn_t

def batch_domain_experiment(model_name, files, num_prompts=5):
    """
    ì—¬ëŸ¬ ë„ë©”ì¸ì— ëŒ€í•´ evidence ì–´í…ì…˜ ì‹¤í—˜ì„ ì¼ê´„ ìˆ˜í–‰í•˜ê³  í†µê³„ ì§‘ê³„
    model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
    files: get_dataset_files() ê²°ê³¼
    num_prompts: ë„ë©”ì¸ë³„ ìƒ˜í”Œë§í•  í”„ë¡¬í”„íŠ¸ ê°œìˆ˜
    """
    results = []
    
    # ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'model' in st.session_state and 'tokenizer' in st.session_state:
        model = st.session_state['model']
        tokenizer = st.session_state['tokenizer']
        model_loaded_in_session = True
    else:
        st.error("ëª¨ë¸ì´ ì„¸ì…˜ì— ë¡œë“œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì§„í–‰í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return results

    for domain, file_list in files.items():
        if not file_list:
            continue
        selected_file = file_list[0]  # ê° ë„ë©”ì¸ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©(í™•ì¥ ê°€ëŠ¥)
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        prompts = get_model_prompts(model_name, domain, selected_file, max_count=10000)
        if not prompts:
            continue
        # í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œë§
        sampled = prompts[:num_prompts]
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ê²½ë¡œ ì‚¬ìš©
        path = os.path.join(get_model_dataset_path(model_name), domain, selected_file)
        with open(path, "r") as f:
            lines = f.readlines()
        for prompt in sampled:
            try:
                idx = prompts.index(prompt)
                data = json.loads(lines[idx])
                evidence_indices = data.get("evidence_indices", [])
                # ì–´í…ì…˜ ì¶”ì¶œ
                inputs = tokenizer(prompt, return_tensors="pt")
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
                with torch.no_grad():
                    outputs = model(**inputs, output_attentions=True)
                attentions = tuple(attn.cpu().numpy() for attn in outputs.attentions)
                tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
                evidence_indices = [i for i in evidence_indices if i < len(tokens)]
                last_attn = attentions[-1][0]  # (head, from_token, to_token)
                # í—¤ë“œë³„ evidence ì–´í…ì…˜ í‰ê· 
                head_count = last_attn.shape[0]
                avg_evidence_attention = []
                for h in range(head_count):
                    if evidence_indices:  # evidence_indicesê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
                        try:
                            avg = last_attn[h, :, evidence_indices].mean()
                        except (IndexError, ValueError):
                            avg = 0.0  # ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ ì‹œ 0 ë°˜í™˜
                    else:
                        avg = 0.0  # evidence_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ 0 ë°˜í™˜
                    avg_evidence_attention.append(avg)
                max_head = int(np.argmax(avg_evidence_attention))
                results.append({
                    "domain": domain,
                    "prompt": prompt,
                    "max_head": max_head,
                    "avg_evidence_attention": avg_evidence_attention[max_head],
                    "evidence_indices": evidence_indices,
                    "tokens": tokens,
                    "model_name": model_name,
                    "tokenizer_name": tokenizer.name_or_path if hasattr(tokenizer, 'name_or_path') else "unknown"
                })
            except Exception as e:
                continue

    return results

def save_experiment_result(results, model_name):
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ experiment_results í´ë”ì— ì €ì¥ (numpy íƒ€ì…ì„ ëª¨ë‘ ë³€í™˜)"""
    # ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    experiment_path = get_model_experiment_path(model_name)
    os.makedirs(experiment_path, exist_ok=True)
    
    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{experiment_path}/{now}_{model_name.replace(':','_')}.json"
    
    # numpy íƒ€ì…ì„ ëª¨ë‘ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    def convert(o):
        if isinstance(o, np.generic):
            return o.item()
        if isinstance(o, (list, tuple)):
            return [convert(x) for x in o]
        if isinstance(o, dict):
            return {k: convert(v) for k, v in o.items()}
        return o
    results_converted = [convert(r) for r in results]
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results_converted, f, ensure_ascii=False, indent=2)
    return filename

def list_model_experiment_results(model_name):
    """íŠ¹ì • ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    if not os.path.exists(experiment_path):
        return []
    
    files = [f for f in os.listdir(experiment_path) if f.endswith(".json")]
    files.sort(reverse=True)
    return files

def load_model_experiment_result(model_name, filename):
    """íŠ¹ì • ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    experiment_path = get_model_experiment_path(model_name)
    path = os.path.join(experiment_path, filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def get_all_model_experiments():
    """ëª¨ë“  ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    experiment_root = "experiment_results"
    if not os.path.exists(experiment_root):
        return {}
    
    model_experiments = {}
    for model_dir in os.listdir(experiment_root):
        model_path = os.path.join(experiment_root, model_dir)
        if os.path.isdir(model_path):
            # ë””ë ‰í† ë¦¬ëª…ì„ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€í™˜
            model_name = model_dir.replace('_', '/').replace('_', ':')
            files = [f for f in os.listdir(model_path) if f.endswith(".json")]
            if files:
                model_experiments[model_name] = files
    
    return model_experiments

def check_model_loaded():
    """
    ì„œë²„ ë©”ëª¨ë¦¬ì— ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì„¸ì…˜ì— ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ ìºì‹œì—ì„œ ë³µêµ¬í•˜ê³ , ê·¸ê²ƒë„ ì—†ìœ¼ë©´ íŒŒì¼ ìºì‹œì—ì„œ ë³µêµ¬í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ë¨¼ì € ì„¸ì…˜ ìƒíƒœ í™•ì¸
        if 'model' in st.session_state and 'tokenizer' in st.session_state:
            model = st.session_state['model']
            if model is not None:
                return True, st.session_state.get('model_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸')
        
        # 2. ì„¸ì…˜ì— ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ ìºì‹œì—ì„œ ë³µêµ¬
        if MODEL_CACHE["model"] is not None and MODEL_CACHE["tokenizer"] is not None:
            st.session_state['model'] = MODEL_CACHE["model"]
            st.session_state['tokenizer'] = MODEL_CACHE["tokenizer"]
            st.session_state['model_name'] = MODEL_CACHE["model_name"]
            return True, MODEL_CACHE["model_name"]
        
        # 3. ê¸€ë¡œë²Œ ìºì‹œì—ë„ ì—†ìœ¼ë©´ íŒŒì¼ ìºì‹œì—ì„œ ë³µêµ¬ ì‹œë„
        cache_info = load_model_cache()
        if cache_info and cache_info.get('model_name'):
            model_name = cache_info['model_name']
            # ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œ
            if load_model_to_session(model_name):
                return True, model_name
        
        # 4. ì„¸ì…˜ì— ëª¨ë¸ ì´ë¦„ë§Œ ìˆê³  ì‹¤ì œ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° (ìƒˆë¡œê³ ì¹¨ í›„)
        if 'model_name' in st.session_state and st.session_state['model_name']:
            model_name = st.session_state['model_name']
            # ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œ
            if load_model_to_session(model_name):
                return True, model_name
        
        return False, None
    except Exception as e:
        print(f"ëª¨ë¸ ì²´í¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return False, None

def show():
    # í˜ì´ì§€ ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ëª¨ë¸ ìƒíƒœ ë³µêµ¬ ì‹œë„
    if 'model_auto_restored' not in st.session_state:
        st.session_state['model_auto_restored'] = True
        # ìë™ ë³µêµ¬ ì‹œë„ (ì¡°ìš©íˆ)
        try:
            is_loaded, loaded_model_name = check_model_loaded()
            if is_loaded:
                st.session_state['auto_restored_model'] = loaded_model_name
        except Exception:
            pass
    
    st.title("ì‹¤í—˜ íƒ­")
    st.write("ì‹¤í—˜ íƒ­ì…ë‹ˆë‹¤. (ì–´í…ì…˜ íˆíŠ¸ë§µ ë° evidence ë¶„ì„ ê¸°ëŠ¥ì´ ì—¬ê¸°ì— ì¶”ê°€ë¨)")

    # ìë™ ë³µêµ¬ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì•Œë¦¼
    if 'auto_restored_model' in st.session_state:
        st.success(f"ì´ì „ì— ë¡œë“œëœ {st.session_state['auto_restored_model']} ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        del st.session_state['auto_restored_model']

    st.markdown("---")
    st.subheader(":rocket: huggingface ëª¨ë¸ ë¡œë“œ/í•´ì œ")
    
    # ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸
    is_loaded, loaded_model_name = check_model_loaded()
    if is_loaded:
        st.success(f"í˜„ì¬ {loaded_model_name} ëª¨ë¸ì´ ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # Hugging Face ëª¨ë¸ ëª©ë¡ (ì‹¤í—˜ìš©)
    hf_model_list = [
        "mistral:7b",
        "llama2:7b", 
        "gemma:7b",
        "gemma:2b",
        "qwen:7b",
        "qwen:14b",
        "deepseek:7b",
        "yi:6b",
        "yi:34b",
        "openchat:7b",
        "neural:7b",
        "phi:2.7b",
        "stable:7b"
    ]
    
    # Ollama ëª¨ë¸ ëª©ë¡ (ì°¸ê³ ìš©)
    ollama_model_list = get_available_models()
    
    # ëª¨ë¸ ì„ íƒ ë°©ì‹
    model_selection_method = st.radio(
        "ëª¨ë¸ ì„ íƒ ë°©ì‹",
        ["Hugging Face ëª¨ë¸ (ì‹¤í—˜ìš©)", "Ollama ëª¨ë¸ (ì°¸ê³ ìš©)", "ì§ì ‘ ì…ë ¥"],
        horizontal=True,
        key="model_selection_method"
    )
    
    if model_selection_method == "Hugging Face ëª¨ë¸ (ì‹¤í—˜ìš©)":
        selected_model = st.selectbox(
            "ì‚¬ìš©í•  Hugging Face ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", 
            hf_model_list, 
            key="hf_model_select"
        )
        st.info("ğŸ’¡ Hugging Face ëª¨ë¸ì€ ì–´í…ì…˜ ì‹¤í—˜ì— ì‚¬ìš©ë©ë‹ˆë‹¤.")
        
    elif model_selection_method == "Ollama ëª¨ë¸ (ì°¸ê³ ìš©)":
        if ollama_model_list:
            selected_model = st.selectbox(
                "ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ (ì°¸ê³ ìš©)", 
                ollama_model_list, 
                key="ollama_model_select"
            )
            st.warning("âš ï¸ Ollama ëª¨ë¸ì€ ì–´í…ì…˜ ì‹¤í—˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Hugging Face ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        else:
            selected_model = st.text_input("ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”", key="ollama_model_input")
            st.warning("âš ï¸ Ollama ëª¨ë¸ì€ ì–´í…ì…˜ ì‹¤í—˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        selected_model = st.text_input("ëª¨ë¸ëª…ì„ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: mistral:7b)", key="custom_model_input")
        st.info("ğŸ’¡ ì§€ì›ë˜ëŠ” ëª¨ë¸: mistral:7b, llama2:7b, gemma:7b, qwen:7b ë“±")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ëª¨ë¸ ë¡œë“œ"):
            if is_loaded:
                st.warning(f"ì´ë¯¸ {loaded_model_name} ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ë¨¼ì € í˜„ì¬ ëª¨ë¸ì„ í•´ì œí•´ì£¼ì„¸ìš”.")
            else:
                if load_model_to_session(selected_model):
                    st.success(f"ëª¨ë¸ {selected_model}ì´(ê°€) ì„œë²„ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    with col2:
        if st.button("ëª¨ë¸ í•´ì œ"):
            if not is_loaded:
                st.warning("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                unload_model_from_session()
                st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸

    # ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì‹¤í—˜ UI
    if is_loaded:
        st.markdown("---")
        st.subheader(":gear: ëª¨ë¸ë³„ ì‹¤í—˜ ì„¤ì •")
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ì¤€ë¹„
        with st.expander("ğŸ“ ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ì¤€ë¹„", expanded=False):
            st.markdown("""
            **ëª¨ë¸ë³„ ì‹¤í—˜ì„ ìœ„í•´ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤:**
            - ê° ëª¨ë¸ë§ˆë‹¤ ë³„ë„ì˜ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë©ë‹ˆë‹¤.
            - ì›ë³¸ ë°ì´í„°ì…‹ì´ ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬ë©ë‹ˆë‹¤.
            - ëª¨ë¸ë³„ë¡œ ë…ë¦½ì ì¸ ì‹¤í—˜ í™˜ê²½ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """)
            
            if st.button("í˜„ì¬ ëª¨ë¸ìš© ë°ì´í„°ì…‹ ì¤€ë¹„"):
                with st.spinner(f"{loaded_model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
                    if copy_original_dataset_to_model(loaded_model_name):
                        st.success(f"{loaded_model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
        
        # ëª¨ë¸ë³„ ë°ì´í„°ì…‹ í™•ì¸
        model_dataset_files = get_model_dataset_files(loaded_model_name)
        has_model_dataset = any(files for files in model_dataset_files.values())
        
        if has_model_dataset:
            st.success(f"âœ… {loaded_model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            
            # ëª¨ë¸ë³„ ì‹¤í—˜ UI
            st.markdown("---")
            st.subheader(":microscope: ëª¨ë¸ë³„ ì‹¤í—˜")
            
            domains = list(model_dataset_files.keys())
            selected_domain = st.selectbox("ë„ë©”ì¸ ì„ íƒ", domains, key="model_domain")
            dataset_files = model_dataset_files[selected_domain]
            
            if dataset_files:
                selected_file = st.selectbox("ë°ì´í„°ì…‹ íŒŒì¼ ì„ íƒ", dataset_files, key="model_file")
                prompts = get_model_prompts(loaded_model_name, selected_domain, selected_file)
                
                if prompts:
                    selected_prompt = st.selectbox("í”„ë¡¬í”„íŠ¸ ì„ íƒ (ìµœëŒ€ 100ê°œ ë¯¸ë¦¬ë³´ê¸°)", prompts, key="model_prompt")
                    prompt_idx = prompts.index(selected_prompt)
                    path = os.path.join(get_model_dataset_path(loaded_model_name), selected_domain, selected_file)
                    
                    # ì„ íƒëœ í”„ë¡¬í”„íŠ¸ì˜ ì „ì²´ ë°ì´í„° ë¡œë“œ
                    try:
                        with open(path, "r") as f:
                            for i, line in enumerate(f):
                                if i == prompt_idx:
                                    data = json.loads(line)
                                    # ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
                                    st.markdown("### ğŸ“ ì„ íƒëœ ë°ì´í„°ì…‹ ìƒì„¸ ì •ë³´")
                                    st.markdown("**í”„ë¡¬í”„íŠ¸:**")
                                    st.markdown(f"```\n{data.get('prompt', '')}\n```")
                                    
                                    st.markdown("### ğŸ” Evidence ì •ë³´")
                                    evidence_tokens = data.get("evidence_tokens", [])
                                    evidence_indices = data.get("evidence_indices", [])
                                    
                                    # Evidence í† í°ê³¼ ì¸ë±ìŠ¤ë¥¼ í…Œì´ë¸”ë¡œ í‘œì‹œ
                                    evidence_data = []
                                    for idx, token in zip(evidence_indices, evidence_tokens):
                                        evidence_data.append({
                                            "ì¸ë±ìŠ¤": idx,
                                            "í† í°": token
                                        })
                                    if evidence_data:
                                        st.table(pd.DataFrame(evidence_data))
                                    else:
                                        st.warning("Evidence ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    
                                    # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                                    st.markdown("### ğŸ“Š ë©”íƒ€ë°ì´í„°")
                                    meta_data = {
                                        "ë„ë©”ì¸": data.get("domain", ""),
                                        "ëª¨ë¸": data.get("model", ""),
                                        "íƒ€ì„ìŠ¤íƒ¬í”„": data.get("timestamp", ""),
                                        "ì¸ë±ìŠ¤": data.get("index", "")
                                    }
                                    st.json(meta_data)
                                    break
                    except Exception as e:
                        st.error(f"ë°ì´í„°ì…‹ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    
                    st.markdown("---")
                    st.subheader("ì–´í…ì…˜ ì‹¤í—˜")
                    if st.button("ì–´í…ì…˜ ì¶”ì¶œ ë° í† í°í™” ë³´ê¸°"):
                        with st.spinner("ëª¨ë¸ì—ì„œ ì–´í…ì…˜ ì¶”ì¶œ ì¤‘..."):
                            attentions, tokens, token_ids = get_attention_from_session(selected_prompt)
                        if attentions is None:
                            st.error("í•´ë‹¹ ëª¨ë¸ì€ ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.markdown(f"**í† í°í™” ê²°ê³¼:** {' | '.join(tokens)}")
                            evidence_set = set(evidence_tokens)
                            colored = []
                            for t in tokens:
                                if t in evidence_set:
                                    colored.append(f"<span style='background-color: #ffe066'>{t}</span>")
                                else:
                                    colored.append(t)
                            st.markdown("**evidence í† í° ê°•ì¡°:**<br>" + ' '.join(colored), unsafe_allow_html=True)
                            st.info(f"ì–´í…ì…˜ shape: {len(attentions)} layers, {attentions[-1].shape[1]} heads, {attentions[-1].shape[2]} tokens")
                            last_attn = attentions[-1][0]
                            fig, avg_evidence_attention = plot_attention_head_heatmap(last_attn, tokens, evidence_indices)
                            st.pyplot(fig)
                            max_head = int(np.argmax(avg_evidence_attention))
                            st.success(f"ê°€ì¥ evidenceì— ê°•í•˜ê²Œ ë°˜ì‘í•˜ëŠ” í—¤ë“œ: Head {max_head} (í‰ê·  ì–´í…ì…˜ {avg_evidence_attention[max_head]:.4f})")
                            fig2, avg_attn = plot_attention_head_token_heatmap(last_attn, tokens, evidence_indices)
                            st.markdown("---")
                            st.subheader("í—¤ë“œë³„ í† í° ì–´í…ì…˜ íˆíŠ¸ë§µ")
                            st.pyplot(fig2)
                            st.caption("* xì¶•: í† í° ì¸ë±ìŠ¤ (evidence í† í°ì€ ë¹¨ê°„ìƒ‰), yì¶•: í—¤ë“œ ì¸ë±ìŠ¤, ê°’: ì–´í…ì…˜ í‰ê·  *")
                            st.markdown("---")
                            st.subheader("í† í°ë³„ í—¤ë“œ ì–´í…ì…˜ íˆíŠ¸ë§µ")
                            fig3, avg_attn_t = plot_token_head_attention_heatmap(last_attn, tokens, evidence_indices)
                            st.pyplot(fig3)
                            st.caption("* yì¶•: í† í° ì¸ë±ìŠ¤(ë¬¸ìì—´, evidence í† í°ì€ ë¹¨ê°„ìƒ‰), xì¶•: í—¤ë“œ ì¸ë±ìŠ¤, ê°’: ì–´í…ì…˜ í‰ê·  *")
                else:
                    st.warning("í•´ë‹¹ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning(f"{selected_domain} ë„ë©”ì¸ì— ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning(f"âš ï¸ {loaded_model_name} ëª¨ë¸ìš© ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ 'ëª¨ë¸ë³„ ë°ì´í„°ì…‹ ì¤€ë¹„' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")

        # ì¼ê´„ ì‹¤í—˜ ì„¹ì…˜ ì¶”ê°€
        st.markdown("---")
        st.subheader(":rocket: ëª¨ë¸ë³„ ì¼ê´„ ì‹¤í—˜")
        
        if has_model_dataset:
            # ì‹¤í—˜ ì„¤ì •
            num_prompts = st.number_input("ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ìˆ˜", min_value=1, max_value=10000, value=5)
            
            # ì‹¤í—˜ ì‹œì‘ ë²„íŠ¼
            if st.button("ëª¨ë¸ë³„ ì‹¤í—˜ ì‹œì‘"):
                st.info("ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # ì‹¤í—˜ ì‹¤í–‰
                try:
                    with st.spinner("ì‹¤í—˜ì„ ì‹¤í–‰í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                        # ì „ì²´ ë„ë©”ì¸ì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰
                        all_results = []
                        total_domains = len([d for d, files in model_dataset_files.items() if files])
                        current_domain = 0
                        
                        for domain, file_list in model_dataset_files.items():
                            if not file_list:
                                continue
                            
                            current_domain += 1
                            st.text(f"ì§„í–‰ ì¤‘: {domain} ë„ë©”ì¸ ({current_domain}/{total_domains})")
                            
                            results = batch_domain_experiment(loaded_model_name, {domain: file_list}, num_prompts)
                            all_results.extend(results)
                        
                        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
                        if all_results:
                            filename = save_experiment_result(all_results, loaded_model_name)
                            st.success(f"âœ… ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.success(f"ğŸ“ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
                            st.info(f"ğŸ“Š ì´ {len(all_results)}ê°œì˜ ì‹¤í—˜ ê²°ê³¼ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
                            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                            st.markdown("### ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
                            preview_data = []
                            for result in all_results[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                                preview_data.append({
                                    "ë„ë©”ì¸": result["domain"],
                                    "ìµœëŒ€ ì–´í…ì…˜ í—¤ë“œ": result["max_head"],
                                    "í‰ê·  ì–´í…ì…˜": f"{result['avg_evidence_attention']:.4f}",
                                    "Evidence í† í° ìˆ˜": len(result["evidence_indices"])
                                })
                            if preview_data:
                                st.table(pd.DataFrame(preview_data))
                        else:
                            st.warning("ì‹¤í—˜ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            
                except Exception as e:
                    st.error(f"ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.error("ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ëª¨ë¸ë³„ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì¼ê´„ ì‹¤í—˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì§„í–‰í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")