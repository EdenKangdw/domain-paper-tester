import streamlit as st
import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats

def list_experiment_results():
    if not os.path.exists("experiment_results"):
        return []
    files = [f for f in os.listdir("experiment_results") if f.endswith(".json")]
    files.sort(reverse=True)
    return files

def load_experiment_result(filename):
    path = os.path.join("experiment_results", filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def show():
    st.title(":open_file_folder: ì‹¤í—˜ ê¸°ë¡")
    
    # ì‹¤í—˜ ê°œìš”ë¥¼ expanderë¡œ ë³€ê²½
    with st.expander("ğŸ“ ì‹¤í—˜ ê°œìš”", expanded=False):
        st.markdown("""
        ì´ ì‹¤í—˜ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸(General, Technical, Legal, Medical)ì˜ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬, ê° í”„ë¡¬í”„íŠ¸ì—ì„œ evidence í† í°ì— ëŒ€í•´ ì–¸ì–´ëª¨ë¸ì˜ ì–´í…ì…˜ í—¤ë“œê°€ ì–´ë–»ê²Œ ë°˜ì‘í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
        
        **evidenceì˜ ì˜ë¯¸:**
        - ë°ì´í„°ì…‹ì—ì„œ `evidence`ë€, ê° í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸)ì— ëŒ€í•´ ì •ë‹µì„ ë„ì¶œí•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬(í† í°)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        - ì˜ˆë¥¼ ë“¤ì–´, ì˜í•™ ë„ë©”ì¸ì—ì„œ "What are the main side effects of this medication?"ë¼ëŠ” ì§ˆë¬¸ì´ ìˆë‹¤ë©´, ì •ë‹µì´ ë˜ëŠ” ë¶€ì‘ìš© ê´€ë ¨ ë‹¨ì–´ë“¤ì´ evidenceë¡œ ì§€ì •ë©ë‹ˆë‹¤.
        - ê° ë°ì´í„°ì…‹ ìƒ˜í”Œì—ëŠ” evidence í† í°ì˜ ì¸ë±ìŠ¤(`evidence_indices`)ì™€ ì‹¤ì œ í† í°(`evidence_tokens`) ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        
        **ì‹¤í—˜ ëª©ì :**
        - ì–¸ì–´ëª¨ë¸ì˜ ê° ì–´í…ì…˜ í—¤ë“œê°€ evidence í† í°ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€(ì–´í…ì…˜ì„ ì£¼ëŠ”ì§€) ë¶„ì„í•©ë‹ˆë‹¤.
        - ë„ë©”ì¸ë³„ë¡œ íŠ¹ì • í—¤ë“œê°€ evidenceì— ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ”ì§€, ë˜ëŠ” ë²”ìš©ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” í—¤ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        - ì´ë¥¼ í†µí•´ ëª¨ë¸ ë‚´ë¶€ì˜ í•´ì„ ê°€ëŠ¥ì„±(interpretability)ê³¼ ë„ë©”ì¸ íŠ¹í™”/ë²”ìš© ì–´í…ì…˜ íŒ¨í„´ì„ íƒêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        **âš ï¸ ì¤‘ìš”í•œ ì£¼ì˜ì‚¬í•­ - í† í°í™” ì°¨ì´:**
        - **ëª¨ë¸ë§ˆë‹¤ í† í°í™”ê°€ ë‹¤ë¦…ë‹ˆë‹¤**: ê°™ì€ í…ìŠ¤íŠ¸ë¼ë„ ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í† í°í™” ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤.
        - **Evidence ì¸ë±ìŠ¤ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤**: í† í°í™”ê°€ ë‹¤ë¥´ë¯€ë¡œ evidence í† í°ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤)ë„ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
        - **í—¤ë“œ ë¹„êµ ì‹œ ì£¼ì˜**: ë‹¤ë¥¸ ëª¨ë¸ ê°„ì˜ í—¤ë“œ ë¹„êµëŠ” í† í°í™” ì°¨ì´ë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
        - **ê°™ì€ ëª¨ë¸ ë‚´ì—ì„œë§Œ ì§ì ‘ ë¹„êµ**: ê°™ì€ ëª¨ë¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¼ë¦¬ë§Œ ì§ì ‘ì ì¸ í—¤ë“œ ë¹„êµê°€ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.
        """)
    
    result_files = list_experiment_results()
    if result_files:
        # íƒ­ìœ¼ë¡œ êµ¬ë¶„
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë‹¨ì¼ ì‹¤í—˜ ë¶„ì„", "ğŸ” ìƒì„¸ í•´ì„", "ğŸ“ˆ nì°¨ ì‹¤í—˜ ë¹„êµ"])
        
        with tab1:
            selected_result = st.selectbox("ì¡°íšŒí•  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", result_files, key="single_analysis")
            
            if st.button("ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰", key="basic_analysis"):
                loaded_results = load_experiment_result(selected_result)
                df = pd.DataFrame(loaded_results)
                
                # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                if 'model_name' in df.columns and len(df) > 0:
                    model_name = df['model_name'].iloc[0] if df['model_name'].iloc[0] else "ì•Œ ìˆ˜ ì—†ìŒ"
                    tokenizer_name = df['tokenizer_name'].iloc[0] if 'tokenizer_name' in df.columns and df['tokenizer_name'].iloc[0] else "ì•Œ ìˆ˜ ì—†ìŒ"
                    
                    st.markdown(f"**ğŸ”§ ì‚¬ìš©ëœ ëª¨ë¸:** {model_name}")
                    st.markdown(f"**ğŸ”¤ í† í¬ë‚˜ì´ì €:** {tokenizer_name}")
                
                # ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                st.markdown("### ğŸ“Š ê¸°ë³¸ ì‹¤í—˜ ê²°ê³¼")
                st.dataframe(df[["domain", "max_head", "avg_evidence_attention"]])
                
                # ê¸°ë³¸ ì •ë³´
                with st.expander("ğŸ“Š ê¸°ë³¸ ì‹¤í—˜ ì •ë³´", expanded=True):
                    st.markdown(f"#### ì´ {len(df)}ê°œ ìƒ˜í”Œ ë¶„ì„")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì´ ìƒ˜í”Œ ìˆ˜", len(df))
                    with col2:
                        st.metric("ë¶„ì„ ë„ë©”ì¸ ìˆ˜", len(df['domain'].unique()))
                    with col3:
                        st.metric("í‰ê·  Evidence Attention", f"{df['avg_evidence_attention'].mean():.4f}")
                    with col4:
                        st.metric("ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ", f"í—¤ë“œ {df['max_head'].value_counts().idxmax()}")
                
                # ë„ë©”ì¸ë³„ í—¤ë“œ ë¶„í¬ ì„¤ëª…
                st.markdown("""
                ### ğŸ“ˆ ë„ë©”ì¸ë³„ Evidence ë°˜ì‘ í—¤ë“œ ë¶„í¬
                ì´ ê·¸ë˜í”„ëŠ” ê° ë„ë©”ì¸ì—ì„œ evidence í† í°ì— ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ ì–´í…ì…˜ í—¤ë“œì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
                
                **í•´ì„ ë°©ë²•:**
                - xì¶•: ì–´í…ì…˜ í—¤ë“œ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
                - yì¶•: ê° í—¤ë“œê°€ evidenceì— ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ íšŸìˆ˜
                - ê° ë§‰ëŒ€ì˜ ë†’ì´: í•´ë‹¹ í—¤ë“œê°€ evidenceì— ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„
                
                **ì£¼ìš” ì¸ì‚¬ì´íŠ¸:**
                - íŠ¹ì • ë„ë©”ì¸ì—ì„œ íŠ¹ì • í—¤ë“œê°€ ìì£¼ ì„ íƒëœë‹¤ë©´, ê·¸ í—¤ë“œê°€ í•´ë‹¹ ë„ë©”ì¸ì˜ evidenceë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ë™ì¼í•œ í—¤ë“œê°€ ìì£¼ ì„ íƒëœë‹¤ë©´, ê·¸ í—¤ë“œê°€ ì¼ë°˜ì ì¸ evidence ì²˜ë¦¬ì— ì¤‘ìš”í•œ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """)
                hist = df.groupby(["domain", "max_head"]).size().unstack(fill_value=0)
                st.bar_chart(hist)
                st.caption("*ë„ë©”ì¸ë³„ë¡œ evidenceì— ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ í—¤ë“œì˜ ë¹ˆë„ ë¶„í¬*")
                
                # ë„ë©”ì¸-í—¤ë“œ íˆíŠ¸ë§µ ì„¤ëª…
                st.markdown("""
                ### ğŸ”¥ ë„ë©”ì¸-í—¤ë“œ Evidence ë°˜ì‘ íˆíŠ¸ë§µ
                ì´ íˆíŠ¸ë§µì€ ê° ë„ë©”ì¸ê³¼ ì–´í…ì…˜ í—¤ë“œì˜ ì¡°í•©ì—ì„œ evidence í† í°ì— ëŒ€í•œ ë°˜ì‘ ë¹ˆë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
                
                **í•´ì„ ë°©ë²•:**
                - xì¶•: ì–´í…ì…˜ í—¤ë“œ ë²ˆí˜¸
                - yì¶•: ë„ë©”ì¸ (Medical, Legal, Technical, General)
                - ìƒ‰ìƒ ê°•ë„: í•´ë‹¹ ë„ë©”ì¸-í—¤ë“œ ì¡°í•©ì—ì„œ evidenceì— ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ íšŸìˆ˜
                    - ë°ì€ ìƒ‰ìƒ(ë…¸ë€ìƒ‰/ë¹¨ê°„ìƒ‰): ë†’ì€ ë¹ˆë„
                    - ì–´ë‘ìš´ ìƒ‰ìƒ: ë‚®ì€ ë¹ˆë„
                - ìˆ«ì: ì‹¤ì œ ë¹ˆë„ ìˆ˜
                
                **ì£¼ìš” ì¸ì‚¬ì´íŠ¸:**
                1. **ë„ë©”ì¸ íŠ¹í™” í—¤ë“œ:**
                   - íŠ¹ì • ë„ë©”ì¸ì—ì„œë§Œ ë°ì€ ìƒ‰ìƒì„ ë³´ì´ëŠ” í—¤ë“œëŠ” í•´ë‹¹ ë„ë©”ì¸ì— íŠ¹í™”ëœ evidence ì²˜ë¦¬ì— ê´€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                   - ì˜ˆ: Medical ë„ë©”ì¸ì—ì„œë§Œ ë†’ì€ ê°’ì„ ë³´ì´ëŠ” í—¤ë“œëŠ” ì˜í•™ ê´€ë ¨ evidence ì²˜ë¦¬ì— íŠ¹í™”ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                
                2. **ë²”ìš© í—¤ë“œ:**
                   - ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ë†’ì€ ê°’ì„ ë³´ì´ëŠ” í—¤ë“œëŠ” ì¼ë°˜ì ì¸ evidence ì²˜ë¦¬ì— ê´€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                   - ì´ëŸ¬í•œ í—¤ë“œë“¤ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ evidenceë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                
                3. **ë„ë©”ì¸ë³„ íŒ¨í„´:**
                   - ê° ë„ë©”ì¸ë³„ë¡œ íŠ¹ì • í—¤ë“œë“¤ì— ì§‘ì¤‘ë˜ëŠ” íŒ¨í„´ì´ ìˆë‹¤ë©´, í•´ë‹¹ ë„ë©”ì¸ì˜ evidence ì²˜ë¦¬ì— íŠ¹í™”ëœ í—¤ë“œ ê·¸ë£¹ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                   - ì´ëŸ¬í•œ íŒ¨í„´ì€ ëª¨ë¸ì´ ê° ë„ë©”ì¸ì˜ evidenceë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
                """)
                fig, ax = plt.subplots(figsize=(min(12, 2+0.5*hist.shape[1]), 1.5+0.5*hist.shape[0]))
                sns.heatmap(hist, annot=True, fmt="d", cmap="YlOrRd", ax=ax)
                ax.set_xlabel("Head Index")
                ax.set_ylabel("Domain")
                ax.set_title("Domain-Head Evidence Response Frequency Heatmap")
                st.pyplot(fig)
                st.caption("*xì¶•: í—¤ë“œ ì¸ë±ìŠ¤, yì¶•: ë„ë©”ì¸, ê°’: evidenceì— ê°€ì¥ ë§ì´ ë°˜ì‘í•œ íšŸìˆ˜*")
        
        with tab2:
            selected_result = st.selectbox("ì¡°íšŒí•  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", result_files, key="detailed_analysis")
            
            if st.button("ìƒì„¸ í•´ì„ ì‹¤í–‰", key="detailed_analysis_btn"):
                loaded_results = load_experiment_result(selected_result)
                df = pd.DataFrame(loaded_results)
                
                # ê¸°ë³¸ ì •ë³´
                with st.expander("ğŸ“Š ê¸°ë³¸ ì‹¤í—˜ ì •ë³´", expanded=True):
                    st.markdown(f"#### ì´ {len(df)}ê°œ ìƒ˜í”Œ ë¶„ì„")
                    
                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                    if 'model_name' in df.columns and len(df) > 0:
                        model_name = df['model_name'].iloc[0] if df['model_name'].iloc[0] else "ì•Œ ìˆ˜ ì—†ìŒ"
                        tokenizer_name = df['tokenizer_name'].iloc[0] if 'tokenizer_name' in df.columns and df['tokenizer_name'].iloc[0] else "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        st.markdown(f"**ğŸ”§ ì‚¬ìš©ëœ ëª¨ë¸:** {model_name}")
                        st.markdown(f"**ğŸ”¤ í† í¬ë‚˜ì´ì €:** {tokenizer_name}")
                        
                        if tokenizer_name != "ì•Œ ìˆ˜ ì—†ìŒ":
                            st.info("""
                            **í† í°í™” ì •ë³´:**
                            - ì´ ì‹¤í—˜ì€ ìœ„ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í–ˆìŠµë‹ˆë‹¤.
                            - ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•  ë•ŒëŠ” í† í°í™” ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”.
                            """)
                    
                    st.dataframe(df[["domain", "max_head", "avg_evidence_attention"]])
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì´ ìƒ˜í”Œ ìˆ˜", len(df))
                    with col2:
                        st.metric("ë¶„ì„ ë„ë©”ì¸ ìˆ˜", len(df['domain'].unique()))
                    with col3:
                        st.metric("í‰ê·  Evidence Attention", f"{df['avg_evidence_attention'].mean():.4f}")
                    with col4:
                        st.metric("ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ", f"í—¤ë“œ {df['max_head'].value_counts().idxmax()}")

                # ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„
                with st.expander("ğŸ“ˆ ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„", expanded=False):
                    domain_stats = df.groupby('domain').agg({
                        'avg_evidence_attention': ['mean', 'std', 'min', 'max'],
                        'max_head': ['mean', 'std', 'min', 'max']
                    }).round(4)
                    st.dataframe(domain_stats)
                    
                    fig, ax = plt.subplots()
                    domain_means = df.groupby('domain')['avg_evidence_attention'].mean().sort_values(ascending=False)
                    ax.bar(domain_means.index, domain_means.values)
                    ax.set_title('Domain Performance by Average Evidence Attention')
                    ax.set_ylabel('Average Evidence Attention')
                    st.pyplot(fig)

                # í—¤ë“œ ë¶„í¬ ë¶„ì„
                with st.expander("ğŸ§  í—¤ë“œ ë¶„í¬ ë¶„ì„", expanded=False):
                    head_counts = df['max_head'].value_counts().sort_index()
                    st.bar_chart(head_counts)
                    
                    # ë„ë©”ì¸-í—¤ë“œ íˆíŠ¸ë§µ
                    hist = df.groupby(["domain", "max_head"]).size().unstack(fill_value=0)
                    fig2, ax2 = plt.subplots(figsize=(min(12, 2+0.5*hist.shape[1]), 1.5+0.5*hist.shape[0]))
                    sns.heatmap(hist, annot=True, fmt="d", cmap="YlOrRd", ax=ax2)
                    ax2.set_xlabel("Head Index")
                    ax2.set_ylabel("Domain")
                    ax2.set_title("Domain-Head Evidence Response Frequency Heatmap")
                    st.pyplot(fig2)

                # ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
                with st.expander("ğŸ“ˆ Evidence Attention ë¶„í¬", expanded=False):
                    fig3, ax3 = plt.subplots()
                    ax3.hist(df['avg_evidence_attention'], bins=30, alpha=0.7)
                    ax3.set_title('Overall Evidence Attention Distribution')
                    ax3.set_xlabel('Evidence Attention')
                    ax3.set_ylabel('Frequency')
                    st.pyplot(fig3)
                    
                    # ë„ë©”ì¸ë³„ ë¶„í¬
                    for domain in df['domain'].unique():
                        st.markdown(f"**{domain} ë„ë©”ì¸ Evidence Attention ë¶„í¬**")
                        fig4, ax4 = plt.subplots()
                        ax4.hist(df[df['domain']==domain]['avg_evidence_attention'], bins=20, alpha=0.7)
                        ax4.set_title(f'{domain} Domain Evidence Attention Distribution')
                        ax4.set_xlabel('Evidence Attention')
                        ax4.set_ylabel('Frequency')
                        st.pyplot(fig4)

                # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì˜í–¥ ë¶„ì„
                with st.expander("ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì˜í–¥ ë¶„ì„", expanded=False):
                    df['prompt_length'] = df['prompt'].str.len()
                    df['token_count'] = df['tokens'].apply(len)
                    fig5, (ax5, ax6) = plt.subplots(1,2,figsize=(12,5))
                    ax5.scatter(df['prompt_length'], df['avg_evidence_attention'], alpha=0.6)
                    ax5.set_xlabel('Prompt Length (characters)')
                    ax5.set_ylabel('Evidence Attention')
                    ax5.set_title('Prompt Length vs Evidence Attention')
                    ax6.scatter(df['token_count'], df['avg_evidence_attention'], alpha=0.6)
                    ax6.set_xlabel('Token Count')
                    ax6.set_ylabel('Evidence Attention')
                    ax6.set_title('Token Count vs Evidence Attention')
                    st.pyplot(fig5)

                # Evidence ì¸ë±ìŠ¤ íŒ¨í„´ ë¶„ì„
                with st.expander("ğŸ§© Evidence ì¸ë±ìŠ¤ íŒ¨í„´ ë¶„ì„", expanded=False):
                    evidence_counts = df['evidence_indices'].apply(len)
                    st.markdown(f"í‰ê·  evidence í† í° ìˆ˜: {evidence_counts.mean():.2f}")
                    fig7, ax7 = plt.subplots()
                    ax7.hist(evidence_counts, bins=20, alpha=0.7)
                    ax7.set_title('Evidence Token Count Distribution')
                    ax7.set_xlabel('Evidence Token Count')
                    ax7.set_ylabel('Frequency')
                    st.pyplot(fig7)

        with tab3:
            st.markdown("### ğŸ“ˆ nì°¨ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„")
            st.markdown("ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì„ íƒí•˜ì—¬ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ë‹¤ì¤‘ ì„ íƒ
            selected_results = st.multiselect(
                "ë¹„êµí•  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 5ê°œ)",
                result_files,
                default=result_files[:2] if len(result_files) >= 2 else result_files
            )
            
            if len(selected_results) >= 2:
                if st.button("nì°¨ ì‹¤í—˜ ë¹„êµ ë¶„ì„ ì‹¤í–‰", key="n_analysis"):
                    # ì„ íƒëœ ì‹¤í—˜ ê²°ê³¼ë“¤ì„ ë¡œë“œ
                    experiment_data = {}
                    for filename in selected_results:
                        results = load_experiment_result(filename)
                        df = pd.DataFrame(results)
                        # íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ ì •ë³´ ì¶”ì¶œ
                        experiment_name = filename.replace('.json', '').replace('_', ' ')
                        experiment_data[experiment_name] = df
                    
                    st.markdown("### ğŸ“Š ì‹¤í—˜ ê°œìš” ë¹„êµ")
                    
                    # ê¸°ë³¸ í†µê³„ ë¹„êµ
                    comparison_data = []
                    for exp_name, df in experiment_data.items():
                        # ëª¨ë¸ ì •ë³´ ì¶”ì¶œ (ìƒˆë¡œìš´ ì‹¤í—˜ ê²°ê³¼ì—ëŠ” model_nameì´ í¬í•¨ë¨)
                        model_info = "ì•Œ ìˆ˜ ì—†ìŒ"
                        if 'model_name' in df.columns and len(df) > 0:
                            model_info = df['model_name'].iloc[0] if df['model_name'].iloc[0] else "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        comparison_data.append({
                            "ì‹¤í—˜ëª…": exp_name,
                            "ëª¨ë¸": model_info,
                            "ì´ ìƒ˜í”Œ ìˆ˜": len(df),
                            "ë„ë©”ì¸ ìˆ˜": len(df['domain'].unique()),
                            "í‰ê·  Evidence Attention": round(df['avg_evidence_attention'].mean(), 4),
                            "ìµœëŒ€ Evidence Attention": round(df['avg_evidence_attention'].max(), 4),
                            "ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ": f"í—¤ë“œ {df['max_head'].value_counts().idxmax()}",
                            "í—¤ë“œ ë‹¤ì–‘ì„±": len(df['max_head'].unique())
                        })
                    
                    comparison_df = pd.DataFrame(comparison_data)
                    st.dataframe(comparison_df)
                    
                    # ëª¨ë¸ë³„ í† í°í™” ì°¨ì´ ê²½ê³ 
                    unique_models = comparison_df['ëª¨ë¸'].unique()
                    if len(unique_models) > 1 and not all(model == "ì•Œ ìˆ˜ ì—†ìŒ" for model in unique_models):
                        st.warning("""
                        âš ï¸ **ëª¨ë¸ë³„ í† í°í™” ì°¨ì´ ì£¼ì˜ì‚¬í•­**
                        
                        ì„ íƒëœ ì‹¤í—˜ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ:
                        - **í† í°í™” ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤**: ê°™ì€ í…ìŠ¤íŠ¸ë¼ë„ ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.
                        - **Evidence ì¸ë±ìŠ¤ê°€ ë‹¤ë¦…ë‹ˆë‹¤**: í† í°í™”ê°€ ë‹¤ë¥´ë¯€ë¡œ evidence í† í°ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤)ë„ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
                        - **í—¤ë“œ ë¹„êµì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤**: í† í°í™” ì°¨ì´ë¡œ ì¸í•´ ê°™ì€ í—¤ë“œë¼ë„ ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ í† í°ì„ ë³´ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        
                        ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ê²°ê³¼ë¥¼ í•´ì„í•´ì£¼ì„¸ìš”.
                        """)
                    
                    # ëª¨ë¸ë³„ ê·¸ë£¹í™” ë¶„ì„
                    if len(unique_models) > 1 and not all(model == "ì•Œ ìˆ˜ ì—†ìŒ" for model in unique_models):
                        st.markdown("### ğŸ¤– ëª¨ë¸ë³„ ë¶„ì„")
                        
                        for model in unique_models:
                            if model != "ì•Œ ìˆ˜ ì—†ìŒ":
                                st.markdown(f"**{model} ëª¨ë¸ ì‹¤í—˜ë“¤:**")
                                model_experiments = [exp for exp, df in experiment_data.items() 
                                                   if 'model_name' in df.columns and len(df) > 0 and df['model_name'].iloc[0] == model]
                                
                                if model_experiments:
                                    # í•´ë‹¹ ëª¨ë¸ì˜ ì‹¤í—˜ë“¤ë§Œ í•„í„°ë§
                                    model_data = {exp: df for exp, df in experiment_data.items() if exp in model_experiments}
                                    
                                    # ëª¨ë¸ë³„ í†µê³„
                                    model_stats = []
                                    for exp_name, df in model_data.items():
                                        model_stats.append({
                                            "ì‹¤í—˜ëª…": exp_name,
                                            "í‰ê·  Evidence Attention": round(df['avg_evidence_attention'].mean(), 4),
                                            "ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ": f"í—¤ë“œ {df['max_head'].value_counts().idxmax()}",
                                            "í—¤ë“œ ë‹¤ì–‘ì„±": len(df['max_head'].unique())
                                        })
                                    
                                    st.dataframe(pd.DataFrame(model_stats))
                                    
                                    # ëª¨ë¸ë³„ í—¤ë“œ ì‚¬ìš© íŒ¨í„´
                                    fig_model, ax_model = plt.subplots(figsize=(12, 6))
                                    for exp_name, df in model_data.items():
                                        head_counts = df['max_head'].value_counts().sort_index()
                                        ax_model.plot(head_counts.index, head_counts.values, 
                                                    marker='o', label=exp_name, linewidth=2, markersize=6)
                                    
                                    ax_model.set_xlabel('Head Index')
                                    ax_model.set_ylabel('Usage Frequency')
                                    ax_model.set_title(f'{model} Model - Head Usage Pattern Comparison')
                                    ax_model.legend()
                                    ax_model.grid(True, alpha=0.3)
                                    st.pyplot(fig_model)
                    
                    # í† í°í™” ì°¨ì´ ë¶„ì„ (ìƒˆë¡œìš´ ì‹¤í—˜ ê²°ê³¼ì—ë§Œ í•´ë‹¹)
                    st.markdown("### ğŸ”¤ í† í°í™” ì°¨ì´ ë¶„ì„")
                    
                    # í† í¬ë‚˜ì´ì € ì •ë³´ê°€ ìˆëŠ” ì‹¤í—˜ë“¤ë§Œ í•„í„°ë§
                    experiments_with_tokenizer = []
                    for exp_name, df in experiment_data.items():
                        if 'tokenizer_name' in df.columns and len(df) > 0 and df['tokenizer_name'].iloc[0] != "unknown":
                            experiments_with_tokenizer.append(exp_name)
                    
                    if len(experiments_with_tokenizer) >= 2:
                        st.markdown("**í† í¬ë‚˜ì´ì € ì •ë³´ê°€ ìˆëŠ” ì‹¤í—˜ë“¤ ê°„ì˜ ë¹„êµ:**")
                        
                        # ê°™ì€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ í† í°í™” ì°¨ì´ ë¶„ì„
                        tokenizer_comparison = {}
                        for exp_name in experiments_with_tokenizer:
                            df = experiment_data[exp_name]
                            if len(df) > 0:
                                tokenizer_name = df['tokenizer_name'].iloc[0]
                                tokenizer_comparison[exp_name] = {
                                    'tokenizer': tokenizer_name,
                                    'avg_tokens_per_prompt': df['tokens'].apply(len).mean(),
                                    'max_tokens_per_prompt': df['tokens'].apply(len).max(),
                                    'min_tokens_per_prompt': df['tokens'].apply(len).min()
                                }
                        
                        if tokenizer_comparison:
                            tokenizer_df = pd.DataFrame(tokenizer_comparison).T
                            st.dataframe(tokenizer_df)
                            
                            # í† í° ìˆ˜ ë¶„í¬ ë¹„êµ
                            fig_tokens, ax_tokens = plt.subplots(figsize=(12, 6))
                            for exp_name in experiments_with_tokenizer:
                                df = experiment_data[exp_name]
                                token_counts = df['tokens'].apply(len)
                                ax_tokens.hist(token_counts, bins=20, alpha=0.6, label=f"{exp_name} ({df['tokenizer_name'].iloc[0]})")
                            
                            ax_tokens.set_xlabel('Token Count per Prompt')
                            ax_tokens.set_ylabel('Frequency')
                            ax_tokens.set_title('Token Count Distribution by Tokenizer')
                            ax_tokens.legend()
                            ax_tokens.grid(True, alpha=0.3)
                            st.pyplot(fig_tokens)
                    else:
                        st.info("í† í¬ë‚˜ì´ì € ì •ë³´ê°€ ìˆëŠ” ì‹¤í—˜ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ í† í°í™” ì°¨ì´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¹„êµ
                    st.markdown("### ğŸ“ˆ ë„ë©”ì¸ë³„ Evidence Attention ë¹„êµ")
                    fig, ax = plt.subplots(figsize=(12, 6))
                    
                    for i, (exp_name, df) in enumerate(experiment_data.items()):
                        domain_means = df.groupby('domain')['avg_evidence_attention'].mean()
                        ax.plot(domain_means.index, domain_means.values, 
                               marker='o', label=exp_name, linewidth=2, markersize=8)
                    
                    ax.set_xlabel('Domain')
                    ax.set_ylabel('Average Evidence Attention')
                    ax.set_title('Domain Performance Comparison Across Experiments')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    st.pyplot(fig)
                    
                    # ë„ë©”ì¸-í—¤ë“œ íˆíŠ¸ë§µ ë¹„êµ
                    st.markdown("### ğŸ”¥ ë„ë©”ì¸-í—¤ë“œ íˆíŠ¸ë§µ ë¹„êµ")
                    
                    # ëª¨ë“  ì‹¤í—˜ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œ
                    combined_data = []
                    for exp_name, df in experiment_data.items():
                        for _, row in df.iterrows():
                            combined_data.append({
                                'experiment': exp_name,
                                'domain': row['domain'],
                                'max_head': row['max_head'],
                                'avg_evidence_attention': row['avg_evidence_attention']
                            })
                    
                    combined_df = pd.DataFrame(combined_data)
                    
                    # ì‹¤í—˜ë³„ ë„ë©”ì¸-í—¤ë“œ íˆíŠ¸ë§µ
                    fig_combined, ax_combined = plt.subplots(figsize=(15, 8))
                    
                    # ì‹¤í—˜ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                    pivot_data = combined_df.groupby(['experiment', 'domain', 'max_head']).size().unstack(fill_value=0)
                    
                    # íˆíŠ¸ë§µ ìƒì„±
                    sns.heatmap(pivot_data, annot=True, fmt="d", cmap="YlOrRd", ax=ax_combined, cbar_kws={'label': 'Frequency'})
                    ax_combined.set_xlabel("Head Index")
                    ax_combined.set_ylabel("Experiment-Domain")
                    ax_combined.set_title("Combined Domain-Head Evidence Response Frequency Across Experiments")
                    
                    # yì¶• ë¼ë²¨ì„ ë” ì½ê¸° ì‰½ê²Œ ì¡°ì •
                    y_labels = [f"{exp}-{domain}" for exp, domain in pivot_data.index]
                    ax_combined.set_yticklabels(y_labels, rotation=0)
                    
                    st.pyplot(fig_combined)
                    st.caption("*ëª¨ë“  ì‹¤í—˜ì˜ ë„ë©”ì¸-í—¤ë“œ ë°˜ì‘ ë¹ˆë„ë¥¼ í•˜ë‚˜ì˜ íˆíŠ¸ë§µìœ¼ë¡œ í†µí•© í‘œì‹œ*")
                    
                    # ì‹¤í—˜ë³„ í‰ê·  Evidence Attention ë¹„êµ íˆíŠ¸ë§µ
                    st.markdown("### ğŸ“Š ì‹¤í—˜ë³„ í‰ê·  Evidence Attention ë¹„êµ")
                    
                    # ì‹¤í—˜-ë„ë©”ì¸ë³„ í‰ê·  Evidence Attention ê³„ì‚°
                    avg_attention_pivot = combined_df.groupby(['experiment', 'domain'])['avg_evidence_attention'].mean().unstack(fill_value=0)
                    
                    fig_avg, ax_avg = plt.subplots(figsize=(12, 6))
                    sns.heatmap(avg_attention_pivot, annot=True, fmt=".4f", cmap="Blues", ax=ax_avg, cbar_kws={'label': 'Average Evidence Attention'})
                    ax_avg.set_xlabel("Domain")
                    ax_avg.set_ylabel("Experiment")
                    ax_avg.set_title("Average Evidence Attention by Experiment and Domain")
                    st.pyplot(fig_avg)
                    st.caption("*ê° ì‹¤í—˜-ë„ë©”ì¸ ì¡°í•©ì˜ í‰ê·  Evidence Attention ê°’*")
                    
                    # í—¤ë“œ ì‚¬ìš© íŒ¨í„´ í†µí•© ë¹„êµ
                    st.markdown("### ğŸ§  í—¤ë“œ ì‚¬ìš© íŒ¨í„´ í†µí•© ë¹„êµ")
                    
                    # ëª¨ë“  ì‹¤í—˜ì˜ í—¤ë“œ ì‚¬ìš© ë¹ˆë„ë¥¼ í†µí•©
                    all_head_usage = combined_df['max_head'].value_counts().sort_index()
                    
                    fig_head, ax_head = plt.subplots(figsize=(12, 6))
                    bars = ax_head.bar(all_head_usage.index, all_head_usage.values, alpha=0.7)
                    ax_head.set_xlabel('Head Index')
                    ax_head.set_ylabel('Total Usage Frequency')
                    ax_head.set_title('Combined Head Usage Pattern Across All Experiments')
                    ax_head.grid(True, alpha=0.3)
                    
                    # ìƒìœ„ 5ê°œ í—¤ë“œ ê°•ì¡°
                    top_heads = all_head_usage.head(5)
                    for head in top_heads.index:
                        if head in all_head_usage.index:
                            idx = list(all_head_usage.index).index(head)
                            bars[idx].set_color('red')
                            bars[idx].set_alpha(0.8)
                    
                    st.pyplot(fig_head)
                    st.caption("*ëª¨ë“  ì‹¤í—˜ì„ í†µí•©í•œ í—¤ë“œ ì‚¬ìš© ë¹ˆë„ (ë¹¨ê°„ìƒ‰: ìƒìœ„ 5ê°œ í—¤ë“œ)*")
                    
                    # ì‹¤í—˜ë³„ í—¤ë“œ ì‚¬ìš© íŒ¨í„´ ë¹„êµ (í•˜ë‚˜ì˜ ê·¸ë˜í”„)
                    st.markdown("### ğŸ“ˆ ì‹¤í—˜ë³„ í—¤ë“œ ì‚¬ìš© íŒ¨í„´ ë¹„êµ")
                    
                    fig_compare, ax_compare = plt.subplots(figsize=(15, 8))
                    
                    # ê° ì‹¤í—˜ë³„ë¡œ í—¤ë“œ ì‚¬ìš© ë¹ˆë„ ê³„ì‚°
                    for exp_name, df in experiment_data.items():
                        head_counts = df['max_head'].value_counts().sort_index()
                        ax_compare.plot(head_counts.index, head_counts.values, 
                                      marker='o', label=exp_name, linewidth=2, markersize=6)
                    
                    ax_compare.set_xlabel('Head Index')
                    ax_compare.set_ylabel('Usage Frequency')
                    ax_compare.set_title('Head Usage Pattern Comparison Across Experiments')
                    ax_compare.legend()
                    ax_compare.grid(True, alpha=0.3)
                    st.pyplot(fig_compare)
                    st.caption("*ê° ì‹¤í—˜ì—ì„œ ì‚¬ìš©ëœ í—¤ë“œì˜ ë¹ˆë„ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ë¹„êµ*")
                    
                    # Evidence Attention ë¶„í¬ í†µí•© ë¹„êµ
                    st.markdown("### ğŸ“Š Evidence Attention ë¶„í¬ í†µí•© ë¹„êµ")
                    
                    fig_dist, ax_dist = plt.subplots(figsize=(12, 6))
                    
                    # ê° ì‹¤í—˜ë³„ë¡œ íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
                    for exp_name, df in experiment_data.items():
                        ax_dist.hist(df['avg_evidence_attention'], bins=20, alpha=0.6, label=exp_name, density=True)
                    
                    ax_dist.set_xlabel('Evidence Attention')
                    ax_dist.set_ylabel('Density')
                    ax_dist.set_title('Evidence Attention Distribution Comparison (Normalized)')
                    ax_dist.legend()
                    ax_dist.grid(True, alpha=0.3)
                    st.pyplot(fig_dist)
                    st.caption("*ê° ì‹¤í—˜ì˜ Evidence Attention ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ ë¹„êµ*")
                    
                    # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
                    st.markdown("### ğŸ“Š í†µê³„ì  ë¹„êµ")
                    
                    if len(experiment_data) == 2:
                        # ë‘ ì‹¤í—˜ ê²°ê³¼ì˜ Evidence Attention ë¹„êµ
                        exp_names = list(experiment_data.keys())
                        df1 = experiment_data[exp_names[0]]
                        df2 = experiment_data[exp_names[1]]
                        
                        # t-test ìˆ˜í–‰
                        t_stat, p_value = stats.ttest_ind(df1['avg_evidence_attention'], df2['avg_evidence_attention'])
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("t-í†µê³„ëŸ‰", f"{t_stat:.4f}")
                        with col2:
                            st.metric("p-ê°’", f"{p_value:.4f}")
                        with col3:
                            significance = "ìœ ì˜í•¨" if p_value < 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                            st.metric("í†µê³„ì  ìœ ì˜ì„±", significance)
                        
                        st.markdown(f"""
                        **í•´ì„:**
                        - p-ê°’ì´ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ ë‘ ì‹¤í—˜ ê²°ê³¼ ê°„ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        - í˜„ì¬ p-ê°’: {p_value:.4f} ({significance})
                        """)
                    elif len(experiment_data) > 2:
                        # ë‹¤ì¤‘ ì‹¤í—˜ ë¹„êµë¥¼ ìœ„í•œ ANOVA
                        # ANOVA ìˆ˜í–‰
                        groups = [df['avg_evidence_attention'].values for df in experiment_data.values()]
                        f_stat, p_value = stats.f_oneway(*groups)
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("F-í†µê³„ëŸ‰", f"{f_stat:.4f}")
                        with col2:
                            st.metric("p-ê°’", f"{p_value:.4f}")
                        with col3:
                            significance = "ìœ ì˜í•¨" if p_value < 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                            st.metric("í†µê³„ì  ìœ ì˜ì„±", significance)
                        
                        st.markdown(f"""
                        **í•´ì„:**
                        - ANOVAë¥¼ í†µí•´ ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ê°„ì˜ í†µê³„ì  ì°¨ì´ë¥¼ ê²€ì •í•©ë‹ˆë‹¤.
                        - p-ê°’ì´ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ ì‹¤í—˜ë“¤ ê°„ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        - í˜„ì¬ p-ê°’: {p_value:.4f} ({significance})
                        """)
                        
                    # ì‹¤í—˜ ê°„ ì¼ê´€ì„± ë¶„ì„
                    st.markdown("### ğŸ”„ ì‹¤í—˜ ê°„ ì¼ê´€ì„± ë¶„ì„")
                    
                    # ê³µí†µ í—¤ë“œ ë¶„ì„
                    all_heads = set()
                    for df in experiment_data.values():
                        all_heads.update(df['max_head'].unique())
                    
                    head_consistency = {}
                    for head in all_heads:
                        consistency_count = 0
                        for df in experiment_data.values():
                            if head in df['max_head'].values:
                                consistency_count += 1
                        head_consistency[head] = consistency_count / len(experiment_data)
                    
                    # ì¼ê´€ì„± ë†’ì€ í—¤ë“œë“¤ í‘œì‹œ
                    consistent_heads = {head: score for head, score in head_consistency.items() if score >= 0.5}
                    
                    if consistent_heads:
                        st.markdown("**ì—¬ëŸ¬ ì‹¤í—˜ì—ì„œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ëœ í—¤ë“œë“¤ (50% ì´ìƒ):**")
                        for head, consistency in sorted(consistent_heads.items(), key=lambda x: x[1], reverse=True):
                            st.markdown(f"- í—¤ë“œ {head}: {consistency*100:.1f}% ì¼ê´€ì„±")
                    else:
                        st.markdown("**ì¼ê´€ë˜ê²Œ ì‚¬ìš©ëœ í—¤ë“œê°€ ì—†ìŠµë‹ˆë‹¤.**")
                    
                    # ì¼ê´€ì„± ì‹œê°í™”
                    if head_consistency:
                        fig_consistency, ax_consistency = plt.subplots(figsize=(12, 6))
                        heads = list(head_consistency.keys())
                        consistency_scores = list(head_consistency.values())
                        
                        bars = ax_consistency.bar(heads, consistency_scores, alpha=0.7)
                        ax_consistency.set_xlabel('Head Index')
                        ax_consistency.set_ylabel('Consistency Score')
                        ax_consistency.set_title('Head Consistency Across Experiments')
                        ax_consistency.grid(True, alpha=0.3)
                        
                        # 50% ì´ìƒ ì¼ê´€ì„± ìˆëŠ” í—¤ë“œ ê°•ì¡°
                        for i, score in enumerate(consistency_scores):
                            if score >= 0.5:
                                bars[i].set_color('green')
                                bars[i].set_alpha(0.8)
                        
                        st.pyplot(fig_consistency)
                        st.caption("*ê° í—¤ë“œì˜ ì‹¤í—˜ ê°„ ì¼ê´€ì„± ì ìˆ˜ (ë…¹ìƒ‰: 50% ì´ìƒ ì¼ê´€ì„±)*")
                    
                    # ì¢…í•© ë¶„ì„ ìš”ì•½
                    st.markdown("### ğŸ“‹ ì¢…í•© ë¶„ì„ ìš”ì•½")
                    
                    summary_points = []
                    
                    # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì‹¤í—˜
                    best_exp = max(experiment_data.items(), key=lambda x: x[1]['avg_evidence_attention'].mean())
                    summary_points.append(f"**ìµœê³  ì„±ëŠ¥ ì‹¤í—˜:** {best_exp[0]} (í‰ê·  Evidence Attention: {best_exp[1]['avg_evidence_attention'].mean():.4f})")
                    
                    # ê°€ì¥ ì¼ê´€ëœ í—¤ë“œ
                    if consistent_heads:
                        most_consistent_head = max(consistent_heads.items(), key=lambda x: x[1])
                        summary_points.append(f"**ê°€ì¥ ì¼ê´€ëœ í—¤ë“œ:** í—¤ë“œ {most_consistent_head[0]} ({most_consistent_head[1]*100:.1f}% ì¼ê´€ì„±)")
                    
                    # ë„ë©”ì¸ë³„ íŒ¨í„´
                    domain_patterns = {}
                    for exp_name, df in experiment_data.items():
                        for domain in df['domain'].unique():
                            if domain not in domain_patterns:
                                domain_patterns[domain] = []
                            domain_patterns[domain].append(df[df['domain']==domain]['avg_evidence_attention'].mean())
                    
                    for domain, values in domain_patterns.items():
                        if len(values) > 1:
                            variance = np.var(values)
                            if variance < 0.01:  # ë‚®ì€ ë¶„ì‚°
                                summary_points.append(f"**{domain} ë„ë©”ì¸:** ì‹¤í—˜ ê°„ ì¼ê´€ëœ ì„±ëŠ¥ (ë¶„ì‚°: {variance:.4f})")
                            else:
                                summary_points.append(f"**{domain} ë„ë©”ì¸:** ì‹¤í—˜ ê°„ ì„±ëŠ¥ ë³€ë™ ìˆìŒ (ë¶„ì‚°: {variance:.4f})")
                    
                    # ì „ì²´ í†µê³„
                    total_samples = sum(len(df) for df in experiment_data.values())
                    summary_points.append(f"**ì´ ë¶„ì„ ìƒ˜í”Œ ìˆ˜:** {total_samples}")
                    summary_points.append(f"**ë¶„ì„ ì‹¤í—˜ ìˆ˜:** {len(experiment_data)}")
                    
                    for point in summary_points:
                        st.markdown(f"- {point}")
                    
                    st.success("nì°¨ ì‹¤í—˜ ë¹„êµ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.warning("ë¹„êµ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.warning("ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤í—˜ íƒ­ì—ì„œ ë¨¼ì € ì‹¤í—˜ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.") 