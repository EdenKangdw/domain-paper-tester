import streamlit as st
import json
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pickle
import os
import matplotlib.pyplot as plt
from scipy import stats

def extract_features_from_results(results_json):
    data = []
    for r in results_json:
        # headë³„ evidence attention í‰ê· ê°’ì„ featureë¡œ ì‚¬ìš©
        # ì‹¤í—˜ ê²°ê³¼ì— avg_evidence_attention_per_head ë˜ëŠ” avg_evidence_attention_whole(ë¦¬ìŠ¤íŠ¸)ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if 'avg_evidence_attention_per_head' in r:
            attention_vector = r['avg_evidence_attention_per_head']
        elif isinstance(r.get('avg_evidence_attention_whole'), list):
            attention_vector = r['avg_evidence_attention_whole']
        elif isinstance(r.get('avg_evidence_attention'), list):
            attention_vector = r['avg_evidence_attention']  # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
        else:
            # fallback: max_head one-hot
            attention_vector = [0]*32
            if 0 <= r.get('max_head', -1) < 32:
                attention_vector[r['max_head']] = 1
        
        attention_vector = np.array(attention_vector)
        
        # ê¸°ë³¸ 32ê°œ head attention ê°’
        basic_features = list(attention_vector)
        
        # í†µê³„ì  íŠ¹ì„± ì¶”ê°€
        stats_features = [
            attention_vector.mean(),      # í‰ê· 
            attention_vector.std(),       # í‘œì¤€í¸ì°¨
            attention_vector.var(),       # ë¶„ì‚°
            attention_vector.max(),       # ìµœëŒ€ê°’
            attention_vector.min(),       # ìµœì†Œê°’
            attention_vector.max() - attention_vector.min(),  # ë²”ìœ„
            np.percentile(attention_vector, 25),  # 1ì‚¬ë¶„ìœ„ìˆ˜
            np.percentile(attention_vector, 50),  # ì¤‘ì•™ê°’
            np.percentile(attention_vector, 75),  # 3ì‚¬ë¶„ìœ„ìˆ˜
            np.percentile(attention_vector, 75) - np.percentile(attention_vector, 25),  # IQR
        ]
        
        # ìƒëŒ€ì  ë¹„ìœ¨ (ì •ê·œí™”)
        total_attention = attention_vector.sum()
        if total_attention > 0:
            normalized_features = list(attention_vector / total_attention)
        else:
            normalized_features = [0] * 32
        
        # ìˆœìœ„ ì •ë³´
        rank_features = list(np.argsort(attention_vector)[::-1])  # ë‚´ë¦¼ì°¨ìˆœ ìˆœìœ„
        
        # ì°¨ë¶„ê°’ (ì¸ì ‘í•œ head ê°„ì˜ attention ì°¨ì´)
        diff_features = list(np.diff(attention_vector))
        if len(diff_features) < 31:  # 32ê°œ headë©´ 31ê°œ ì°¨ë¶„ê°’
            diff_features.extend([0] * (31 - len(diff_features)))
        
        # ì§‘ì¤‘ë„ ì§€í‘œ
        concentration_features = [
            attention_vector.max() / (attention_vector.mean() + 1e-8),  # ìµœëŒ€ê°’/í‰ê·  ë¹„ìœ¨
            attention_vector.max() / (attention_vector.std() + 1e-8),   # ìµœëŒ€ê°’/í‘œì¤€í¸ì°¨ ë¹„ìœ¨
            (attention_vector > attention_vector.mean()).sum(),         # í‰ê·  ì´ìƒì¸ head ìˆ˜
            (attention_vector > attention_vector.mean() + attention_vector.std()).sum(),  # í‰ê· +í‘œì¤€í¸ì°¨ ì´ìƒì¸ head ìˆ˜
        ]
        
        # ëª¨ë“  feature ê²°í•©
        all_features = (basic_features + stats_features + normalized_features + 
                       rank_features + diff_features + concentration_features)
        
        data.append(all_features + [r['domain']])
    
    # ì»¬ëŸ¼ëª… ìƒì„±
    basic_cols = [f'head_{i}' for i in range(32)]
    stats_cols = ['mean', 'std', 'var', 'max', 'min', 'range', 'q25', 'median', 'q75', 'iqr']
    norm_cols = [f'norm_head_{i}' for i in range(32)]
    rank_cols = [f'rank_head_{i}' for i in range(32)]
    diff_cols = [f'diff_head_{i}' for i in range(31)]
    concentration_cols = ['max_mean_ratio', 'max_std_ratio', 'above_mean_count', 'above_std_count']
    
    all_cols = basic_cols + stats_cols + norm_cols + rank_cols + diff_cols + concentration_cols + ['domain']
    
    df = pd.DataFrame(data, columns=all_cols)
    return df

def show():
    st.title("ğŸ› ï¸ ë„ë©”ì¸ ë¶„ë¥˜ê¸° í•™ìŠµ (ê³ ê¸‰ Feature)")
    st.markdown("""
    ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì„ ì„ íƒí•´ì„œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    
    **ìƒˆë¡œìš´ Feature ì—”ì§€ë‹ˆì–´ë§:**
    - ê¸°ë³¸ 32ê°œ head attention ê°’
    - í†µê³„ì  íŠ¹ì„± (í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ì‚°, ìµœëŒ€/ìµœì†Œ, ì‚¬ë¶„ìœ„ìˆ˜ ë“±)
    - ì •ê·œí™”ëœ attention ë¹„ìœ¨
    - ìˆœìœ„ ì •ë³´
    - ì¸ì ‘ head ê°„ ì°¨ë¶„ê°’
    - ì§‘ì¤‘ë„ ì§€í‘œ
    
    ì´ **133ê°œ feature**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!
    """)

    # ì‹¤í—˜ ê²°ê³¼ í´ë” íƒìƒ‰
    experiment_root = "experiment_results"
    if not os.path.exists(experiment_root):
        st.error("experiment_results í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    model_dirs = [d for d in os.listdir(experiment_root) if os.path.isdir(os.path.join(experiment_root, d))]
    if not model_dirs:
        st.error("ì‹¤í—˜ ê²°ê³¼ê°€ ìˆëŠ” ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    selected_model = st.selectbox("ëª¨ë¸ ì„ íƒ", model_dirs)
    result_files = [f for f in os.listdir(os.path.join(experiment_root, selected_model)) if f.endswith('.json') and not f.endswith('_errors.json')]
    if not result_files:
        st.error("ì„ íƒí•œ ëª¨ë¸ì— ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    selected_file = st.selectbox("ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ì„ íƒ", result_files)

    # ëª¨ë¸ ì„ íƒ ì˜µì…˜
    st.markdown("### ğŸ¤– ëª¨ë¸ ì„ íƒ")
    model_options = st.multiselect(
        "í•™ìŠµí•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
        ["RandomForest", "XGBoost", "SVM", "Neural Network"],
        default=["RandomForest", "XGBoost"]
    )

    if st.button("ê³ ê¸‰ ë¶„ë¥˜ê¸° í•™ìŠµ ë° ì €ì¥"):
        with st.spinner("Feature ì¶”ì¶œ ì¤‘..."):
            path = os.path.join(experiment_root, selected_model, selected_file)
            with open(path, 'r', encoding='utf-8') as f:
                results = json.load(f)
            df = extract_features_from_results(results)
            
            st.success(f"âœ… {len(df)}ê°œ ìƒ˜í”Œ, {len(df.columns)-1}ê°œ feature ì¶”ì¶œ ì™„ë£Œ!")
            
            # Feature ì •ë³´ í‘œì‹œ
            st.markdown("### ğŸ“Š Feature ì •ë³´")
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**ì´ ìƒ˜í”Œ ìˆ˜**: {len(df)}")
                st.write(f"**ì´ Feature ìˆ˜**: {len(df.columns)-1}")
                st.write(f"**ë„ë©”ì¸ ìˆ˜**: {len(df['domain'].unique())}")
            with col2:
                st.write("**ë„ë©”ì¸ë³„ ìƒ˜í”Œ ìˆ˜**:")
                domain_counts = df['domain'].value_counts()
                for domain, count in domain_counts.items():
                    st.write(f"- {domain}: {count}ê°œ")
        
        with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘..."):
            # Featureì™€ íƒ€ê²Ÿ ë¶„ë¦¬
            feature_cols = [col for col in df.columns if col != 'domain']
            X = df[feature_cols].values
            y = df['domain'].values
            
            # ë ˆì´ë¸” ì¸ì½”ë”©
            label_encoder = LabelEncoder()
            y_encoded = label_encoder.fit_transform(y)
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)
            
            # ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
            models = {}
            results_comparison = []
            
            if "RandomForest" in model_options:
                from sklearn.ensemble import RandomForestClassifier
                rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
                rf.fit(X_train, y_train)
                y_pred_rf = rf.predict(X_test)
                # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
                y_test_original = label_encoder.inverse_transform(y_test)
                y_pred_rf_original = label_encoder.inverse_transform(y_pred_rf)
                report_rf = classification_report(y_test_original, y_pred_rf_original, output_dict=True)
                models["RandomForest"] = (rf, label_encoder)
                results_comparison.append({
                    "Model": "RandomForest",
                    "Accuracy": report_rf['accuracy'],
                    "Macro F1": report_rf['macro avg']['f1-score'],
                    "Weighted F1": report_rf['weighted avg']['f1-score']
                })
                
                # RandomForest ëª¨ë¸ ì €ì¥ (label_encoder í¬í•¨)
                with open('domain_classifier_rf.pkl', 'wb') as f:
                    pickle.dump((rf, label_encoder), f)
            
            if "XGBoost" in model_options:
                try:
                    import xgboost as xgb
                    xgb_model = xgb.XGBClassifier(
                        n_estimators=200,
                        max_depth=6,
                        learning_rate=0.1,
                        random_state=42,
                        eval_metric='mlogloss'
                    )
                    xgb_model.fit(X_train, y_train)
                    y_pred_xgb = xgb_model.predict(X_test)
                    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
                    y_test_original = label_encoder.inverse_transform(y_test)
                    y_pred_xgb_original = label_encoder.inverse_transform(y_pred_xgb)
                    report_xgb = classification_report(y_test_original, y_pred_xgb_original, output_dict=True)
                    models["XGBoost"] = (xgb_model, label_encoder)
                    results_comparison.append({
                        "Model": "XGBoost",
                        "Accuracy": report_xgb['accuracy'],
                        "Macro F1": report_xgb['macro avg']['f1-score'],
                        "Weighted F1": report_xgb['weighted avg']['f1-score']
                    })
                    
                    # XGBoost ëª¨ë¸ ì €ì¥ (label_encoder í¬í•¨)
                    with open('domain_classifier_xgb.pkl', 'wb') as f:
                        pickle.dump((xgb_model, label_encoder), f)
                except ImportError:
                    st.warning("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install xgboost`ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            
            if "SVM" in model_options:
                from sklearn.svm import SVC
                from sklearn.preprocessing import StandardScaler
                
                # SVMì„ ìœ„í•´ ë°ì´í„° ì •ê·œí™”
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
                svm.fit(X_train_scaled, y_train)
                y_pred_svm = svm.predict(X_test_scaled)
                # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
                y_test_original = label_encoder.inverse_transform(y_test)
                y_pred_svm_original = label_encoder.inverse_transform(y_pred_svm)
                report_svm = classification_report(y_test_original, y_pred_svm_original, output_dict=True)
                models["SVM"] = (svm, scaler, label_encoder)
                results_comparison.append({
                    "Model": "SVM",
                    "Accuracy": report_svm['accuracy'],
                    "Macro F1": report_svm['macro avg']['f1-score'],
                    "Weighted F1": report_svm['weighted avg']['f1-score']
                })
                
                # SVM ëª¨ë¸ ì €ì¥ (scaler, label_encoder í¬í•¨)
                with open('domain_classifier_svm.pkl', 'wb') as f:
                    pickle.dump((svm, scaler, label_encoder), f)
            
            if "Neural Network" in model_options:
                try:
                    from sklearn.neural_network import MLPClassifier
                    nn = MLPClassifier(
                        hidden_layer_sizes=(100, 50),
                        max_iter=500,
                        random_state=42,
                        early_stopping=True,
                        validation_fraction=0.1
                    )
                    nn.fit(X_train, y_train)
                    y_pred_nn = nn.predict(X_test)
                    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
                    y_test_original = label_encoder.inverse_transform(y_test)
                    y_pred_nn_original = label_encoder.inverse_transform(y_pred_nn)
                    report_nn = classification_report(y_test_original, y_pred_nn_original, output_dict=True)
                    models["Neural Network"] = (nn, label_encoder)
                    results_comparison.append({
                        "Model": "Neural Network",
                        "Accuracy": report_nn['accuracy'],
                        "Macro F1": report_nn['macro avg']['f1-score'],
                        "Weighted F1": report_nn['weighted avg']['f1-score']
                    })
                    
                    # Neural Network ëª¨ë¸ ì €ì¥ (label_encoder í¬í•¨)
                    with open('domain_classifier_nn.pkl', 'wb') as f:
                        pickle.dump((nn, label_encoder), f)
                except Exception as e:
                    st.warning(f"Neural Network í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            st.success("âœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        
        # ê²°ê³¼ ë¹„êµ í‘œì‹œ
        st.markdown("### ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        comparison_df = pd.DataFrame(results_comparison)
        st.dataframe(comparison_df)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        if results_comparison:
            best_model = max(results_comparison, key=lambda x: x['Macro F1'])
            st.success(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['Model']} (Macro F1: {best_model['Macro F1']:.4f})")
        
        # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
        for model_name, model_tuple in models.items():
            st.markdown(f"### ğŸ“Š {model_name} ìƒì„¸ ê²°ê³¼")
            
            # ëª¨ë¸ê³¼ label_encoder ë¶„ë¦¬
            if isinstance(model_tuple, tuple):
                if len(model_tuple) == 2:  # (model, label_encoder)
                    model, label_encoder = model_tuple
                    scaler = None
                elif len(model_tuple) == 3:  # (model, scaler, label_encoder)
                    model, scaler, label_encoder = model_tuple
                else:
                    continue
            else:
                model = model_tuple
                label_encoder = None
                scaler = None
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            if scaler:
                X_test_processed = scaler.transform(X_test)
            else:
                X_test_processed = X_test
            
            y_pred = model.predict(X_test_processed)
            
            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ë ˆì´ë¸”ë¡œ ë³€í™˜
            if label_encoder:
                y_test_original = label_encoder.inverse_transform(y_test)
                y_pred_original = label_encoder.inverse_transform(y_pred)
                report = classification_report(y_test_original, y_pred_original, output_dict=True)
            else:
                report = classification_report(y_test, y_pred, output_dict=True)
            
            st.json(report)
            
            # Feature ì¤‘ìš”ë„ ë¶„ì„ (RandomForest, XGBoostë§Œ)
            if hasattr(model, 'feature_importances_'):
                st.markdown(f"#### ğŸ” {model_name} Feature ì¤‘ìš”ë„ (ìƒìœ„ 20ê°œ)")
                feature_importance = pd.DataFrame({
                    'feature': feature_cols,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False).head(20)
                
                st.dataframe(feature_importance)
                
                # ì¤‘ìš”ë„ ì‹œê°í™” (RandomForestë§Œ)
                if model_name == "RandomForest":
                    fig, ax = plt.subplots(figsize=(10, 6))
                    top_features = feature_importance.head(10)
                    ax.barh(range(len(top_features)), top_features['importance'])
                    ax.set_yticks(range(len(top_features)))
                    ax.set_yticklabels(top_features['feature'])
                    ax.set_xlabel('Feature Importance')
                    ax.set_title(f'{model_name} - Top 10 Most Important Features')
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # Medical ë„ë©”ì¸ íŠ¹ì„± ë¶„ì„
                st.markdown(f"#### ğŸ¥ Medical ë„ë©”ì¸ íŠ¹ì„± ë¶„ì„")
                medical_analysis = analyze_domain_characteristics(model, X_train, y_train, feature_cols, 'medical')
                st.markdown(medical_analysis)

def analyze_domain_characteristics(model, X_train, y_train, feature_cols, target_domain='medical'):
    """íŠ¹ì • ë„ë©”ì¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    analysis = []
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ê³¼ ë‹¤ë¥¸ ë„ë©”ì¸ ë¶„ë¦¬
    target_mask = (y_train == target_domain)
    other_mask = (y_train != target_domain)
    
    if not target_mask.any() or not other_mask.any():
        return f"âš ï¸ {target_domain} ë„ë©”ì¸ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    X_target = X_train[target_mask]
    X_other = X_train[other_mask]
    
    # ê° featureì— ëŒ€í•´ t-test ìˆ˜í–‰
    significant_features = []
    for i, feature_name in enumerate(feature_cols):
        target_values = X_target[:, i]
        other_values = X_other[:, i]
        
        # t-test
        t_stat, p_value = stats.ttest_ind(target_values, other_values)
        
        if p_value < 0.05:  # ìœ ì˜ìˆ˜ì¤€ 5%
            target_mean = target_values.mean()
            other_mean = other_values.mean()
            effect_size = abs(target_mean - other_mean) / (target_values.std() + other_values.std()) * 0.5
            
            significant_features.append({
                'feature': feature_name,
                'target_mean': target_mean,
                'other_mean': other_mean,
                'difference': target_mean - other_mean,
                'p_value': p_value,
                'effect_size': effect_size
            })
    
    # íš¨ê³¼ í¬ê¸°ë¡œ ì •ë ¬
    significant_features.sort(key=lambda x: x['effect_size'], reverse=True)
    
    analysis.append(f"## {target_domain.capitalize()} ë„ë©”ì¸ íŠ¹ì„± ë¶„ì„ ê²°ê³¼")
    analysis.append(f"")
    analysis.append(f"**ì´ Feature ìˆ˜**: {len(feature_cols)}")
    analysis.append(f"**ìœ ì˜í•œ Feature ìˆ˜**: {len(significant_features)} (p < 0.05)")
    analysis.append(f"**{target_domain} ìƒ˜í”Œ ìˆ˜**: {target_mask.sum()}")
    analysis.append(f"**ë‹¤ë¥¸ ë„ë©”ì¸ ìƒ˜í”Œ ìˆ˜**: {other_mask.sum()}")
    analysis.append(f"")
    
    if significant_features:
        analysis.append("### ğŸ¯ ê°€ì¥ ì¤‘ìš”í•œ ì°¨ì´ì  (ìƒìœ„ 10ê°œ)")
        analysis.append("")
        analysis.append("| Feature | Medical í‰ê·  | ë‹¤ë¥¸ ë„ë©”ì¸ í‰ê·  | ì°¨ì´ | p-value | íš¨ê³¼í¬ê¸° |")
        analysis.append("|---------|-------------|----------------|------|---------|----------|")
        
        for i, feat in enumerate(significant_features[:10]):
            analysis.append(f"| {feat['feature']} | {feat['target_mean']:.4f} | {feat['other_mean']:.4f} | {feat['difference']:.4f} | {feat['p_value']:.4f} | {feat['effect_size']:.4f} |")
        
        analysis.append("")
        analysis.append("### ğŸ’¡ í•´ì„")
        analysis.append("")
        
        # ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” feature ë¶„ì„
        top_feature = significant_features[0]
        if top_feature['difference'] > 0:
            analysis.append(f"- **{top_feature['feature']}**: Medical ë„ë©”ì¸ì´ ë‹¤ë¥¸ ë„ë©”ì¸ë³´ë‹¤ {top_feature['difference']:.4f} ë†’ìŒ")
        else:
            analysis.append(f"- **{top_feature['feature']}**: Medical ë„ë©”ì¸ì´ ë‹¤ë¥¸ ë„ë©”ì¸ë³´ë‹¤ {abs(top_feature['difference']):.4f} ë‚®ìŒ")
        
        # Feature íƒ€ì…ë³„ ë¶„ì„
        head_features = [f for f in significant_features if f['feature'].startswith('head_')]
        stats_features = [f for f in significant_features if f['feature'] in ['mean', 'std', 'var', 'max', 'min', 'range', 'q25', 'median', 'q75', 'iqr']]
        norm_features = [f for f in significant_features if f['feature'].startswith('norm_head_')]
        
        analysis.append(f"")
        analysis.append("### ğŸ“Š Feature íƒ€ì…ë³„ ë¶„ì„")
        analysis.append(f"- **ê¸°ë³¸ Head Features**: {len(head_features)}ê°œ ìœ ì˜í•¨")
        analysis.append(f"- **í†µê³„ì  Features**: {len(stats_features)}ê°œ ìœ ì˜í•¨")
        analysis.append(f"- **ì •ê·œí™” Features**: {len(norm_features)}ê°œ ìœ ì˜í•¨")
        
        if head_features:
            analysis.append(f"")
            analysis.append("#### ğŸ§  ì¤‘ìš”í•œ Headë“¤")
            for feat in head_features[:5]:
                head_num = feat['feature'].replace('head_', '')
                analysis.append(f"- **Head {head_num}**: Medical ë„ë©”ì¸ì—ì„œ {feat['difference']:.4f} {'ë†’ìŒ' if feat['difference'] > 0 else 'ë‚®ìŒ'}")
    
    else:
        analysis.append("âš ï¸ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì´ëŠ” featureê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    return "\n".join(analysis) 