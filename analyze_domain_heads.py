import json
import os
import pandas as pd
import numpy as np
from collections import defaultdict

def load_experiment_results():
    """ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    results_dir = "experiment_results"
    all_results = []
    
    for filename in os.listdir(results_dir):
        if filename.endswith('.json'):
            filepath = os.path.join(results_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for result in data:
                        all_results.append(result)
            except Exception as e:
                print(f"íŒŒì¼ {filename} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return all_results

def analyze_domain_significant_heads(results):
    """ë„ë©”ì¸ë³„ ìœ ì˜ë¯¸í•œ ì–´í…ì…˜ í—¤ë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    df = pd.DataFrame(results)
    
    print("=" * 80)
    print("ë„ë©”ì¸ë³„ ìœ ì˜ë¯¸í•œ ì–´í…ì…˜ í—¤ë“œ ë¶„ì„")
    print("=" * 80)
    print()
    
    # ì „ì²´ í—¤ë“œ ì‚¬ìš© í†µê³„
    print("ğŸ“Š ì „ì²´ í—¤ë“œ ì‚¬ìš© í†µê³„")
    total_head_usage = df['max_head'].value_counts().sort_values(ascending=False)
    print(f"ì´ ì‚¬ìš©ëœ í—¤ë“œ ìˆ˜: {len(total_head_usage)}")
    print(f"ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ: {total_head_usage.index[0]} (ì‚¬ìš© {total_head_usage.iloc[0]}íšŒ)")
    print()
    
    # ë„ë©”ì¸ë³„ ë¶„ì„
    domains = df['domain'].unique()
    
    for domain in domains:
        print(f"ğŸ” {domain.upper()} ë„ë©”ì¸ ë¶„ì„")
        print("-" * 50)
        
        domain_df = df[df['domain'] == domain]
        domain_head_usage = domain_df['max_head'].value_counts().sort_values(ascending=False)
        
        # ê¸°ë³¸ í†µê³„
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(domain_df)}")
        print(f"ì‚¬ìš©ëœ í—¤ë“œ ìˆ˜: {len(domain_df['max_head'].unique())}")
        print(f"í‰ê·  Evidence Attention: {domain_df['avg_evidence_attention'].mean():.4f}")
        print()
        
        # ìƒìœ„ 5ê°œ í—¤ë“œ
        print("ğŸ† ìƒìœ„ 5ê°œ ìœ ì˜ë¯¸í•œ í—¤ë“œ:")
        for i, (head, count) in enumerate(domain_head_usage.head(5).items(), 1):
            percentage = (count / len(domain_df)) * 100
            avg_attention = domain_df[domain_df['max_head'] == head]['avg_evidence_attention'].mean()
            print(f"  {i}. í—¤ë“œ {head}: {count}íšŒ ì‚¬ìš© ({percentage:.1f}%), í‰ê·  ì–´í…ì…˜: {avg_attention:.4f}")
        
        # ì£¼ìš” í—¤ë“œ ì‹ë³„ (ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨ì´ ë†’ì€ í—¤ë“œ)
        print("\nğŸ¯ ë„ë©”ì¸ íŠ¹í™” í—¤ë“œ (ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨ì´ ë†’ì€ í—¤ë“œ):")
        domain_specific_heads = []
        for head in domain_head_usage.index:
            domain_ratio = domain_head_usage[head] / len(domain_df)
            total_ratio = total_head_usage.get(head, 0) / len(df)
            if total_ratio > 0:  # ì „ì²´ì—ì„œ ì‚¬ìš©ëœ í—¤ë“œë§Œ
                relative_ratio = domain_ratio / total_ratio
                if relative_ratio > 1.5:  # 1.5ë°° ì´ìƒ ë†’ìœ¼ë©´ íŠ¹í™” í—¤ë“œë¡œ ê°„ì£¼
                    domain_specific_heads.append((head, relative_ratio, domain_head_usage[head]))
        
        domain_specific_heads.sort(key=lambda x: x[1], reverse=True)
        for head, ratio, count in domain_specific_heads[:3]:
            print(f"  í—¤ë“œ {head}: ì „ì²´ ëŒ€ë¹„ {ratio:.2f}ë°° ë†’ì€ ì‚¬ìš©ë¥ , {count}íšŒ ì‚¬ìš©")
        
        print("\n" + "=" * 80)
        print()

def analyze_head_domain_specialization(results):
    """í—¤ë“œë³„ ë„ë©”ì¸ íŠ¹í™”ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    df = pd.DataFrame(results)
    
    print("ğŸ¯ í—¤ë“œë³„ ë„ë©”ì¸ íŠ¹í™”ë„ ë¶„ì„")
    print("=" * 80)
    print()
    
    # ê° í—¤ë“œë³„ë¡œ ë„ë©”ì¸ ë¶„í¬ ë¶„ì„
    head_domain_dist = df.groupby(['max_head', 'domain']).size().unstack(fill_value=0)
    
    # íŠ¹í™”ë„ ê³„ì‚° (íŠ¹ì • ë„ë©”ì¸ì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í—¤ë“œ)
    specialization_scores = {}
    
    for head in head_domain_dist.index:
        domain_counts = head_domain_dist.loc[head]
        total_usage = domain_counts.sum()
        
        if total_usage >= 5:  # ìµœì†Œ 5íšŒ ì´ìƒ ì‚¬ìš©ëœ í—¤ë“œë§Œ ë¶„ì„
            # ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë„ë©”ì¸
            max_domain = domain_counts.idxmax()
            max_count = domain_counts.max()
            
            # íŠ¹í™”ë„ = í•´ë‹¹ ë„ë©”ì¸ì—ì„œì˜ ì‚¬ìš© ë¹„ìœ¨
            specialization = max_count / total_usage
            
            # ì „ì²´ì—ì„œì˜ í•´ë‹¹ ë„ë©”ì¸ ë¹„ìœ¨
            total_domain_ratio = df[df['domain'] == max_domain].shape[0] / len(df)
            
            # ìƒëŒ€ì  íŠ¹í™”ë„ (ì „ì²´ ëŒ€ë¹„)
            relative_specialization = specialization / total_domain_ratio
            
            specialization_scores[head] = {
                'specialized_domain': max_domain,
                'usage_count': max_count,
                'total_usage': total_usage,
                'specialization_ratio': specialization,
                'relative_specialization': relative_specialization
            }
    
    # íŠ¹í™”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_specialization = sorted(specialization_scores.items(), 
                                  key=lambda x: x[1]['relative_specialization'], 
                                  reverse=True)
    
    print("ğŸ† ë„ë©”ì¸ íŠ¹í™” í—¤ë“œ ìˆœìœ„ (ìƒìœ„ 10ê°œ):")
    print()
    for i, (head, info) in enumerate(sorted_specialization[:10], 1):
        print(f"{i:2d}. í—¤ë“œ {head:2d}: {info['specialized_domain']:10s} ë„ë©”ì¸ íŠ¹í™”")
        print(f"     ì‚¬ìš© íšŸìˆ˜: {info['usage_count']}/{info['total_usage']} ({info['specialization_ratio']:.1%})")
        print(f"     ìƒëŒ€ì  íŠ¹í™”ë„: {info['relative_specialization']:.2f}ë°°")
        print()
    
    return sorted_specialization

def analyze_attention_strength_by_domain(results):
    """ë„ë©”ì¸ë³„ ì–´í…ì…˜ ê°•ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    df = pd.DataFrame(results)
    
    print("ğŸ’ª ë„ë©”ì¸ë³„ ì–´í…ì…˜ ê°•ë„ ë¶„ì„")
    print("=" * 80)
    print()
    
    # ë„ë©”ì¸ë³„ í‰ê·  ì–´í…ì…˜ ê°•ë„
    domain_attention = df.groupby('domain')['avg_evidence_attention'].agg(['mean', 'std', 'min', 'max']).round(4)
    
    print("ğŸ“Š ë„ë©”ì¸ë³„ Evidence Attention í†µê³„:")
    print(domain_attention)
    print()
    
    # ê° ë„ë©”ì¸ì—ì„œ ê°€ì¥ ê°•í•œ ì–´í…ì…˜ì„ ë³´ì¸ í—¤ë“œë“¤
    print("ğŸ”¥ ë„ë©”ì¸ë³„ ìµœê³  ì–´í…ì…˜ í—¤ë“œ:")
    for domain in df['domain'].unique():
        domain_df = df[df['domain'] == domain]
        max_attention_idx = domain_df['avg_evidence_attention'].idxmax()
        max_attention_row = domain_df.loc[max_attention_idx]
        
        print(f"\n{domain.upper()} ë„ë©”ì¸:")
        print(f"  ìµœê³  ì–´í…ì…˜: í—¤ë“œ {max_attention_row['max_head']} ({max_attention_row['avg_evidence_attention']:.4f})")
        print(f"  í”„ë¡¬í”„íŠ¸: {max_attention_row['prompt'][:100]}...")
    
    print()

def generate_domain_head_summary(results):
    """ë„ë©”ì¸ë³„ í—¤ë“œ ì‚¬ìš© ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    df = pd.DataFrame(results)
    
    print("ğŸ“‹ ë„ë©”ì¸ë³„ í—¤ë“œ ì‚¬ìš© ìš”ì•½")
    print("=" * 80)
    print()
    
    summary = {}
    
    for domain in df['domain'].unique():
        domain_df = df[df['domain'] == domain]
        head_usage = domain_df['max_head'].value_counts()
        
        # ì£¼ìš” í—¤ë“œ (ì‚¬ìš©ë¥  10% ì´ìƒ)
        total_samples = len(domain_df)
        major_heads = head_usage[head_usage >= total_samples * 0.1]
        
        # í‰ê·  ì–´í…ì…˜
        avg_attention = domain_df['avg_evidence_attention'].mean()
        
        summary[domain] = {
            'total_samples': total_samples,
            'unique_heads': len(head_usage),
            'major_heads': major_heads.to_dict(),
            'avg_attention': avg_attention,
            'most_used_head': head_usage.index[0] if len(head_usage) > 0 else None
        }
    
    # ìš”ì•½ ì¶œë ¥
    for domain, info in summary.items():
        print(f"ğŸ”¹ {domain.upper()} ë„ë©”ì¸:")
        print(f"   ìƒ˜í”Œ ìˆ˜: {info['total_samples']}")
        print(f"   ì‚¬ìš©ëœ í—¤ë“œ ìˆ˜: {info['unique_heads']}")
        print(f"   í‰ê·  Evidence Attention: {info['avg_attention']:.4f}")
        print(f"   ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í—¤ë“œ: {info['most_used_head']}")
        
        if info['major_heads']:
            print(f"   ì£¼ìš” í—¤ë“œ (10% ì´ìƒ ì‚¬ìš©):")
            for head, count in info['major_heads'].items():
                percentage = (count / info['total_samples']) * 100
                print(f"     í—¤ë“œ {head}: {count}íšŒ ({percentage:.1f}%)")
        print()

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    print("ë„ë©”ì¸ë³„ ìœ ì˜ë¯¸í•œ ì–´í…ì…˜ í—¤ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print()
    
    # ê²°ê³¼ ë¡œë“œ
    results = load_experiment_results()
    
    if not results:
        print("ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(results)}ê°œì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    print()
    
    # ê°ì¢… ë¶„ì„ ìˆ˜í–‰
    analyze_domain_significant_heads(results)
    analyze_head_domain_specialization(results)
    analyze_attention_strength_by_domain(results)
    generate_domain_head_summary(results)
    
    print("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 