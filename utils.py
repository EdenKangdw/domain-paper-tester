import streamlit as st
import requests
import json
import subprocess
import re
from subprocess import PIPE, Popen
import threading
import queue
import time
import shutil

OLLAMA_API_BASE = "http://localhost:11434"
LOG_FILE = "ollama_output.log"

def check_ollama_installed():
    """Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ ìƒíƒœ í™•ì¸"""
    # which ëª…ë ¹ì–´ë¡œ ollama ì‹¤í–‰ íŒŒì¼ í™•ì¸
    ollama_path = shutil.which('ollama')
    if not ollama_path:
        return False, """
### âš ï¸ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!

WSLì— Ollamaë¥¼ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

ì„¤ì¹˜ í›„ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
```bash
ollama serve
```
        """
    
    # Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags", timeout=2)
        return True, None
    except:
        return False, """
### âš ï¸ Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!

ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
```bash
ollama serve
```
        """

def clean_ansi(text):
    """ANSI ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°"""
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    text = re.sub(r'^.*\r(?!$)', '', text)
    text = ansi_escape.sub('', text)
    text = re.sub(r'[â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â ]', '', text)
    text = re.sub(r'^\s*$\n', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def format_progress(text):
    """ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í¬ë§·íŒ…"""
    if "pulling" in text.lower():
        match = re.search(r'pulling ([^:]+): (\d+)%', text.lower())
        if match:
            component, percentage = match.groups()
            bar_length = 20
            filled_length = int(bar_length * int(percentage) / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            return f"ğŸ“¥ {component}:\n{bar} {percentage}%"
    return text

def stream_process_output(process, queue):
    """í”„ë¡œì„¸ìŠ¤ì˜ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íì— ì „ë‹¬"""
    for line in iter(process.stdout.readline, ''):
        queue.put(line)
    process.stdout.close()

def show_terminal_output(process, timeout=60):
    """í„°ë¯¸ë„ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    log_container = st.empty()
    progress_container = st.empty()
    log_text = []
    last_update = ""
    
    output_queue = queue.Queue()
    output_thread = threading.Thread(
        target=stream_process_output, 
        args=(process, output_queue),
        daemon=True
    )
    output_thread.start()
    
    start_time = time.time()
    while time.time() - start_time < timeout and output_thread.is_alive():
        try:
            while True:
                try:
                    line = output_queue.get_nowait()
                    if line:
                        clean_line = clean_ansi(line)
                        if clean_line and clean_line != last_update:
                            formatted_line = format_progress(clean_line)
                            log_text.append(formatted_line)
                            last_update = clean_line
                            if len(log_text) > 20:
                                log_text.pop(0)
                            log_container.code('\n'.join(log_text))
                except queue.Empty:
                    break
            
            time.sleep(0.1)
            progress_container.text(f"ì§„í–‰ ì¤‘... ({int(time.time() - start_time)}ì´ˆ)")
            
        except Exception as e:
            st.error(f"ì¶œë ¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            break
    
    progress_container.empty()
    return log_text

def check_ollama_model_status(model_name):
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": "test",
                "stream": False
            },
            timeout=5
        )
        return response.status_code == 200
    except:
        return False

def start_ollama_model(model_name):
    """ëª¨ë¸ ì‹œì‘"""
    try:
        st.write("ğŸš€ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ì¤‘...")
        
        process = Popen(
            f"ollama run {model_name} 2>&1 | tee {LOG_FILE}",
            shell=True,
            stdout=PIPE,
            stderr=PIPE,
            bufsize=1,
            universal_newlines=True
        )
        
        log_text = show_terminal_output(process)
        
        if check_ollama_model_status(model_name):
            return True, "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "ëª¨ë¸ ì‹¤í–‰ì€ ì‹œì‘ë˜ì—ˆìœ¼ë‚˜, ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
                
    except Exception as e:
        return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def stop_ollama_model():
    """ëª¨ë¸ ì¤‘ì§€"""
    try:
        st.write("ğŸ›‘ ëª¨ë¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•˜ëŠ” ì¤‘...")
        
        process = Popen(
            f"pkill ollama 2>&1 | tee -a {LOG_FILE}",
            shell=True,
            stdout=PIPE,
            stderr=PIPE,
            bufsize=1,
            universal_newlines=True
        )
        
        log_text = show_terminal_output(process, timeout=10)
        
        if process.returncode == 0:
            return True, "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            if "no process found" in ''.join(log_text):
                return True, "ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            return False, "ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
    except Exception as e:
        return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def chat_with_model(model_name, prompt):
    """ëª¨ë¸ê³¼ ëŒ€í™”"""
    try:
        response_container = st.empty()
        current_response = []

        process = Popen(
            f"curl -s -N -X POST http://localhost:11434/api/generate -d '{{\"model\":\"{model_name}\",\"prompt\":\"{prompt}\"}}'",
            shell=True,
            stdout=PIPE,
            stderr=PIPE,
            bufsize=1,
            universal_newlines=True
        )
        
        for line in iter(process.stdout.readline, ''):
            if line.strip():
                try:
                    data = json.loads(line)
                    if 'response' in data:
                        current_response.append(data['response'])
                        response_container.markdown(''.join(current_response))
                except json.JSONDecodeError:
                    continue
        
        process.stdout.close()
        process.wait()
        
        if current_response:
            return ''.join(current_response)
        else:
            return 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
            
    except Exception as e:
        st.error(f"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def get_available_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except:
        return []

def fetch_ollama_models():
    """Ollama í—ˆë¸Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        st.write("ğŸ” Ollama ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        process = Popen(
            f"curl -s https://ollama.com/library 2>&1 | tee -a {LOG_FILE}",
            shell=True,
            stdout=PIPE,
            stderr=PIPE,
            bufsize=1,
            universal_newlines=True
        )
        
        log_text = show_terminal_output(process, timeout=10)
        
        content = '\n'.join(log_text)
        models = re.findall(r'"/library/([^"]+)"', content)
        return sorted(set(models))
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return [] 