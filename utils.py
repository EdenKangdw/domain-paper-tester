import streamlit as st
import requests
import json
import subprocess
import re
from subprocess import PIPE, Popen
import threading
import queue
import time
import shutil

OLLAMA_API_BASE = "http://localhost:11434"
LOG_FILE = "ollama_output.log"

def check_ollama_installed():
    """Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ ìƒíƒœ í™•ì¸"""
    # which ëª…ë ¹ì–´ë¡œ ollama ì‹¤í–‰ íŒŒì¼ í™•ì¸
    ollama_path = shutil.which('ollama')
    if not ollama_path:
        return False, """
### âš ï¸ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!

WSLì— Ollamaë¥¼ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

ì„¤ì¹˜ í›„ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
```bash
ollama serve
```
        """
    
    # Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags", timeout=2)
        return True, None
    except:
        return False, """
### âš ï¸ Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!

ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
```bash
ollama serve
```
        """

def clean_ansi(text):
    """ANSI ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°"""
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    text = re.sub(r'^.*\r(?!$)', '', text)
    text = ansi_escape.sub('', text)
    text = re.sub(r'[â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â ]', '', text)
    text = re.sub(r'^\s*$\n', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def format_progress(text):
    """ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í¬ë§·íŒ…"""
    if "pulling" in text.lower():
        match = re.search(r'pulling ([^:]+): (\d+)%', text.lower())
        if match:
            component, percentage = match.groups()
            bar_length = 20
            filled_length = int(bar_length * int(percentage) / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            return f"ğŸ“¥ {component}:\n{bar} {percentage}%"
    return text

def stream_process_output(process, queue):
    """í”„ë¡œì„¸ìŠ¤ì˜ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íì— ì „ë‹¬"""
    for line in iter(process.stdout.readline, ''):
        queue.put(line)
    process.stdout.close()

def show_terminal_output(process, timeout=60):
    """í„°ë¯¸ë„ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    st.markdown('<style>div[data-testid="stCodeBlock"] > div { width: 100% !important; }</style>', unsafe_allow_html=True)
    log_container = st.empty()
    progress_container = st.empty()
    log_text = []
    last_update = ""
    
    output_queue = queue.Queue()
    output_thread = threading.Thread(
        target=stream_process_output, 
        args=(process, output_queue),
        daemon=True
    )
    output_thread.start()
    
    start_time = time.time()
    while time.time() - start_time < timeout and output_thread.is_alive():
        try:
            while True:
                try:
                    line = output_queue.get_nowait()
                    if line:
                        clean_line = clean_ansi(line)
                        if clean_line and clean_line != last_update:
                            formatted_line = format_progress(clean_line)
                            log_text.append(formatted_line)
                            last_update = clean_line
                            if len(log_text) > 20:
                                log_text.pop(0)
                            log_container.code('\n'.join(log_text))
                except queue.Empty:
                    break
            
            time.sleep(0.1)
            progress_container.text(f"ì§„í–‰ ì¤‘... ({int(time.time() - start_time)}ì´ˆ)")
            
        except Exception as e:
            st.error(f"ì¶œë ¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            break
    
    progress_container.empty()
    return log_text

def check_ollama_model_status(model_name):
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": "test",
                "stream": False
            },
            timeout=15  # íƒ€ì„ì•„ì›ƒì„ 15ì´ˆë¡œ ì—°ì¥ (í° ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤)
        )
        return response.status_code == 200
    except:
        return False

def start_ollama_model(model_name):
    """ëª¨ë¸ ì‹œì‘"""
    try:
        st.write("ğŸš€ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ì¤‘...")
        
        process = Popen(
            f"ollama run {model_name} 2>&1 | tee {LOG_FILE}",
            shell=True,
            stdout=PIPE,
            stderr=PIPE,
            bufsize=1,
            universal_newlines=True
        )
        
        log_text = show_terminal_output(process)
        
        if check_ollama_model_status(model_name):
            return True, "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "ëª¨ë¸ ì‹¤í–‰ì€ ì‹œì‘ë˜ì—ˆìœ¼ë‚˜, ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
                
    except Exception as e:
        return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def stop_ollama_model():
    """ëª¨ë¸ ì¤‘ì§€"""
    try:
        st.write("ğŸ›‘ ëª¨ë¸ì„ ì¤‘ì§€í•˜ëŠ” ì¤‘...")
        
        # ë¨¼ì € ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ í™•ì¸
        response = requests.get(f"{OLLAMA_API_BASE}/api/ps", timeout=5)
        if response.status_code != 200:
            return False, "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        running_models = response.json().get("models", [])
        if not running_models:
            return True, "ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ëª¨ë“  ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ì¤‘ì§€
        stopped_count = 0
        for model_info in running_models:
            model_name = model_info.get("name")
            if model_name:
                try:
                    # Ollama APIë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ ì¤‘ì§€ (ë” ì•ˆì „í•œ ë°©ë²•)
                    # ë¹ˆ í”„ë¡¬í”„íŠ¸ë¡œ ì§§ì€ ìš”ì²­ì„ ë³´ë‚´ì„œ ëª¨ë¸ì„ ì¢…ë£Œì‹œí‚´
                    stop_response = requests.post(
                        f"{OLLAMA_API_BASE}/api/generate",
                        json={
                            "model": model_name,
                            "prompt": "stop",
                            "stream": False,
                            "options": {
                                "num_predict": 1,
                                "temperature": 0
                            }
                        },
                        timeout=3
                    )
                    
                    if stop_response.status_code == 200:
                        stopped_count += 1
                        st.write(f"âœ… {model_name} ëª¨ë¸ ì¤‘ì§€ë¨")
                    else:
                        st.write(f"âš ï¸ {model_name} ëª¨ë¸ ì¤‘ì§€ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {stop_response.status_code})")
                        
                except Exception as e:
                    st.write(f"âŒ {model_name} ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        if stopped_count > 0:
            return True, f"{stopped_count}ê°œ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "ëª¨ë¸ ì¤‘ì§€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def chat_with_model(model_name, prompt):
    """ëª¨ë¸ê³¼ ëŒ€í™”"""
    try:
        # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ API í˜¸ì¶œ
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            return f"API ì˜¤ë¥˜: {response.status_code}"
            
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

@st.cache_data(ttl=30)  # 30ì´ˆ ìºì‹œ
def get_available_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ìºì‹œë¨)"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except:
        return []

def get_model_parameters(model_name):
    """ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë°˜í™˜"""
    model_params = {
        'llama2': ['7B', '13B', '70B'],
        'llama2-uncensored': ['7B', '13B'],
        'mistral': ['7B'],
        'mixtral': ['8x7B'],
        'gemma': ['2B', '7B'],
        'qwen': ['7B', '14B', '72B'],
        'yi': ['6B', '34B'],
        'openchat': ['7B'],
        'neural': ['7B'],
        'falcon': ['7B', '40B'],
        'dolphin': ['7B'],
        'vicuna': ['7B', '13B'],
        'zephyr': ['7B'],
        'nous-hermes': ['7B', '13B'],
        'orca': ['3B', '13B'],
        'starling': ['7B'],
        'openhermes': ['7B', '13B'],
        'wizard': ['7B', '13B'],
        'stable-beluga': ['7B', '13B'],
        'samantha': ['7B'],
        'phind': ['34B'],
        'deepseek': ['7B', '67B'],
        'deepseek-r1-distill-llama': ['8B'],
        'solar': ['7B', '10.7B'],
        'meditron': ['7B'],
        'xwin': ['7B', '13B', '70B'],
        'tinyllama': ['1.1B'],
        'phi': ['2.7B'],
        'notus': ['7B'],
        'codellama': ['7B', '13B', '34B'],
        'wizardcoder': ['13B', '15B', '34B']
    }
    
    # ëª¨ë¸ ì´ë¦„ì—ì„œ ë²„ì „ ì •ë³´ ì œê±°
    base_model = model_name.split(':')[0]
    
    # ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
    for key in model_params:
        if key in base_model.lower():
            return model_params[key]
    
    return []

def fetch_ollama_models():
    """Ollama í—ˆë¸Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        st.write("ğŸ” Ollama LLM ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        response = requests.get("https://ollama.com/library", timeout=10)
        response.raise_for_status()
        
        # HTML ì‘ë‹µì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
        models = re.findall(r'"/library/([^"]+)"', response.text)
        
        # LLMì´ ì•„ë‹Œ ëª¨ë¸ë“¤ í•„í„°ë§
        excluded_keywords = [
            'coder', 'code', 'instruct', 'solar', 'phi', 
            'neural-chat', 'wizard-math', 'dolphin', 
            'stablelm', 'starcoder', 'wizardcoder'
        ]
        
        # LLM ëª¨ë¸ë§Œ í•„í„°ë§í•˜ê³  íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ê°€
        llm_models = []
        for model in models:
            # ì œì™¸í•  í‚¤ì›Œë“œê°€ ëª¨ë¸ ì´ë¦„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not any(keyword in model.lower() for keyword in excluded_keywords):
                params = get_model_parameters(model)
                # íŒŒë¼ë¯¸í„° ì •ë³´ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì¶”ê°€
                if params:
                    base_name = model.split(':')[0]
                    # ê° íŒŒë¼ë¯¸í„° ë²„ì „ë³„ë¡œ ë³„ë„ì˜ í•­ëª© ì¶”ê°€
                    for param in params:
                        param_code = param.lower().replace('x', '')  # 8x7B -> 7b
                        model_code = f"{base_name}:{param_code}"
                        llm_models.append({
                            'name': base_name,
                            'code': model_code,
                            'parameters': param
                        })
        
        return sorted(llm_models, key=lambda x: (x['name'], x['parameters']))
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return []

def extract_evidence_with_ollama(prompt, tokens, model_key, domain):
    """
    Ollama ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì—ì„œ evidence í† í°ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
        tokens (list): í† í¬ë‚˜ì´ì €ë¡œ ë¶„ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸
        model_key (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        domain (str): ë„ë©”ì¸ ì´ë¦„
    
    Returns:
        tuple: (evidence_indices, evidence_tokens)
    """
    try:
        # ë„ë©”ì¸ë³„ evidence ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        domain_prompts = {
            "Economy": "Find tokens related to economy, finance, market, investment, currency, and trade.",
            "Legal": "Find tokens related to law, regulations, contracts, rights, and obligations.",
            "Medical": "Find tokens related to medicine, health, disease, treatment, and drugs.",
            "Technical": "Find tokens related to technology, science, engineering, computers, and systems."
        }
        
        domain_instruction = domain_prompts.get(domain, "ë„ë©”ì¸ ê´€ë ¨ ì¤‘ìš”í•œ í† í°ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.")
        
        # Evidence ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        evidence_prompt = f"""
ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì—ì„œ {domain} ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ evidence í† í°ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

{domain_instruction}

í”„ë¡¬í”„íŠ¸: {prompt}

í† í° ë¦¬ìŠ¤íŠ¸: {tokens}

ìœ„ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ {domain} ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ evidence í† í°ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: ["ì˜í•™", "ì¹˜ë£Œ", "ì•½ë¬¼", "ì§„ë‹¨"]

ì‘ë‹µ í˜•ì‹:
["í† í°1", "í† í°2", "í† í°3", ...]
"""
        
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_key,
                "prompt": evidence_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 100
                }
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "").strip()
            
            # ì‘ë‹µì—ì„œ í† í° ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            import re
            import ast
            
            # JSON í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            tokens_match = re.search(r'\[["\']([^"\']*(?:["\'][^"\']*["\'][^"\']*)*)["\']\]', response_text)
            
            if tokens_match:
                try:
                    # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±
                    list_match = re.search(r'\[[^\]]+\]', response_text)
                    if list_match:
                        evidence_tokens = ast.literal_eval(list_match.group())
                        if isinstance(evidence_tokens, list):
                            # ì‹¤ì œ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
                            indices = []
                            for token in evidence_tokens:
                                if token in tokens:
                                    # í† í°ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì°¾ê¸°
                                    for i, t in enumerate(tokens):
                                        if t == token:
                                            indices.append(i)
                            
                            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                            indices = sorted(list(set(indices)))
                            return indices, evidence_tokens
                except:
                    pass
            
            # ëŒ€ì•ˆ: ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ í† í°ë“¤ ì¶”ì¶œ
            quoted_tokens = re.findall(r'["\']([^"\']+)["\']', response_text)
            if quoted_tokens:
                evidence_tokens = quoted_tokens
                # ì‹¤ì œ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
                indices = []
                for token in evidence_tokens:
                    if token in tokens:
                        # í† í°ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì°¾ê¸°
                        for i, t in enumerate(tokens):
                            if t == token:
                                indices.append(i)
                
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                indices = sorted(list(set(indices)))
                return indices, evidence_tokens
            
            # ë§ˆì§€ë§‰ ëŒ€ì•ˆ: ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ë“¤ ì¶”ì¶œ
            words = re.findall(r'\b\w+\b', response_text)
            evidence_tokens = [word for word in words if word in tokens]
            if evidence_tokens:
                indices = []
                for token in evidence_tokens:
                    if token in tokens:
                        for i, t in enumerate(tokens):
                            if t == token:
                                indices.append(i)
                
                indices = sorted(list(set(indices)))
                return indices, evidence_tokens
            
            return [], []
        else:
            print(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
            return [], []
            
    except Exception as e:
        print(f"Evidence ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return [], []

def get_model_response(model_name, prompt):
    """ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ë°›ì•„ í”„ë¡¬í”„íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤ (responseëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ)"""
    try:
        # ë”¥ì‹œí¬ ëª¨ë¸ì€ ë” ì§§ì€ ì‘ë‹µê³¼ ë¹ ë¥¸ íƒ€ì„ì•„ì›ƒ
        if "deepseek" in model_name.lower():
            num_predict = 80
            timeout = 10
        else:
            num_predict = 150
            timeout = 15
            
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": num_predict
                }
            },
            timeout=timeout
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "").strip()
            
            # deepseek ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
            if "deepseek" in model_name.lower():
                import re
                # <think>...</think> íƒœê·¸ì™€ ë‚´ìš© ì œê±°
                response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
                # <think> íƒœê·¸ë§Œ ìˆëŠ” ê²½ìš° ì œê±°
                response_text = re.sub(r'<think>\s*</think>', '', response_text)
                response_text = response_text.strip()
            
            # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            if not response_text or response_text.lower().startswith(('please enter', 'error', 'failed', 'i cannot', 'i am unable')):
                print(f"Invalid response from model {model_name}: {response_text}")
                return ""
            
            return response_text
        else:
            print(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            return ""
            
    except Exception as e:
        print(f"ëª¨ë¸ ì‘ë‹µ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return ""

@st.cache_data(ttl=30)  # 30ì´ˆ ìºì‹œ
def get_available_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ìºì‹œë¨)"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except:
        return []

def get_model_parameters(model_name):
    """ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë°˜í™˜"""
    model_params = {
        'llama2': ['7B', '13B', '70B'],
        'llama2-uncensored': ['7B', '13B'],
        'mistral': ['7B'],
        'mixtral': ['8x7B'],
        'gemma': ['2B', '7B'],
        'qwen': ['7B', '14B', '72B'],
        'yi': ['6B', '34B'],
        'openchat': ['7B'],
        'neural': ['7B'],
        'falcon': ['7B', '40B'],
        'dolphin': ['7B'],
        'vicuna': ['7B', '13B'],
        'zephyr': ['7B'],
        'nous-hermes': ['7B', '13B'],
        'orca': ['3B', '13B'],
        'starling': ['7B'],
        'openhermes': ['7B', '13B'],
        'wizard': ['7B', '13B'],
        'stable-beluga': ['7B', '13B'],
        'samantha': ['7B'],
        'phind': ['34B'],
        'deepseek': ['7B', '67B'],
        'deepseek-r1': ['7B'],
        'solar': ['7B', '10.7B'],
        'meditron': ['7B'],
        'xwin': ['7B', '13B', '70B'],
        'tinyllama': ['1.1B'],
        'phi': ['2.7B'],
        'notus': ['7B'],
        'codellama': ['7B', '13B', '34B'],
        'wizardcoder': ['13B', '15B', '34B']
    }
    
    # ëª¨ë¸ ì´ë¦„ì—ì„œ ë²„ì „ ì •ë³´ ì œê±° (íŒŒë¼ë¯¸í„° ìˆ˜ ë¬´ì‹œ)
    base_model = model_name.split(':')[0]
    
    # ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
    for key in model_params:
        if key.lower() == base_model.lower():
            return model_params[key]
    
    return []

def fetch_ollama_models():
    """Ollama í—ˆë¸Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        st.write("ğŸ” Ollama LLM ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        response = requests.get("https://ollama.com/library", timeout=10)
        response.raise_for_status()
        
        # HTML ì‘ë‹µì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
        models = re.findall(r'"/library/([^"]+)"', response.text)
        
        # LLMì´ ì•„ë‹Œ ëª¨ë¸ë“¤ í•„í„°ë§
        excluded_keywords = [
            'coder', 'code', 'instruct', 'solar', 'phi', 
            'neural-chat', 'wizard-math', 'dolphin', 
            'stablelm', 'starcoder', 'wizardcoder'
        ]
        
        # LLM ëª¨ë¸ë§Œ í•„í„°ë§í•˜ê³  íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ê°€
        llm_models = []
        for model in models:
            # ì œì™¸í•  í‚¤ì›Œë“œê°€ ëª¨ë¸ ì´ë¦„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not any(keyword in model.lower() for keyword in excluded_keywords):
                params = get_model_parameters(model)
                # íŒŒë¼ë¯¸í„° ì •ë³´ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì¶”ê°€
                if params:
                    base_name = model.split(':')[0]
                    # ê° íŒŒë¼ë¯¸í„° ë²„ì „ë³„ë¡œ ë³„ë„ì˜ í•­ëª© ì¶”ê°€
                    for param in params:
                        param_code = param.lower().replace('x', '')  # 8x7B -> 7b
                        model_code = f"{base_name}:{param_code}"
                        llm_models.append({
                            'name': base_name,
                            'code': model_code,
                            'parameters': param
                        })
        
        return sorted(llm_models, key=lambda x: (x['name'], x['parameters']))
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return []

def extract_evidence_with_ollama(prompt, tokens, model_key, domain):
    """
    Ollama ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì—ì„œ evidence í† í°ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
        tokens (list): í† í¬ë‚˜ì´ì €ë¡œ ë¶„ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸
        model_key (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        domain (str): ë„ë©”ì¸ ì´ë¦„
    
    Returns:
        tuple: (evidence_indices, evidence_tokens)
    """
    try:
        # ë„ë©”ì¸ë³„ evidence ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        domain_prompts = {
            "Economy": "Find tokens related to economy, finance, market, investment, currency, and trade.",
            "Legal": "Find tokens related to law, regulations, contracts, rights, and obligations.",
            "Medical": "Find tokens related to medicine, health, disease, treatment, and drugs.",
            "Technical": "Find tokens related to technology, science, engineering, computers, and systems."
        }
        
        domain_instruction = domain_prompts.get(domain, "ë„ë©”ì¸ ê´€ë ¨ ì¤‘ìš”í•œ í† í°ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.")
        
        # Evidence ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        evidence_prompt = f"""
ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì—ì„œ {domain} ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ evidence í† í°ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

{domain_instruction}

í”„ë¡¬í”„íŠ¸: {prompt}

í† í° ë¦¬ìŠ¤íŠ¸: {tokens}

ìœ„ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ {domain} ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ evidence í† í°ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: ["ì˜í•™", "ì¹˜ë£Œ", "ì•½ë¬¼", "ì§„ë‹¨"]

ì‘ë‹µ í˜•ì‹:
["í† í°1", "í† í°2", "í† í°3", ...]
"""
        
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model_key,
                "prompt": evidence_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 100
                }
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "").strip()
            
            # ì‘ë‹µì—ì„œ í† í° ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            import re
            import ast
            
            # JSON í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            tokens_match = re.search(r'\[["\']([^"\']*(?:["\'][^"\']*["\'][^"\']*)*)["\']\]', response_text)
            
            if tokens_match:
                try:
                    # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±
                    list_match = re.search(r'\[[^\]]+\]', response_text)
                    if list_match:
                        evidence_tokens = ast.literal_eval(list_match.group())
                        if isinstance(evidence_tokens, list):
                            # ì‹¤ì œ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
                            indices = []
                            for token in evidence_tokens:
                                if token in tokens:
                                    # í† í°ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì°¾ê¸°
                                    for i, t in enumerate(tokens):
                                        if t == token:
                                            indices.append(i)
                            
                            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                            indices = sorted(list(set(indices)))
                            return indices, evidence_tokens
                except:
                    pass
            
            # ëŒ€ì•ˆ: ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ í† í°ë“¤ ì¶”ì¶œ
            quoted_tokens = re.findall(r'["\']([^"\']+)["\']', response_text)
            if quoted_tokens:
                evidence_tokens = quoted_tokens
                # ì‹¤ì œ í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
                indices = []
                for token in evidence_tokens:
                    if token in tokens:
                        # í† í°ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì°¾ê¸°
                        for i, t in enumerate(tokens):
                            if t == token:
                                indices.append(i)
                
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                indices = sorted(list(set(indices)))
                return indices, evidence_tokens
            
            # ë§ˆì§€ë§‰ ëŒ€ì•ˆ: ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ë“¤ ì¶”ì¶œ
            words = re.findall(r'\b\w+\b', response_text)
            evidence_tokens = [word for word in words if word in tokens]
            if evidence_tokens:
                indices = []
                for token in evidence_tokens:
                    if token in tokens:
                        for i, t in enumerate(tokens):
                            if t == token:
                                indices.append(i)
                
                indices = sorted(list(set(indices)))
                return indices, evidence_tokens
            
            return [], []
        else:
            print(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
            return [], []
            
    except Exception as e:
        print(f"Evidence ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return [], [] 