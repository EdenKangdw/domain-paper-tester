#!/usr/bin/env python3
"""
DeepSeek í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

from transformers import AutoTokenizer
import json

def test_deepseek_tokenizer():
    """DeepSeek í† í¬ë‚˜ì´ì €ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    print("ğŸ” DeepSeek í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘: deepseek-ai/deepseek-llm-7b-base")
        tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm-7b-base", trust_remote_code=True)
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ!")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompt = "What are the key clinical features associated with a diagnosis of atrial fibrillation in patients presenting with stroke?"
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {test_prompt}")
        
        # í† í¬ë‚˜ì´ì§•
        tokens = tokenizer.tokenize(test_prompt)
        print(f"ğŸ”¤ í† í°í™” ê²°ê³¼ ({len(tokens)}ê°œ í† í°):")
        for i, token in enumerate(tokens):
            print(f"  {i:2d}: '{token}'")
        
        # ì¸ì½”ë”©/ë””ì½”ë”© í…ŒìŠ¤íŠ¸
        encoded = tokenizer.encode(test_prompt)
        decoded = tokenizer.decode(encoded)
        print(f"\nğŸ”„ ì¸ì½”ë”©/ë””ì½”ë”© í…ŒìŠ¤íŠ¸:")
        print(f"ì›ë³¸: {test_prompt}")
        print(f"ë³µì›: {decoded}")
        
        # í† í¬ë‚˜ì´ì € ì •ë³´
        print(f"\nğŸ“Š í† í¬ë‚˜ì´ì € ì •ë³´:")
        print(f"  - vocab_size: {tokenizer.vocab_size}")
        print(f"  - model_max_length: {tokenizer.model_max_length}")
        print(f"  - pad_token: {tokenizer.pad_token}")
        print(f"  - eos_token: {tokenizer.eos_token}")
        print(f"  - bos_token: {tokenizer.bos_token}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_evidence_extraction():
    """Evidence ì¶”ì¶œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    print("\n" + "="*50)
    print("ğŸ§  Evidence ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm-7b-base", trust_remote_code=True)
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "What are the key clinical features associated with a diagnosis of atrial fibrillation in patients presenting with stroke?",
            "Describe the primary ethical considerations associated with the use of artificial intelligence in patient diagnosis and treatment.",
            "List the most common side effects associated with long-term use of Metformin for Type 2 Diabetes management."
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            
            # í† í¬ë‚˜ì´ì§•
            tokens = tokenizer.tokenize(prompt)
            print(f"ğŸ”¤ í† í° ìˆ˜: {len(tokens)}")
            
            # ê°„ë‹¨í•œ evidence ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” ë‹¨ì–´ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨íˆ ì²˜ë¦¬
            words = prompt.split()
            print(f"ğŸ“š ë‹¨ì–´ ìˆ˜: {len(words)}")
            
            # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ evidenceë¡œ ê°€ì •
            evidence_words = [words[0], words[-1]] if len(words) > 1 else words
            print(f"ğŸ¯ ì¶”ì¶œëœ evidence ë‹¨ì–´ë“¤: {evidence_words}")
            
        return True
        
    except Exception as e:
        print(f"âŒ Evidence ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

if __name__ == "__main__":
    print("ğŸš€ DeepSeek í† í¬ë‚˜ì´ì € ë° Evidence ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    tokenizer_success = test_deepseek_tokenizer()
    
    # Evidence ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    evidence_success = test_evidence_extraction()
    
    print("\n" + "="*60)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"  í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸: {'âœ… ì„±ê³µ' if tokenizer_success else 'âŒ ì‹¤íŒ¨'}")
    print(f"  Evidence ì¶”ì¶œ í…ŒìŠ¤íŠ¸: {'âœ… ì„±ê³µ' if evidence_success else 'âŒ ì‹¤íŒ¨'}")
    
    if tokenizer_success and evidence_success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì´ì œ Streamlit ì•±ì—ì„œ deepseek-r1:7b ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.") 